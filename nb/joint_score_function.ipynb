{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from transformers import EncoderDecoderModel, BertTokenizer, BertModel, BertForNextSentencePrediction, BatchEncoding\n",
    "import torch\n",
    "from torch.nn import Softmax, Linear, CrossEntropyLoss\n",
    "from torch.nn.functional import normalize\n",
    "from typing import List, Iterable\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.TextProcessing import build_word_tree, process_keywords, batched_sent_tokenize, clean_text\n",
    "from tools.BasicUtils import my_write, my_csv_read, my_read, my_json_read, ntopidx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loss = CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "file_description = [\n",
    "    \"keyword_f.txt ---- CS keywords\",\n",
    "    \"wordtree.json ---- word tree for cs keywords\",\n",
    "    \"entity.txt ---- Reformed cs keywords with '_' replacing ' '\"\n",
    "]\n",
    "\n",
    "if not os.path.exists('../data/temp/joint_score_function'):\n",
    "    os.mkdir('../data/temp/joint_score_function')\n",
    "    \n",
    "my_write('../data/temp/joint_score_function/readme.txt', file_description)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect keywords from terms-cs-cfl-epoch200.txt\n",
    "stable_kw = []\n",
    "unstable_kw = []\n",
    "r = my_csv_read('../data/raw_data/terms-cs-cfl-epoch200.txt', delimiter='\\t')\n",
    "candidate_kw_list = [item[0] for item in r if float(item[1]) > 0.1]\n",
    "stable_kw, unstable_kw = process_keywords(candidate_kw_list)\n",
    "# Save keywords\n",
    "my_write('../data/temp/joint_score_function/keyword_f.txt', stable_kw)\n",
    "# Generate word tree (25 seconds)\n",
    "build_word_tree('../data/temp/joint_score_function/keyword_f.txt', '../data/temp/joint_score_function/wordtree.json', '../data/temp/joint_score_function/entity.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Go to py folder and run followings in the backend \n",
    "# \"python gen_co_occur.py ../data/temp/joint_score_function/wordtree.json ../data/corpus/small_sent.txt ../data/temp/joint_score_function/co_occur.txt\"\n",
    "# \"python gen_occur.py ../data/temp/joint_score_function/keyword_f.txt ../data/temp/joint_score_function/co_occur.txt ../data/temp/joint_score_function/occur.json\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Knowledge Graph filtering\n",
    "\n",
    "# Load known cs keywords\n",
    "kw_set = set(my_read('../data/temp/joint_score_function/keyword_f.txt'))\n",
    "# Get potential cs entity id\n",
    "eid_set = set([eid for eid, ent in my_csv_read('../data/raw_data/wikidata/entity_names.txt', delimiter='\\t') if ent.lower() in kw_set])\n",
    "# Get the subgraph that both entities are potential cs keywords\n",
    "kg_cs_triples = [(eid1, eid2, rid) for eid1, eid2, rid in my_csv_read('../data/raw_data/wikidata/triples.txt', delimiter=' ') if eid1 in eid_set and eid2 in eid_set]\n",
    "# Get cs entities and relations from subgraph\n",
    "cs_eid_set = set()\n",
    "cs_rid_set = set()\n",
    "for eid1, eid2, rid in kg_cs_triples:\n",
    "    cs_eid_set.update((eid1, eid2))\n",
    "    cs_rid_set.add(rid)\n",
    "# Map id to text\n",
    "eid2ent_dict = {eid:ent.lower() for eid, ent in my_csv_read('../data/raw_data/wikidata/entity_names.txt', delimiter='\\t') if eid in cs_eid_set}\n",
    "rid2rel_dict = {rid:rel.lower() for rid, rel in my_csv_read('../data/raw_data/wikidata/relation_names.txt', delimiter='\\t') if rid in cs_rid_set}\n",
    "# Save files\n",
    "json.dump(eid2ent_dict, open('../data/temp/joint_score_function/eid2ent.json', 'w'))\n",
    "json.dump(rid2rel_dict, open('../data/temp/joint_score_function/rid2rel.json', 'w'))\n",
    "csv.writer(open('../data/temp/joint_score_function/kg_cs_triples.csv', 'w')).writerows(kg_cs_triples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eid2ent_dict = my_json_read('../data/temp/joint_score_function/eid2ent.json')\n",
    "rid2rel_dict = my_json_read('../data/temp/joint_score_function/rid2rel.json')\n",
    "kg_cs_triples = my_csv_read('../data/temp/joint_score_function/kg_cs_triples.csv')\n",
    "rel_list = list(set(rid2rel_dict.values()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i, item in enumerate(kg_cs_triples):\n",
    "    ent1, ent2, rel = eid2ent_dict[item[0]], eid2ent_dict[item[1]], rid2rel_dict[item[2]]\n",
    "    print('%s--%s--%s' % (ent1, ent2, rel))\n",
    "    if i >= 10:\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class RetrieveSentForPairCoOccur:\n",
    "    def __init__(self, sent_file:str, occur_file:str):\n",
    "        self._sents = my_read(sent_file)\n",
    "        self._occur_dict = defaultdict(set)\n",
    "        for k, v in json.load(open(occur_file)).items():\n",
    "            self._occur_dict[k] = set(v)\n",
    "\n",
    "    def retrieve(self, kw1:str, kw2:str):\n",
    "        co_occur_index = self._occur_dict[kw1] & self._occur_dict[kw2]\n",
    "        return [self._sents[idx] for idx in co_occur_index]\n",
    "\n",
    "retriever = RetrieveSentForPairCoOccur('../data/corpus/small_sent.txt', '../data/temp/joint_score_function/occur.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ScoreFunction1(torch.nn.Module):\n",
    "    def __init__(self, model_file:str, additional_special_tokens:List[str]=None, device:str=None):\n",
    "        super().__init__()\n",
    "        self._score_function = BertForNextSentencePrediction.from_pretrained(model_file)\n",
    "        self._tokenizer = BertTokenizer.from_pretrained(model_file)\n",
    "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else torch.device(device)\n",
    "        self._sm = Softmax(1)\n",
    "        \n",
    "        if additional_special_tokens is not None:\n",
    "            self._tokenizer.add_special_tokens({'additional_special_tokens' : additional_special_tokens})\n",
    "            self._score_function.resize_token_embeddings(len(self._tokenizer))\n",
    "        self._score_function.to(self._device)\n",
    "\n",
    "    def forward(self, candidate_sents:List[str], query:str):\n",
    "        inputs = BatchEncoding(self._tokenizer(candidate_sents, [query]*len(candidate_sents), padding=True, truncation=True, max_length=80, return_tensors=\"pt\")).to(self._device)\n",
    "        output = self._score_function(**inputs, labels=torch.LongTensor([1]*len(candidate_sents)).to(self._device))\n",
    "        return self._sm(output.logits)[:, 1]\n",
    "        \n",
    "sf1 = ScoreFunction1('bert-base-uncased', additional_special_tokens=['<RELATION>'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ScoreFunction2(torch.nn.Module):\n",
    "    def __init__(self, context_model_file:str, query_model_file:str, additional_special_tokens:List[str]=None, device:str=None):\n",
    "        super().__init__()\n",
    "        self._context_encoder = BertModel.from_pretrained(context_model_file)\n",
    "        self._query_encoder = BertModel.from_pretrained(query_model_file)\n",
    "        self._tokenizer = BertTokenizer.from_pretrained(query_model_file)\n",
    "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else torch.device(device)\n",
    "        \n",
    "        if additional_special_tokens is not None:\n",
    "            self._tokenizer.add_special_tokens({'additional_special_tokens' : additional_special_tokens})\n",
    "            self._query_encoder.resize_token_embeddings(len(self._tokenizer))\n",
    "        self._query_encoder.to(self._device)\n",
    "        self._context_encoder.to(self._device)\n",
    "\n",
    "    def forward(self, candidate_sents:List[str], query:str):\n",
    "        context_inputs = BatchEncoding(self._tokenizer(candidate_sents, padding=True, truncation=True, max_length=80, return_tensors=\"pt\")).to(self._device)\n",
    "        query_inputs = BatchEncoding(self._tokenizer(query, padding=True, truncation=True, max_length=20, return_tensors=\"pt\")).to(self._device)\n",
    "        context_emb = normalize(self._context_encoder(**context_inputs).last_hidden_state[:, 0, :])\n",
    "        query_emb = normalize(self._query_encoder(**query_inputs).last_hidden_state[:, 0, :])\n",
    "        return torch.inner(context_emb, query_emb)\n",
    "\n",
    "sf2 = ScoreFunction2('bert-base-uncased', 'bert-base-uncased', additional_special_tokens=['<RELATION>'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Reader1(torch.nn.Module):\n",
    "    def __init__(self, encoder_model:str, rels:List[str], device:str=None):\n",
    "        super().__init__()\n",
    "        self._rel2cls = {rel:i for i, rel in enumerate(rels)}\n",
    "        self._rels = rels\n",
    "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else torch.device(device)\n",
    "        self._classifier = Linear(768, len(self._rel2cls), device=self._device)\n",
    "        self._encoder = BertModel.from_pretrained(encoder_model).to(self._device)\n",
    "        self._tokenizer = BertTokenizer.from_pretrained(encoder_model)\n",
    "        self._loss_cal = CrossEntropyLoss()\n",
    "        self._sm = Softmax(1)\n",
    "\n",
    "    def forward(self, sents:List[str], score:torch.Tensor, rel:str=None):\n",
    "        inputs = BatchEncoding(self._tokenizer(sents, padding=True, truncation=True, max_length=80, return_tensors=\"pt\")).to(self._device)\n",
    "        sents_emb = self._encoder(**inputs).last_hidden_state[:, 0, :]\n",
    "        merged_emb = torch.inner(score, sents_emb)\n",
    "        cls_ret = self._classifier(merged_emb)\n",
    "        if rel is not None:\n",
    "            temp_cls = self._rel2cls[rel]\n",
    "            return self._rels[torch.argmax(self._sm(cls_ret), dim=1)], self._loss_cal(cls_ret, torch.tensor(temp_cls, dtype=torch.long))\n",
    "        else:\n",
    "            return self._rels[torch.argmax(self._sm(cls_ret), dim=1)]\n",
    "\n",
    "reader1 = Reader1('bert-base-uncased', rel_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Reader2(torch.nn.Module):\n",
    "    def __init__(self, encoder_model:str, rels:List[str], device:str=None):\n",
    "        super().__init__()\n",
    "        self._rel2cls = {rel:i for i, rel in enumerate(rels)}\n",
    "        self._rels = rels\n",
    "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else torch.device(device)\n",
    "        self._classifier = Linear(768, len(self._rel2cls), device=self._device)\n",
    "        self._encoder = BertModel.from_pretrained(encoder_model).to(self._device)\n",
    "        self._tokenizer = BertTokenizer.from_pretrained(encoder_model)\n",
    "        self._loss_cal = CrossEntropyLoss()\n",
    "        self._sm = Softmax(1)\n",
    "\n",
    "    def forward(self, sents:List[str], score:torch.Tensor, rel:str=None):\n",
    "        inputs = BatchEncoding(self._tokenizer(sents, padding=True, truncation=True, max_length=80, return_tensors=\"pt\")).to(self._device)\n",
    "        sents_emb = self._encoder(**inputs).last_hidden_state[:, 0, :]\n",
    "        merged_emb = torch.inner(score, sents_emb)\n",
    "        cls_ret = self._classifier(merged_emb)\n",
    "        if rel is not None:\n",
    "            temp_cls = self._rel2cls[rel]\n",
    "            return self._rels[torch.argmax(self._sm(cls_ret), dim=1)], self._loss_cal(cls_ret, torch.tensor(temp_cls, dtype=torch.long))\n",
    "        else:\n",
    "            return self._rels[torch.argmax(self._sm(cls_ret), dim=1)]\n",
    "\n",
    "reader2 = Reader2('bert-base-uncased', rel_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "m = BertModel.from_pretrained('bert-base-uncased')\n",
    "t = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# initialize Bert2Bert from pre-trained checkpoints\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased')\n",
    "bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "bert2bert.config.eos_token_id = tokenizer.sep_token_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for eid1, eid2, rid in kg_cs_triples:\n",
    "    ent1, ent2, rel = eid2ent_dict[eid1], eid2ent_dict[eid2], rid2rel_dict[rid]\n",
    "    sents = retriever.retrieve(ent1, ent2)\n",
    "    score = sf1(sents, ' '.join((ent1, '<RELATION>', ent2)))\n",
    "    top_idx = ntopidx(10, score)\n",
    "    sub_sents = [sents[idx] for idx in top_idx]\n",
    "    sub_score = [score[idx] for idx in top_idx]\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('FWD': conda)"
  },
  "interpreter": {
   "hash": "868d073bc6748a7d27fbe19363d0a926b0fdd78861a04c08009f08b8aa15d600"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}