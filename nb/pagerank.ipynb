{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from typing import List\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from tools.BasicUtils import my_read, get_wiki_page_from_kw, clean_sent, my_write"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def isf(w:str, D:int, counters:List[Counter]):\n",
    "    return math.log(D * 1.0 / sum([1 if w in sent else 0 for sent in counters]))\n",
    "\n",
    "def sentence_filtering(sents:List[str], kw1:str, kw2:str):\n",
    "    return [sent for sent in sents if sent.count(' ') < 80 \n",
    "                and sent.count(kw1) == 1 \n",
    "                and sent.count(kw2) == 1 \n",
    "                and sent.count('%s %s' % (kw1, kw2)) == 0 \n",
    "                and sent.count('%s %s' % (kw2, kw1)) == 0]\n",
    "\n",
    "self_define_stopwords = set(['-', ',', '.'])\n",
    "\n",
    "def do_pagerank(sents:List[str]):\n",
    "    # Remove stop words\n",
    "    sw = set(stopwords.words('english'))\n",
    "    clean_sents = [[token for token in word_tokenize(sent) if token not in sw and token not in self_define_stopwords] for sent in sents]\n",
    "\n",
    "    # Generate word counters\n",
    "    counters = [Counter(sent) for sent in clean_sents]\n",
    "\n",
    "    # Build similarity matrix\n",
    "    D = len(clean_sents)\n",
    "    sim_matrix = np.zeros((D, D))\n",
    "    part_list = [math.sqrt(sum([(sent[w] * isf(w, D, counters)) ** 2 for w in sent])) for sent in counters]\n",
    "    # return part_list\n",
    "    for i in range(D - 1):\n",
    "        for j in range(i + 1, D):\n",
    "            sent_1 = counters[i]\n",
    "            sent_2 = counters[j]\n",
    "            share_word_set = sent_1 & sent_2\n",
    "            numerator = sum([(sent_1[w] * sent_2[w] * (isf(w, D, counters) ** 2)) for w in share_word_set])\n",
    "            denominator = part_list[i] * part_list[j]\n",
    "            sim_matrix[i, j] = numerator / denominator\n",
    "    sim_matrix = sim_matrix + sim_matrix.T\n",
    "    g = nx.from_numpy_array(sim_matrix)\n",
    "    score = nx.pagerank(g)\n",
    "    temp = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "    idx = [item[0] for item in temp]\n",
    "    return [sents[i] for i in idx], [score[i] for i in idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Collect candidate sentences from arxiv documents\n",
    "!grep 'python' ../data/corpus/small_sent.txt > temp.txt\n",
    "!grep 'programming language' temp.txt > all_occurance.txt\n",
    "sent_list = my_read('all_occurance.txt')\n",
    "while '' in sent_list:\n",
    "    sent_list.remove('')\n",
    "sent_list = sentence_filtering(sent_list, 'python', 'programming language')\n",
    "len(sent_list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Collect candidate sentences from wikipedia page\n",
    "remove_list = ['See also', 'References', 'Further reading']\n",
    "\n",
    "page_entity_list = ['python (programming language)', 'programming language']\n",
    "pages = [get_wiki_page_from_kw(entity) for entity in page_entity_list]\n",
    "kw_1 = 'python'\n",
    "kw_2 = 'programming language'\n",
    "sent_list = []\n",
    "if None in pages:\n",
    "    print('Below entities have missing page:')\n",
    "    print([page_entity_list[i] for i in range(len(page_entity_list)) if pages[i] is None])\n",
    "else:\n",
    "    for p in pages:\n",
    "        sections = p.sections.copy()\n",
    "        for item in remove_list:\n",
    "            if item in sections:\n",
    "                sections.remove(item)\n",
    "        for section in sections:\n",
    "            text = p.section(section)\n",
    "            text = clean_sent(text).lower()\n",
    "            sentences = sent_tokenize(text)\n",
    "            sent_list += [sent for sent in sentences if kw_1 in sent and kw_2 in sent]\n",
    "        text = p.summary\n",
    "        text = clean_sent(text).lower()\n",
    "        sentences = sent_tokenize(text)\n",
    "        sent_list += [sent for sent in sentences if kw_1 in sent and kw_2 in sent]\n",
    "\n",
    "    sent_list = sentence_filtering(sent_list, 'python', 'programming language')\n",
    "    print(len(sent_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "sents, score = do_pagerank(sent_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "sents[-10:]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['however, many other programming languages were written in non - english languages, for instance, the chinese basic, the chinese python, the russian rapira, and the arabic loughaty.',\n",
       " 'python, as a popular programming language in recent years, has not been realized in gui design.',\n",
       " 'this database can be conveniently searched and accessed from a wide variety of programming languages, such as c++, python, java, matlab, and r. this contribution provides some details about the successful conversion of the exfor library to a mongodb database and shows simple usage examples to underline its merits.',\n",
       " 'python, a popular and fast - growing programming language, sees heavy use on both sites, with nearly one million questions asked on stack overflow and 400 thousand public gists on github.',\n",
       " 'this engine is released as part of stocpy, a new turing - complete probabilistic programming language, available as a python library.',\n",
       " 'we consider the overhead of function calls in the programming languages matlab/octave, python, cython and c. in many applications a function has to be called very often inside a loop.',\n",
       " 'for instance, python has risen to the top of the list of the programming languages due to the simplicity of its syntax, while still achieving a good performance even being an interpreted language.',\n",
       " 'these block - based tools intend to familiarize students with programming logic, before diving into text - based programming languages such as java, python, etc.',\n",
       " 'measurement automation was developed using python 3.7 programming language.',\n",
       " 'despite the advantages of c++, python is a flexible and dominant programming language that enables rapid prototyping of bioinformatics pipelines.']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "my_write('top_5_from_wiki.txt', sents[:5])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "947ccf1d8baae4b0b3c7136017192ad9c9ad48a2268b8759d45f6c7f995c7f83"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('imojie_env': virtualenvwrapper)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}