{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd0947ccf1d8baae4b0b3c7136017192ad9c9ad48a2268b8759d45f6c7f995c7f83",
   "display_name": "Python 3.6.9 64-bit ('imojie_env': virtualenvwrapper)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from tools.Models import save_model, rescale_gradients, ScoreFunctionModel_1, ScoreFunctionModel_2\n",
    "from tools.TextProcessing import nlp, find_dependency_path_from_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants\n",
    "max_path_len = 10\n",
    "embed_dim = 512\n",
    "dataset_file = '../data/corpus/score_func_dataset.csv'\n",
    "train_file = '../data/temp/train.csv'\n",
    "valid_file = '../data/temp/valid.csv'\n",
    "epochs = 6\n",
    "lr = 0.01\n",
    "grad_norm = 10\n",
    "dev_every = 5000\n",
    "save_path = '../data/output/score_func/save_path_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive examples\n",
    "df_pos = pd.read_csv(dataset_file)\n",
    "df_pos['label'] = 'T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create negative examples for model 1\n",
    "def gen_neg_path(path:str):\n",
    "    items = path.split()\n",
    "    if len(items) < 2 or len(set(items)) == 1:\n",
    "        return ''\n",
    "    shuffled_list = random.sample(items, len(items))\n",
    "    while shuffled_list == items:\n",
    "        shuffled_list = random.sample(items, len(items))\n",
    "    return ' '.join(shuffled_list)\n",
    "\n",
    "df_neg = df_pos.copy()\n",
    "df_neg['path'] = df_neg.apply(lambda row: gen_neg_path(row['path']), axis=1)\n",
    "df_neg = df_neg[df_neg['path'] != '']\n",
    "df_neg['label'] = 'F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create negative examples for model 2\n",
    "df_neg = pd.concat([df_pos.path.to_frame(), \n",
    "                    df_pos.subj.sample(frac=1).reset_index(drop=True).to_frame(), \n",
    "                    df_pos.obj.sample(frac=1).reset_index(drop=True).to_frame()], axis=1)\n",
    "df_neg['label'] = 'F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation dataset\n",
    "df = pd.concat([df_pos, df_neg], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "total_num = len(df)\n",
    "train_df = df[:int(total_num*0.8)]\n",
    "valid_df = df[int(total_num*0.8):]\n",
    "train_df.to_csv(train_file, index=False)\n",
    "valid_df.to_csv(valid_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation dataset\n",
    "train_df = pd.read_csv(train_file)\n",
    "valid_df = pd.read_csv(valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Vocabularies\n",
    "path_tokenizer = lambda x: x.split()\n",
    "\n",
    "path_c = Counter()\n",
    "for line in train_df['path']:\n",
    "    path_c.update(path_tokenizer(line))\n",
    "path_vocab = Vocab(path_c)\n",
    "\n",
    "entity_c = Counter()\n",
    "entity_c.update(train_df['subj'].values.tolist())\n",
    "entity_c.update(train_df['obj'].values.tolist())\n",
    "entity_vocab = Vocab(entity_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and generate training and validation dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, corpus_file):\n",
    "        self.dataset = pd.read_csv(corpus_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset['path'][idx], self.dataset['subj'][idx], self.dataset['obj'][idx], self.dataset['label'][idx]\n",
    "\n",
    "train_dataset = MyDataset(train_file)\n",
    "valid_dataset = MyDataset(valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and generate dataloader\n",
    "def collate_batch(batch):\n",
    "   path_list, subj_list, obj_list, label_list = [], [], [], []\n",
    "   for (_path, _subj, _obj, _label) in batch:\n",
    "        path_list.append(torch.tensor([path_vocab.stoi[item] for item in _path.split()]))\n",
    "        subj_list.append(entity_vocab.stoi[_subj])\n",
    "        obj_list.append(entity_vocab.stoi[_obj])\n",
    "        label_list.append(1 if _label == 'T' else -1)\n",
    "   return pad_sequence(path_list, padding_value=path_vocab.stoi['<pad>']), torch.tensor(subj_list), torch.tensor(obj_list), torch.tensor(label_list)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training process\n",
    "def train(save_path:str, path_vocab:Vocab, \n",
    "    train_iter:DataLoader, val_iter:DataLoader, \n",
    "    model_type:int=1, embed_dim:int=512, max_path_len:int=10, epochs:int=6, dev_every:int = 5000, \n",
    "    grad_norm:int = 10, lr:float=0.01, \n",
    "    entity_vocab:Vocab=None, \n",
    "    retrain:bool=False, cp_file:str=None):\n",
    "\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if model_type == 1:\n",
    "        model = ScoreFunctionModel_1(device, len(path_vocab), embed_dim)\n",
    "    elif model_type == 2:\n",
    "        model = ScoreFunctionModel_2(device, len(path_vocab), len(entity_vocab), embed_dim)\n",
    "    else:\n",
    "        return\n",
    "    if retrain:\n",
    "        checkpoint = torch.load(cp_file)\n",
    "        model.load_state_dict(checkpoint, strict=True)\n",
    "    model.to(device)\n",
    "    best_train_loss = 1000\n",
    "    os.makedirs(save_path)\n",
    "    params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    opt = optim.SGD(params, lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.9, patience=10, verbose=True, threshold=0.001)\n",
    "    iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_accu_loss = 0\n",
    "        train_cnt = 0\n",
    "        \n",
    "        for path_, subj_, obj_, label_ in tqdm(iter(train_iter)):\n",
    "            # Switch model to training mode, clear gradient accumulators\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            iterations += 1\n",
    "                \n",
    "            # forward pass\n",
    "            if model_type == 1:\n",
    "                answer, loss = model(path_.to(device), path_vocab.stoi['<pad>'], label_.to(device))\n",
    "            else:\n",
    "                answer, loss = model(path_.to(device), path_vocab.stoi['<pad>'], subj_.to(device), obj_.to(device), label_.to(device))\n",
    "            \n",
    "            # backpropagate and update optimizer learning rate\n",
    "            loss.backward()\n",
    "\n",
    "            # grad clipping\n",
    "            rescale_gradients(model, grad_norm)\n",
    "            opt.step()\n",
    "            \n",
    "            # aggregate training error\n",
    "            train_accu_loss += loss.item()\n",
    "            train_cnt += 1\n",
    "            \n",
    "        # evaluate performance on validation set periodically\n",
    "        model.eval()\n",
    "        eval_accu_loss = 0\n",
    "        eval_cnt = 0\n",
    "        for dev_path_, dev_subj_, dev_obj_, dev_label_ in iter(val_iter):\n",
    "            if model_type == 1:\n",
    "                answer, loss = model(dev_path_.to(device), path_vocab.stoi['<pad>'], dev_label_.to(device))\n",
    "            else:\n",
    "                answer, loss = model(dev_path_.to(device), path_vocab.stoi['<pad>'], dev_subj_.to(device), dev_obj_.to(device), dev_label_.to(device))\n",
    "            eval_accu_loss += loss.item()\n",
    "            eval_cnt += 1\n",
    "        eval_loss = eval_accu_loss / eval_cnt\n",
    "        scheduler.step(eval_loss)\n",
    "        \n",
    "        train_loss = train_accu_loss / train_cnt\n",
    "\n",
    "        print('train_loss: %.3f, eval_loss: %.3f' % (train_loss, eval_loss))\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            save_model(model, save_path, train_loss, iterations, 'best_train_snapshot')\n",
    "        \n",
    "        save_model(model, save_path, train_loss, iterations, 'epoch_train_snapshot')\n",
    "\n",
    "        # reset train stats\n",
    "        train_accu_loss = 0\n",
    "        train_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(save_path=save_path, path_vocab=path_vocab, train_iter=train_dataloader, val_iter=valid_dataloader, model_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_file = '../data/output/score_func/'\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ScoreFunctionModel_1(device, len(path_vocab), embed_dim)\n",
    "checkpoint = torch.load(cp_file)\n",
    "model.load_state_dict(checkpoint, strict=True)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scrore(model:ScoreFunctionModel_1, path_vocab:Vocab, sent:str, kw1:str, kw2:str):\n",
    "    path = find_dependency_path_from_tree(nlp(sent), kw1, kw2)\n",
    "    path_ = torch.tensor([[path_vocab.stoi[item] for item in _path.split()]]).to(device)\n",
    "    answer, loss = model(path_, path_vocab.stoi['<pad>'], torch.tensor([1]).to(device))\n",
    "    return answer[0]"
   ]
  }
 ]
}