{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tools.TextProcessing import nlp, find_dependency_path_from_tree, find_span, find_root_in_span\n",
    "from tools.BasicUtils import my_write, my_read, SparseRetrieveSentForPairCoOccur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifier_dependencies = {'acl', 'advcl', 'advmod', 'amod', 'mark', 'meta', 'neg', 'nn', 'nmod', 'npmod', 'nummod', 'poss', 'prep', 'quantmod', 'relcl'}\n",
    "adjunctive_dependencies = {'appos', 'aux', 'auxpass', 'compound', 'cop', 'det', 'expl', 'punct'}\n",
    "\n",
    "def expand_dependency_info_from_tree(doc, path:np.ndarray):\n",
    "    dep_path:list = (np.arange(*path.shape)[path!=0]).tolist()\n",
    "    for element in dep_path:\n",
    "        if doc[element].dep_ == 'conj':\n",
    "            path[doc[element].head.i] = 0\n",
    "    dep_path:list = (np.arange(*path.shape)[path!=0]).tolist()\n",
    "    modifiers = []\n",
    "    for element in dep_path:\n",
    "        for child in doc[element].children:\n",
    "            if path[child.i] == 0 and (child.dep_ in modifier_dependencies or child.dep_ in adjunctive_dependencies):\n",
    "                path[child.i] = 1\n",
    "                modifiers.append(child.i)\n",
    "    while len(modifiers) > 0:\n",
    "        modifier = modifiers.pop(0)\n",
    "        for child in doc[modifier].children:\n",
    "            if path[child.i] == 0:\n",
    "                path[child.i] = 1\n",
    "                modifiers.append(child.i)\n",
    "\n",
    "def find_dependency_info_from_tree(doc, kw1:spacy.tokens.span.Span, kw2:spacy.tokens.span.Span):\n",
    "    # Find roots of the spans\n",
    "    idx1 = find_root_in_span(kw1)\n",
    "    idx2 = find_root_in_span(kw2)\n",
    "    kw1_front, kw1_end = kw1[0].i, kw1[-1].i\n",
    "    kw2_front, kw2_end = kw2[0].i, kw2[-1].i\n",
    "    branch = np.zeros(len(doc))\n",
    "    kw1_steps = []\n",
    "    \n",
    "    i = idx1\n",
    "    while branch[i] == 0:\n",
    "        branch[i] = 1\n",
    "        kw1_steps.append(i)\n",
    "        i = doc[i].head.i\n",
    "        if i >= kw2_front and i <= kw2_end:\n",
    "            # kw2 is above kw1\n",
    "            branch[kw1_front : kw1_end+1] = 1\n",
    "            branch[kw2_front : kw2_end+1] = 1\n",
    "            expand_dependency_info_from_tree(doc, branch)\n",
    "            return branch\n",
    "    \n",
    "    i = idx2\n",
    "    while branch[i] != 1:\n",
    "        branch[i] = 2\n",
    "        \n",
    "        if i == doc[i].head.i:\n",
    "            return np.zeros(1)\n",
    "        \n",
    "        i = doc[i].head.i\n",
    "        if i >= kw1_front and i <= kw1_end:\n",
    "            # kw1 is above kw2\n",
    "            branch[branch != 2] = 0\n",
    "            branch[branch == 2] = 1\n",
    "            branch[kw1_front : kw1_end+1] = 1\n",
    "            branch[kw2_front : kw2_end+1] = 1\n",
    "            expand_dependency_info_from_tree(doc, branch)\n",
    "            return branch\n",
    "    # kw1 and kw2 are on two sides, i is their joint\n",
    "    break_point = kw1_steps.index(i)\n",
    "    branch[kw1_steps[break_point+1 : ]] = 0\n",
    "    branch[branch != 0] = 1\n",
    "    branch[kw1_front : kw1_end+1] = 1\n",
    "    branch[kw2_front : kw2_end+1] = 1\n",
    "    expand_dependency_info_from_tree(doc, branch)\n",
    "    return branch\n",
    "\n",
    "def informativeness_demo(sent:str, kw1:str, kw2:str):\n",
    "    doc = nlp(sent)\n",
    "    kw1_span = find_span(doc, kw1, True, True)[0]\n",
    "    kw2_span = find_span(doc, kw2, True, True)[0]\n",
    "    path = find_dependency_info_from_tree(doc, kw1_span, kw2_span)\n",
    "    context = []\n",
    "    temp = []\n",
    "    for i, checked in enumerate(path):\n",
    "        if checked:\n",
    "            temp.append(doc[i].text)\n",
    "        else:\n",
    "            if temp:\n",
    "                context.append(' '.join(temp))\n",
    "                temp = []\n",
    "    if temp:\n",
    "        context.append(' '.join(temp))\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The economy of California , with a gross state product of $ 3.2 trillion as of 2019 , is the largest sub - national economy in the world .']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "informativeness_demo('The economy of California, with a gross state product of $3.2 trillion as of 2019, is the largest sub-national economy in the world.', 'economy of California', 'sub-national economy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('I love machine learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_sent(doc, path_set:set, kw1:str, kw2:str):\n",
    "    kw1_span = find_span(doc, kw1, True)\n",
    "    kw2_span = find_span(doc, kw2, True)\n",
    "    path = ''\n",
    "    for kw1_span in kw1_span:\n",
    "        for kw2_span in kw2_span:\n",
    "            path = find_dependency_path_from_tree(doc, kw1_span, kw2_span)\n",
    "            if path in path_set:\n",
    "                return path\n",
    "            path = ''\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path number: 6534\n",
      "141 paths have the frequency higher than 10\n"
     ]
    }
   ],
   "source": [
    "# Do general analysis about possible paths\n",
    "p2line_dict = {}\n",
    "freq = 10\n",
    "sents = my_read('data/temp_sents.txt')\n",
    "for i, line in enumerate(sents):\n",
    "    doc = nlp(line.strip())\n",
    "    l = [s for s in doc.noun_chunks if s[-1].pos_ != 'PRON']\n",
    "    if len(l) < 2:\n",
    "        continue\n",
    "    for j in range(len(l)-1):\n",
    "        for k in range(j, len(l)):\n",
    "            p = find_dependency_path_from_tree(doc, l[j], l[k])\n",
    "            if not p:\n",
    "                continue\n",
    "            if p not in p2line_dict:\n",
    "                p2line_dict[p] = []\n",
    "            p2line_dict[p].append({'kw1':str(l[j]), 'kw2':str(l[k]), 'line':i})\n",
    "\n",
    "print(\"path number:\", len(p2line_dict))\n",
    "freq_p = [k for k, c in p2line_dict.items() if len(c) > freq]\n",
    "print(len(freq_p), \"paths have the frequency higher than\", freq)\n",
    "my_write('data/freq_path_10.txt', freq_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sentences containing specific path\n",
    "p = 'prep pobj conj'\n",
    "my_write('data/'+p, ['%s\\t\\t%s\\t\\t%s' % (d['kw1'], d['kw2'], sents[d['line']]) for d in p2line_dict[p]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Machine learning algorithms, a model, sample data, training data, order, predictions, decisions]\n"
     ]
    }
   ],
   "source": [
    "# Examine on sentence\n",
    "s = 'Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so.'\n",
    "doc = nlp(s)\n",
    "l = list(doc.noun_chunks)   \n",
    "print(l)\n",
    "# print(get_phrase_full_span(doc, doc[-2:-1]))\n",
    "# find_dependency_path_from_tree(doc, l_[0], l_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_retriever = SparseRetrieveSentForPairCoOccur('../data/corpus/small_sent.txt', '../joint_score_func/data/occur.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw1 = 'database'\n",
    "kw2 = 'data mining'\n",
    "path_set = set(my_read('paths.txt'))\n",
    "sents = sparse_retriever.retrieve(kw1, kw2)\n",
    "df = pd.DataFrame({'sent':sents})\n",
    "df['doc'] = df.apply(lambda x: nlp(x['sent']), axis=1)\n",
    "df['path'] = df.apply(lambda x: examine_sent(x['doc'], path_set, kw1, kw2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_write('temp.txt', sents)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e687002bc60377ae87b855adfe470e827b4be244d7382e97081511de02b6558"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('FWD': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
