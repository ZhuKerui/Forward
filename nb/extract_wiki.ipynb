{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences from Wikipedia\n",
    "+ This notebook is used for collecting sentences that tell relationship between two entities from wikipedia using some dependency path pattern\n",
    "+ **This notebook is fully valid under Owl3 machine (using the /scratch/data/wikipedia/full_text-2021-03-20 data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki sub folder example: ../../data/wikipedia/full_text-2021-03-20/BE\n",
      "save sub folder example: data/extract_wiki/wiki_sent_collect/BE\n",
      "wiki file example: ../../data/wikipedia/full_text-2021-03-20/BE/wiki_00\n",
      "save sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.dat\n",
      "save cooccur file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_co.dat\n",
      "save selected sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_se.dat\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wikipedia\n",
    "import os\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "from collections import Counter\n",
    "import bz2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from tools.BasicUtils import my_read, my_write, my_read_pickle, my_write_pickle\n",
    "from tools.TextProcessing import (\n",
    "                my_sentence_tokenize, build_word_tree_v2, \n",
    "                my_sentence_tokenize, filter_specific_keywords, find_dependency_path_from_tree, find_span, nlp, \n",
    "                exact_match\n",
    "                )\n",
    "\n",
    "from extract_wiki import (\n",
    "    wikipedia_entity_file, record_columns, \n",
    "    save_path, entity_occur_file, graph_file, single_sent_graph_file, \n",
    "    w2vec_dump_file, w2vec_keyword_file, w2vec_wordtree_file, w2vec_token_file, \n",
    "    w2vec_keyword2idx_file, \n",
    "    test_path, path_test_file, \n",
    "    path_pattern_count_file, \n",
    "    save_sub_folders, wiki_sub_folders, \n",
    "    wiki_files, save_sent_files, save_cooccur_files, save_selected_files, save_title_files, save_cooccur__files, \n",
    "    p, patterns, \n",
    "    note2line, line2note, process_file, filter_path_from_df, find_all_triangles, \n",
    "    feature_columns, feature_process, gen_pattern, gen_kw_from_wiki_ent, get_entity_page\n",
    ")\n",
    "\n",
    "# Generate the save dir\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.mkdir(test_path)\n",
    "\n",
    "for save_dir in save_sub_folders:\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "# Get all files under wikipedia/full_text-2021-03-20\n",
    "\n",
    "print('wiki sub folder example:', wiki_sub_folders[0])\n",
    "print('save sub folder example:', save_sub_folders[0])\n",
    "print('wiki file example:', wiki_files[0])\n",
    "print('save sentence file example:', save_sent_files[0])\n",
    "print('save cooccur file example:', save_cooccur_files[0])\n",
    "print('save selected sentence file example:', save_selected_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keruiz2/miniconda3/envs/FWD_py38/lib/python3.8/contextlib.py:113: UserWarning: \"<bz2.BZ2File object at 0x7f4eb0519280>\" is not a raw file, mmap_mode \"c\" flag will be ignored.\n",
      "  return next(self.gen)\n"
     ]
    }
   ],
   "source": [
    "# [Load] wikipedia2vec\n",
    "with bz2.open(w2vec_dump_file) as f_in:\n",
    "    w2vec = Wikipedia2Vec.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] wikipedia2vec\n",
    "\n",
    "# Find similar words or entities\n",
    "# ent1 = 'Python (programming language)'\n",
    "# w2vec.most_similar_by_vector(w2vec.get_entity_vector(ent1), 20)\n",
    "\n",
    "# Get similarity between two entities\n",
    "# ent1 = 'Data mining'\n",
    "# ent2 = 'Database system'\n",
    "# cosine_similarity(w2vec.get_entity_vector(ent1).reshape(1, -1), w2vec.get_entity_vector(ent2).reshape(1, -1))[0, 0]\n",
    "\n",
    "# Check the entity count and document count\n",
    "ent1 = 'Hidden Markov model'\n",
    "e = w2vec.get_entity(ent1)\n",
    "print(e.count)\n",
    "print(e.doc_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect sentences, entities, entities co-occurrances, titles from wikipedia dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py collect_sent_and_cooccur (8 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect wikipedia entities\n",
    "wikipedia_entity = set()\n",
    "for f in tqdm.tqdm(save_title_files):\n",
    "    with open(f) as f_in:\n",
    "        wikipedia_entity.update(f_in.read().split('\\n'))\n",
    "print(len(wikipedia_entity))\n",
    "my_write(wikipedia_entity_file, list(wikipedia_entity))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Correct entity mapping in co-occurrance files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_entity_low2orig_map_file = 'data/extract_wiki/wikipedia_entity_low2orig_map.pickle'\n",
    "wikipedia_entity_low2orig_map = {}\n",
    "with open(wikipedia_entity_file) as f_in:\n",
    "    wikipedia_entity = set(f_in.read().split('\\n'))\n",
    "    for ent in tqdm.tqdm(wikipedia_entity):\n",
    "        ent_low = ent.lower()\n",
    "        if ent_low not in wikipedia_entity_low2orig_map:\n",
    "            wikipedia_entity_low2orig_map[ent_low] = []\n",
    "        wikipedia_entity_low2orig_map[ent_low].append(ent)\n",
    "my_write_pickle(wikipedia_entity_low2orig_map_file, wikipedia_entity_low2orig_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_entity_low2orig_map = my_read_pickle(wikipedia_entity_low2orig_map_file)\n",
    "    \n",
    "with open(wikipedia_entity_file) as f_in:\n",
    "    wikipedia_entity = set(f_in.read().split('\\n'))\n",
    "    \n",
    "for i in tqdm.tqdm(range(len(save_cooccur_files))):\n",
    "    with open(save_cooccur_files[i]) as f_in:\n",
    "        new_file_lines = []\n",
    "        for line_idx, line in enumerate(f_in):\n",
    "            line = line.strip()\n",
    "            entities = line.split('\\t')\n",
    "            new_entities = []\n",
    "            for ent in entities:\n",
    "                if ent in wikipedia_entity:\n",
    "                    new_entities.append(ent)\n",
    "                else:\n",
    "                    ent_low = ent.lower()\n",
    "                    if ent_low in wikipedia_entity_low2orig_map:\n",
    "                        candidates = wikipedia_entity_low2orig_map[ent_low]\n",
    "                        if len(candidates) == 1:\n",
    "                            new_entities.append(candidates[0])\n",
    "                        else:\n",
    "                            note = line2note(save_cooccur_files[i], line_idx, '_co.dat')\n",
    "                            page_title = note2line(note, '_ti.dat').strip()\n",
    "                            try:\n",
    "                                page_ent_vec = w2vec.get_entity_vector(page_title)\n",
    "                            except:\n",
    "                                continue\n",
    "                            most_similar_idx, most_similar_val = -1, -1\n",
    "                            for candidate_idx, candidate_ent in enumerate(candidates):\n",
    "                                try:\n",
    "                                    candidate_vec = w2vec.get_entity_vector(candidate_ent)\n",
    "                                except:\n",
    "                                    continue\n",
    "                                similar_val = cosine_similarity(page_ent_vec.reshape(1, -1), candidate_vec.reshape(1, -1))[0,0]\n",
    "                                if similar_val > most_similar_val:\n",
    "                                    most_similar_val = similar_val\n",
    "                                    most_similar_idx = candidate_idx\n",
    "                            if most_similar_idx >= 0:\n",
    "                                new_entities.append(candidates[most_similar_idx])\n",
    "            new_file_lines.append('\\t'.join(new_entities))\n",
    "        my_write(save_cooccur__files[i], new_file_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test]\n",
    "lines = get_entity_page('Machine learning')\n",
    "sents = [note2line(note) for note in lines]\n",
    "occurs = [note2line(note, '_co_.dat') for note in lines]\n",
    "ori_occurs = [note2line(note, '_co.dat') for note in lines]\n",
    "my_write('sent_check.txt', sents)\n",
    "my_write('occur_check.txt', occurs)\n",
    "my_write('ori_occur_check.txt', ori_occurs)\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Mapping keyword mention to wikipedia2vec entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wikipedia_entity_file) as f_in:\n",
    "    wikipedia_entity = set(f_in.read().split('\\n'))\n",
    "    \n",
    "w2vec_keyword2idx = {}\n",
    "\n",
    "for entity in tqdm.tqdm(wikipedia_entity):\n",
    "    w2vec_entity = w2vec.get_entity(entity)\n",
    "    if w2vec_entity is None:\n",
    "        continue\n",
    "    kw = gen_kw_from_wiki_ent(entity)\n",
    "    if kw not in w2vec_keyword2idx:\n",
    "        w2vec_keyword2idx[kw] = [w2vec_entity.index]\n",
    "    else:\n",
    "        if w2vec_entity.index not in w2vec_keyword2idx[kw]:\n",
    "            w2vec_keyword2idx[kw].append(w2vec_entity.index)\n",
    "w2vec_kws = filter_specific_keywords(list(w2vec_keyword2idx.keys()))\n",
    "my_write(w2vec_keyword_file, w2vec_kws)\n",
    "build_word_tree_v2(w2vec_keyword_file, w2vec_wordtree_file, w2vec_token_file)\n",
    "filter_keyword_from_w2vec = set(w2vec_kws)\n",
    "w2vec_keyword2idx = {k:v for k, v in w2vec_keyword2idx.items() if k in filter_keyword_from_w2vec}\n",
    "my_write_pickle(w2vec_keyword2idx_file, w2vec_keyword2idx)\n",
    "len(w2vec_keyword2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] w2vec_keyword2idx\n",
    "w2vec_keyword2idx = my_read_pickle(w2vec_keyword2idx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] w2vec_keyword2idx\n",
    "kw = 'feature engineering'\n",
    "kw_in_mention = kw in w2vec_keyword2idx\n",
    "print(kw_in_mention)\n",
    "if kw_in_mention:\n",
    "    for idx in w2vec_keyword2idx[kw]:\n",
    "        print(w2vec.dictionary.get_item_by_index(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] Collect sample data using general wikipedia2vec keywords and wiki sent files\n",
    "wiki_path_test_df = pd.concat([pd.DataFrame(process_file(save_sent_files[file_idx], save_cooccur__files[file_idx], w2vec)) for file_idx in range(8)], ignore_index=True)\n",
    "wiki_path_test_df.to_csv(path_test_file, sep='\\t', columns=feature_columns, index=False)\n",
    "print(len(wiki_path_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Load path test data (pd.DataFrame)\n",
    "wiki_path_test_df = pd.read_csv(open(path_test_file), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] Pattern frequency generation\n",
    "\n",
    "sub_df = wiki_path_test_df[wiki_path_test_df['sim'] > 0.5]\n",
    "\n",
    "sub_df = sub_df.assign(pick = sub_df.apply(lambda x: 1 if 'nsubj' in x['dep_path'] else 0, axis=1))\n",
    "sub_df = sub_df[sub_df['pick'] > 0]\n",
    "\n",
    "sub_df['pattern'] = sub_df.apply(lambda x: gen_pattern(x['dep_path']), axis=1)\n",
    "\n",
    "c = Counter(sub_df['pattern'].to_list())\n",
    "\n",
    "my_write_pickle(path_pattern_count_file, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] cal_freq function\n",
    "\n",
    "c = my_read_pickle(path_pattern_count_file)\n",
    "\n",
    "max_cnt = c.most_common(1)[0][1]\n",
    "log_max_cnt = np.log(max_cnt+1)\n",
    "\n",
    "def cal_freq(path:str):\n",
    "    cnt = c.get(path)\n",
    "    cnt = (cnt if cnt else 0.5) + 1\n",
    "    return np.log(cnt) / log_max_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] cal_freq function\n",
    "cal_freq('i_nsubj prep pobj prep pobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] Collect sample data using general wikipedia2vec keywords and wiki sent files\n",
    "sub_df = filter_path_from_df(wiki_path_test_df, cal_freq)\n",
    "sub_df = sub_df.assign(sent = sub_df.apply(lambda x: note2line(x['sent']).strip(), axis=1))\n",
    "sub_df.to_csv('full_phrase_check.tsv', columns = ['sent', 'kw1', 'kw1_recall', 'kw1_full_span', 'kw2', 'kw2_recall', 'kw2_full_span'], sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] collect dataset [collect_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Prepration] Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the graph ['generate_graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the entity occurance ['collect_ent_occur_from_selected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] entity occur dict which has occurance record for all entities in selected sentences\n",
    "entity_occur = my_read_pickle(entity_occur_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo check co-occur of two entities in selected sentences\n",
    "def get_selected_record(entity_dict:dict, ent1:str, ent2:str):\n",
    "    kw1_occur = entity_dict.get(ent1)\n",
    "    kw2_occur = entity_dict.get(ent2)\n",
    "    if not kw1_occur or not kw2_occur:\n",
    "        return None\n",
    "    co_occur = kw1_occur & kw2_occur\n",
    "    data = []\n",
    "    for occur in co_occur:\n",
    "        record = note2line(occur, '_se.dat').strip().split('\\t')\n",
    "        sent = note2line(record[7]).strip()\n",
    "        data_dict = {record_columns[i] : record[i] for i in range(len(record))}\n",
    "        data_dict['sent'] = sent\n",
    "        data.append(data_dict)\n",
    "    \n",
    "    df = pd.DataFrame(data = data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] entity occur dict\n",
    "\n",
    "ent1 = 'Artificial intelligence'\n",
    "# Check the existance of an entity\n",
    "ent1 in entity_occur\n",
    "\n",
    "# Check the sentences where an entity appear\n",
    "# for note in entity_occur[ent1]:\n",
    "#     print(note2line(note2line(note, '_se.dat').split('\\t')[7]).strip())\n",
    "\n",
    "# Check the records of two entities\n",
    "ent2 = 'Natural language processing'\n",
    "df = get_selected_record(entity_occur, ent1, ent2)\n",
    "if df is not None:\n",
    "    print(len(df))\n",
    "    df.to_csv('sents.tsv', columns=['score'] + feature_columns + ['pattern', 'pattern_freq'], sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Graph\n",
    "graph = my_read_pickle(graph_file)\n",
    "\n",
    "print('num of nodes:', len(graph.nodes))\n",
    "print('num of edges:', len(graph.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] graph\n",
    "ent1 = 'Data mining'\n",
    "# Check the neighbours of an entity\n",
    "# list(graph.neighbors(ent1))\n",
    "\n",
    "# Check the edges of two entities\n",
    "graph.edges[ent1, 'Data fusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sentence from file\n",
    "note2line('AC:32:2390')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2neig_cnt = {node : len(list(graph.neighbors(node))) for node in graph.nodes.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neig_cnt = [v for v in node2neig_cnt.values() if v < 20]\n",
    "plt.title('num of nodes vs num of neighbors each node')\n",
    "plt.hist(neig_cnt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2triangle_num = nx.triangles(graph)\n",
    "my_write_pickle('node2tri_num.pickle', node2triangle_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num of triangles:', sum(node2triangle_num.values()) / 3)\n",
    "plt.title('num of nodes vs num of triangles each node')\n",
    "plt.hist([v for v in node2triangle_num.values() if v >= 1 and v < 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data from triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Single sentence graph\n",
    "single_sent_graph = my_read_pickle(single_sent_graph_file)\n",
    "edges = [edge for edge in tqdm.tqdm(single_sent_graph.edges) if single_sent_graph.get_edge_data(*edge)['score'] > 0.65]\n",
    "filtered_graph = single_sent_graph.edge_subgraph(edges)\n",
    "nodes = []\n",
    "for node in tqdm.tqdm(filtered_graph):\n",
    "    ent = w2vec.get_entity(node)\n",
    "    if ent is None:\n",
    "        continue\n",
    "    if ent.count >= 20:\n",
    "        nodes.append(node)\n",
    "filtered_graph = filtered_graph.subgraph(nodes)\n",
    "print('number of nodes:', filtered_graph.number_of_nodes())\n",
    "print('number of edges:', filtered_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk.corpus import stopwords\n",
    "self_define_stopwords = set(['-', ',', '.'])\n",
    "sw = set(stopwords.words('english'))\n",
    "import math\n",
    "\n",
    "def find_triangles(graph:nx.Graph, node:str):\n",
    "    triangles = set()\n",
    "    neighbors = set(graph.neighbors(node))\n",
    "    for neighbor in neighbors:\n",
    "        second_neighbors = set(graph.neighbors(neighbor))\n",
    "        inter_neighbors = neighbors & second_neighbors\n",
    "        for third_neighbor in inter_neighbors:\n",
    "            triangles.add((node, neighbor, third_neighbor) if neighbor < third_neighbor else (node, third_neighbor, neighbor))\n",
    "    return triangles\n",
    "\n",
    "\n",
    "def find_triangle_with_node(graph:nx.Graph, first_node:str, second_node:str='', third_node:str=''):\n",
    "    triangles = list(find_triangles(graph, first_node))\n",
    "    triangles.sort(key=lambda x: x[1])\n",
    "    triangle_with_sents = []\n",
    "    n_seen = set()\n",
    "    for n1, n2, n3 in triangles:\n",
    "        if second_node and n2 != second_node and n3 != second_node:\n",
    "            continue\n",
    "        if third_node and n2 != third_node and n3 != third_node:\n",
    "            continue\n",
    "        if n2 not in n_seen:\n",
    "            n_seen.add(n2)\n",
    "            triangle_with_sents.append((n1, note2line(graph.get_edge_data(n1, n2)['note']).strip(), n2, graph.get_edge_data(n1, n2)['score']))\n",
    "        if n3 not in n_seen:\n",
    "            n_seen.add(n3)\n",
    "            triangle_with_sents.append((n1, note2line(graph.get_edge_data(n1, n3)['note']).strip(), n3, graph.get_edge_data(n1, n3)['score']))\n",
    "        triangle_with_sents.append((n2, note2line(graph.get_edge_data(n3, n2)['note']).strip(), n3, graph.get_edge_data(n3, n2)['score']))\n",
    "    return triangle_with_sents\n",
    "\n",
    "\n",
    "def isf(w:str, D:int, counters:List[Counter]):\n",
    "    return math.log(D * 1.0 / sum([1 if w in sent else 0 for sent in counters]))\n",
    "\n",
    "\n",
    "def do_pagerank(sents:List[str]):\n",
    "    # Remove stop words\n",
    "    clean_sents = [[token for token in sent.split() if token not in sw and token not in self_define_stopwords] for sent in sents]\n",
    "\n",
    "    # Generate word counters\n",
    "    counters = [Counter(sent) for sent in clean_sents]\n",
    "\n",
    "    # Build similarity matrix\n",
    "    D = len(clean_sents)\n",
    "    sim_matrix = np.zeros((D, D))\n",
    "    part_list = [math.sqrt(sum([(sent[w] * isf(w, D, counters)) ** 2 for w in sent])) for sent in counters]\n",
    "    # return part_list\n",
    "    for i in range(D - 1):\n",
    "        for j in range(i + 1, D):\n",
    "            sent_1 = counters[i]\n",
    "            sent_2 = counters[j]\n",
    "            share_word_set = sent_1 & sent_2\n",
    "            numerator = sum([(sent_1[w] * sent_2[w] * (isf(w, D, counters) ** 2)) for w in share_word_set])\n",
    "            denominator = part_list[i] * part_list[j]\n",
    "            sim_matrix[i, j] = numerator / denominator\n",
    "    sim_matrix = sim_matrix + sim_matrix.T\n",
    "    g = nx.from_numpy_array(sim_matrix)\n",
    "    score = nx.pagerank(g)\n",
    "    temp = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "    idx = [item[0] for item in temp]\n",
    "    return [sents[i] for i in idx], [score[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine learning',\n",
       "  'artificial neural network ( ann ) is a subfield of the research area machine learning .',\n",
       "  'Artificial neural network',\n",
       "  0.846597576357714),\n",
       " ('Machine learning',\n",
       "  'deep learning ( also known as deep structured learning ) is part of a broader family of machine learning method based on artificial neural network with representation learning .',\n",
       "  'Deep learning',\n",
       "  0.7140114888712812),\n",
       " ('Artificial neural network',\n",
       "  'deep learning consists of multiple hidden layer in an artificial neural network .',\n",
       "  'Deep learning',\n",
       "  0.8960311100255893)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_triangles = find_triangle_with_node(filtered_graph, 'Machine learning', 'Artificial neural network', 'Deep learning')\n",
    "test_triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.48648582432442083,\n",
       "  'deep learning ( also known as deep structured learning ) is part of a broader family of machine learning method based on artificial neural network with representation learning .'),\n",
       " (0.29493351239384963,\n",
       "  'artificial neural network ( ann ) is a subfield of the research area machine learning .'),\n",
       " (0.21858066328172915,\n",
       "  'deep learning consists of multiple hidden layer in an artificial neural network .')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list = [triangle[1] for triangle in test_triangles]\n",
    "sents, score = do_pagerank(sent_list)\n",
    "list(zip(score, sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentence\n",
    "doc = nlp('sephardi were exempt from the ban , but it appears that few applied for a letter of free passage .')\n",
    "\n",
    "# Check noun phrases in the sentences\n",
    "print(list(doc.noun_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('ada is a structured , statically typed , imperative , and object-oriented high-level programming language , extended from pascal and other language .')\n",
    "pairs = [{'kw1' : 'ada', 'kw2' : 'programming language'}]\n",
    "feature_process(doc, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    remove_list = ['See also', 'References', 'Further reading', 'Sources', 'External links']\n",
    "    dic = {sec : page.section(sec) for sec in page.sections}\n",
    "    dic['summary'] = page.summary\n",
    "    sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    while len(section_list) > 0:\n",
    "        section = section_list.pop()\n",
    "        if section in remove_list:\n",
    "            continue\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        # processed_text = clean_text(section_text)\n",
    "        processed_text = ' '.join(section_text.lower().split())\n",
    "        temp_sents = my_sentence_tokenize(processed_text, True)\n",
    "        sents += temp_sents\n",
    "    return list(sents)\n",
    "\n",
    "def collect_entity_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    return [text.lower() for text in page.links]\n",
    "\n",
    "def collect_keyword_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    soup = BeautifulSoup(page.html(), 'html.parser')\n",
    "    main_block = soup.find('div', class_='mw-parser-output')\n",
    "    keywords = set([l.text.lower() for l in main_block.findAll('a') if re.match(r'^(<a href=\"/wiki/)', str(l))])\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'python'\n",
    "\n",
    "p = wikipedia.page(keyword)\n",
    "if p is not None:\n",
    "    sents = collect_sents_from_wiki_page(p)\n",
    "    keywords = collect_keyword_from_wiki_page(p)\n",
    "    print('sentences collected')\n",
    "    my_write('%s.txt' % keyword, sents)\n",
    "    my_write('%s_kw.txt' % keyword, keywords)\n",
    "    df = filter_by_path(sents)\n",
    "    df.to_csv('%s_out.tsv' % keyword, sep='\\t', index=False)\n",
    "\n",
    "    dff = df[df.apply(lambda x: str(x['head']) in keywords and str(x['tail']) in keywords, axis=1)]\n",
    "    dff.to_csv('%s_out_f.tsv' % keyword, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-crafted analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_df = wiki_path_test_df[wiki_path_test_df['sim'] >= 0.0]\n",
    "\n",
    "def match_path_pattern(path:str):\n",
    "    for pp in patterns:\n",
    "        if exact_match(pp, path):\n",
    "            return pp\n",
    "    return ''\n",
    "\n",
    "wiki_test_df['pattern'] = wiki_test_df.apply(lambda x: match_path_pattern(x['path']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_path_result_sim_based(df:pd.DataFrame, paths:list):\n",
    "    summary_df = pd.DataFrame(columns=['path', 'cnt', 'ratio', 'avg_sim'])\n",
    "    for pp in paths:\n",
    "        sub_df = df[df['pattern'] == pp]\n",
    "        summary_df = summary_df.append({\n",
    "            'path' : pp,\n",
    "            'cnt' : len(sub_df),\n",
    "            'ratio' : len(sub_df) / len(df),\n",
    "            'avg_sim' : sum(sub_df['sim']) / len(sub_df) if len(sub_df) else 0\n",
    "        }, ignore_index=True)\n",
    "    summary_df = summary_df.append({\n",
    "        'path' : 'general',\n",
    "        'cnt' : len(df),\n",
    "        'ratio' : 1,\n",
    "        'avg_sim' : sum(df['sim']) / len(df) if len(df) else 0\n",
    "    }, ignore_index=True)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path_result_sim_based(wiki_test_df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_example_sent_for_pattern(df:pd.DataFrame, path:str, num:int=30, posfix:str='.dat'):\n",
    "    sub_df = df[df['pattern'] == path]\n",
    "    num = min(len(sub_df), num)\n",
    "    sub_df = sub_df[:num]\n",
    "    sub_df['sent'] = sub_df.apply(lambda x: note2line(x['sent'], posfix=posfix).strip(), axis=1)\n",
    "    return sub_df\n",
    "\n",
    "for patt in patterns:\n",
    "    temp_df = collect_example_sent_for_pattern(wiki_test_df, patt)\n",
    "    temp_df.to_csv('%s.tsv' % (patt[:10] if len(patt) >= 10 else patt), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_set = my_read_pickle('data/extract_wiki/triangles.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "818486"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triangle_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'Kennebec County, Maine', 'Waterville, Maine', 'Maine'})\n",
      "frozenset({'Inorganic compound', 'Lithium carbonate', 'Sodium carbonate'})\n",
      "frozenset({'Sixth form', \"Alleyn's School\", 'Church of England'})\n",
      "frozenset({'Illinois', 'Texas', 'Ohio'})\n",
      "frozenset({'Star', 'Exoplanet', 'Substellar object'})\n",
      "frozenset({'John Scofield', 'Michel Jonasz', 'Michel Portal'})\n",
      "frozenset({'Montreal', 'Quebec', 'La Tuque, Quebec'})\n",
      "frozenset({'Star', 'Solar radius', 'Sun'})\n",
      "frozenset({'Plagioclase', 'Olivine', 'Quartz'})\n",
      "frozenset({'First Army (Bulgaria)', 'Debar', 'Struga'})\n",
      "frozenset({'Oak Ridge, Tennessee', 'Knoxville, Tennessee', 'Roane County, Tennessee'})\n",
      "frozenset({'Transcaucasia', 'North Africa', 'Middle East'})\n"
     ]
    }
   ],
   "source": [
    "for i, tri in enumerate(triangle_set):\n",
    "    print(tri)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i, tri in enumerate(triangle_set):\n",
    "    samples.append(find_triangle_with_node(single_sent_graph, *tri))\n",
    "    if i > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Kennebec County, Maine',\n",
       "   'kennebec county is a county in the u.s. state of maine , in the united state .',\n",
       "   'Maine',\n",
       "   0.7699937343936527),\n",
       "  ('Kennebec County, Maine',\n",
       "   'waterville is located in northern kennebec county at .',\n",
       "   'Waterville, Maine',\n",
       "   0.8214094636421466),\n",
       "  ('Maine',\n",
       "   'maine central operated parallel main line between royal junction and waterville .',\n",
       "   'Waterville, Maine',\n",
       "   0.7669628446642484)],\n",
       " [('Inorganic compound',\n",
       "   'lithium carbonate is an inorganic compound , the lithium salt of carbonate with the chemical formula .',\n",
       "   'Lithium carbonate',\n",
       "   0.6549696543360262),\n",
       "  ('Inorganic compound',\n",
       "   'sodium carbonate , , ( also known as washing soda , soda ash and soda crystal ) is the inorganic compound with the formula naco and its various hydrate .',\n",
       "   'Sodium carbonate',\n",
       "   0.7255028023549482),\n",
       "  ('Lithium carbonate',\n",
       "   'unlike sodium carbonate , which forms at least three hydrate , lithium carbonate exists only in the anhydrous form .',\n",
       "   'Sodium carbonate',\n",
       "   0.756178595191623)],\n",
       " [('Sixth form',\n",
       "   \"alleyn 's school is a 4–18 co-educational , independent , church of england , day school and sixth form in dulwich , london , england .\",\n",
       "   \"Alleyn's School\",\n",
       "   0.772191203416505),\n",
       "  ('Sixth form',\n",
       "   'the deanery church of england high school and sixth form college is a coeducational secondary school and sixth form located in wigan , greater manchester , england .',\n",
       "   'Church of England',\n",
       "   0.7160044735034357),\n",
       "  (\"Alleyn's School\",\n",
       "   \"alleyn 's school is a 4–18 co-educational , independent , church of england , day school and sixth form in dulwich , london , england .\",\n",
       "   'Church of England',\n",
       "   0.7071456620203787)],\n",
       " [('Illinois',\n",
       "   'ohio contributed more than 34,000 troop , and was the only one of the five participating state ( the others were indiana , illinois , iowa and wisconsin ) to exceed its quota .',\n",
       "   'Ohio',\n",
       "   0.7214604693115204),\n",
       "  ('Illinois',\n",
       "   \"illinois , maine , oregon , rhode island , texas , alaska and minnesota have elected formally independent candidate as governor : illinois 's first two governor , shadrach bond and edward cole ; james b. longley in 1974 as well as angus king in 1994 and 1998 from maine ; lincoln chafee in 2010 from rhode island ; julius meier in 1930 from oregon ; sam houston in 1859 from texas ; and bill walker in 2014 from alaska .\",\n",
       "   'Texas',\n",
       "   0.7777485800137448),\n",
       "  ('Ohio',\n",
       "   \"colorado , mississippi , missouri , new york , ohio , oregon , and washington do so only for certain type of license plate , such as vanity plate and special issue ; alabama , arizona , idaho , indiana , iowa , kansa , minnesota , montana , nebraska , new jersey , north dakota , south carolina , south dakota , tennessee , texas , wyoming , and the district of columbia have switched to the so-called `` flat plate '' technology for all their license plate .\",\n",
       "   'Texas',\n",
       "   0.7276824681478335)],\n",
       " [('Star',\n",
       "   'an exoplanet has been detected in a close orbit around the star .',\n",
       "   'Exoplanet',\n",
       "   0.8137214476504528),\n",
       "  ('Star',\n",
       "   'a substellar object may be a companion of a star , such as exoplanet or brown dwarf that is orbiting a star .',\n",
       "   'Substellar object',\n",
       "   0.6728430549582403),\n",
       "  ('Exoplanet',\n",
       "   'a substellar object may be a companion of a star , such as exoplanet or brown dwarf that is orbiting a star .',\n",
       "   'Substellar object',\n",
       "   0.7188596052966663)],\n",
       " [('John Scofield',\n",
       "   'youn sun nah , gil evans , david sanborn , randy brecker , charlie haden & liberation orchestra , john scofield , michel portal , yaron herman , leszek mozdzer , joachim kuhn , markus stockhausen , christof lauer , aldo romano , rick margitza , baptiste trottignon , tania maria , paolo fresu , orchestre national de jazz onj , jacky terrasson , bugge wesseltoft , daniel yvinnec , ulf wakenius , lars danielsson , niels lan doky , nils landgren , laurent cugny & big band lumière , manu dibango , jazz baltica ensemble , wdr symphonic orchestra , tokyo symphony orchestra , tivoli symphonic orchestra , les rita mitsouko , victoria tolstoï , caecillie norby , gino vanelli , toot thielmans , claude nougaro , cyril atef , didier lockwood , nil peter molvaer , jan bang , andy emler , cheb mami , safy boutela , baaziz , dominic miller , jean-philippe collard-neven , jean-louis rassinfosse , fabrice alleman , juliette gréco , okay temiz , prabhu edouard , bobo stenson , dave liebman , jean-pierre ma , jon balke , john paricelli , vincent segal , hopen collective , alain jean-marie , louis winsberg , terri lyne carrington , pierre boussaguet , jacques vidal , pierrick pedron , paul wertico , palle mikelborg , leon parker , boy gé mendes , teco cardoso , françois moutin , lelo nazario , dudu trentin , stephane belmondo , lionel belmondo , eric legnini , stephano di battista , julia migenes , edouard ferlet , mads winding , alex riel , baba maal , omar sosa , nguyen lé , courtney pine , london gospel choir , michel jonasz , crazy b , jean-phi dary , baco mikaelian , bireli lagrène , al jarreau , dianne reef , bill evans , john scofield , laurent garnier .',\n",
       "   'Michel Jonasz',\n",
       "   0.6639558037450338),\n",
       "  ('John Scofield',\n",
       "   'youn sun nah , gil evans , david sanborn , randy brecker , charlie haden & liberation orchestra , john scofield , michel portal , yaron herman , leszek mozdzer , joachim kuhn , markus stockhausen , christof lauer , aldo romano , rick margitza , baptiste trottignon , tania maria , paolo fresu , orchestre national de jazz onj , jacky terrasson , bugge wesseltoft , daniel yvinnec , ulf wakenius , lars danielsson , niels lan doky , nils landgren , laurent cugny & big band lumière , manu dibango , jazz baltica ensemble , wdr symphonic orchestra , tokyo symphony orchestra , tivoli symphonic orchestra , les rita mitsouko , victoria tolstoï , caecillie norby , gino vanelli , toot thielmans , claude nougaro , cyril atef , didier lockwood , nil peter molvaer , jan bang , andy emler , cheb mami , safy boutela , baaziz , dominic miller , jean-philippe collard-neven , jean-louis rassinfosse , fabrice alleman , juliette gréco , okay temiz , prabhu edouard , bobo stenson , dave liebman , jean-pierre ma , jon balke , john paricelli , vincent segal , hopen collective , alain jean-marie , louis winsberg , terri lyne carrington , pierre boussaguet , jacques vidal , pierrick pedron , paul wertico , palle mikelborg , leon parker , boy gé mendes , teco cardoso , françois moutin , lelo nazario , dudu trentin , stephane belmondo , lionel belmondo , eric legnini , stephano di battista , julia migenes , edouard ferlet , mads winding , alex riel , baba maal , omar sosa , nguyen lé , courtney pine , london gospel choir , michel jonasz , crazy b , jean-phi dary , baco mikaelian , bireli lagrène , al jarreau , dianne reef , bill evans , john scofield , laurent garnier .',\n",
       "   'Michel Portal',\n",
       "   0.6876258017268999),\n",
       "  ('Michel Jonasz',\n",
       "   'youn sun nah , gil evans , david sanborn , randy brecker , charlie haden & liberation orchestra , john scofield , michel portal , yaron herman , leszek mozdzer , joachim kuhn , markus stockhausen , christof lauer , aldo romano , rick margitza , baptiste trottignon , tania maria , paolo fresu , orchestre national de jazz onj , jacky terrasson , bugge wesseltoft , daniel yvinnec , ulf wakenius , lars danielsson , niels lan doky , nils landgren , laurent cugny & big band lumière , manu dibango , jazz baltica ensemble , wdr symphonic orchestra , tokyo symphony orchestra , tivoli symphonic orchestra , les rita mitsouko , victoria tolstoï , caecillie norby , gino vanelli , toot thielmans , claude nougaro , cyril atef , didier lockwood , nil peter molvaer , jan bang , andy emler , cheb mami , safy boutela , baaziz , dominic miller , jean-philippe collard-neven , jean-louis rassinfosse , fabrice alleman , juliette gréco , okay temiz , prabhu edouard , bobo stenson , dave liebman , jean-pierre ma , jon balke , john paricelli , vincent segal , hopen collective , alain jean-marie , louis winsberg , terri lyne carrington , pierre boussaguet , jacques vidal , pierrick pedron , paul wertico , palle mikelborg , leon parker , boy gé mendes , teco cardoso , françois moutin , lelo nazario , dudu trentin , stephane belmondo , lionel belmondo , eric legnini , stephano di battista , julia migenes , edouard ferlet , mads winding , alex riel , baba maal , omar sosa , nguyen lé , courtney pine , london gospel choir , michel jonasz , crazy b , jean-phi dary , baco mikaelian , bireli lagrène , al jarreau , dianne reef , bill evans , john scofield , laurent garnier .',\n",
       "   'Michel Portal',\n",
       "   0.6623213013850927)],\n",
       " [('Montreal',\n",
       "   'between montreal and quebec , it is divided into six sub-areas or sector which correspond to the division of the three city and three regional county municipality : trois-rivières , shawinigan , la tuque , maskinongé , les chenaux regional county municipality , mekinac regional county municipality .',\n",
       "   'La Tuque, Quebec',\n",
       "   0.7221170947869502),\n",
       "  ('Montreal',\n",
       "   'montreal is also the cultural capital for english quebec .',\n",
       "   'Quebec',\n",
       "   0.8032473447345562),\n",
       "  ('La Tuque, Quebec',\n",
       "   'the la tuque generating station is a hydroelectric power plant , located on the saint-maurice river , at the height of the city of la tuque , in the province of quebec , in canada .',\n",
       "   'Quebec',\n",
       "   0.7174880861349225)],\n",
       " [('Star',\n",
       "   'the star has a solar mass that is 1.06 time that of the sun , and a radius 1.27 time the solar radius .',\n",
       "   'Solar radius',\n",
       "   0.8139688382279147),\n",
       "  ('Star',\n",
       "   'this star has an age of 4.5 billion year ; about the same as the sun .',\n",
       "   'Sun',\n",
       "   0.8305061972614838),\n",
       "  ('Solar radius',\n",
       "   'solar radius is a unit of distance used to express the size of star in astronomy relative to the sun .',\n",
       "   'Sun',\n",
       "   0.7035176496907187)],\n",
       " [('Plagioclase',\n",
       "   'augite , hypersthene and plagioclase are also present , with smaller content of olivine and quartz .',\n",
       "   'Olivine',\n",
       "   0.7626697474280225),\n",
       "  ('Plagioclase',\n",
       "   'quartz and smectite are the main mineral , along with illite , chlorite , and plagioclase in minor amount .',\n",
       "   'Quartz',\n",
       "   0.8239175346701947),\n",
       "  ('Olivine',\n",
       "   'olivine can be present along with apatite , and locally quartz .',\n",
       "   'Quartz',\n",
       "   0.7305527976167407)],\n",
       " [('First Army (Bulgaria)',\n",
       "   'since december 1915 the bulgarian first army consisted of the 8th , 9th and 3rd infantry division and the cavalry division and occupied a 140-kilometer front , from debar and struga to the bend of the river cherna and the vardar .',\n",
       "   'Debar',\n",
       "   0.68159857873721),\n",
       "  ('First Army (Bulgaria)',\n",
       "   'since december 1915 the bulgarian first army consisted of the 8th , 9th and 3rd infantry division and the cavalry division and occupied a 140-kilometer front , from debar and struga to the bend of the river cherna and the vardar .',\n",
       "   'Struga',\n",
       "   0.7020720093148797),\n",
       "  ('Debar',\n",
       "   \"debar ( ; in albanian ; `` dibër '' / '' dibra '' or `` dibra e madhe '' ) is a city in the western part of north macedonia , near the border with albania , off the road from struga to gostivar .\",\n",
       "   'Struga',\n",
       "   0.7639299142934485)],\n",
       " [('Oak Ridge, Tennessee',\n",
       "   'oak ridge is a city in anderson and roane county in the eastern part of the u.s. state of tennessee , about west of downtown knoxville .',\n",
       "   'Knoxville, Tennessee',\n",
       "   0.7227754883649026),\n",
       "  ('Oak Ridge, Tennessee',\n",
       "   'oak ridge is a city in anderson and roane county in the eastern part of the u.s. state of tennessee , about west of downtown knoxville .',\n",
       "   'Roane County, Tennessee',\n",
       "   0.6694630962135755),\n",
       "  ('Knoxville, Tennessee',\n",
       "   'roane county is included in the knoxville , tn metropolitan statistical area .',\n",
       "   'Roane County, Tennessee',\n",
       "   0.73861424541803)],\n",
       " [('Transcaucasia',\n",
       "   \"the broader concept of the `` greater middle east '' ( aka the middle east and north africa or the menap ) also includes the maghreb , sudan , djibouti , somalia , the comoros , afghanistan , pakistan , and sometimes transcaucasia and central asia into the region .\",\n",
       "   'Middle East',\n",
       "   0.7205499500775742),\n",
       "  ('Transcaucasia',\n",
       "   \"the broader concept of the `` greater middle east '' ( aka the middle east and north africa or the menap ) also includes the maghreb , sudan , djibouti , somalia , the comoros , afghanistan , pakistan , and sometimes transcaucasia and central asia into the region .\",\n",
       "   'North Africa',\n",
       "   0.7083499013381356),\n",
       "  ('Middle East',\n",
       "   'in north africa , 17 % of female are unemployed and 16 % of woman in the middle east are unemployed .',\n",
       "   'North Africa',\n",
       "   0.758240560132708)]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9496c91418be784f00ee6456e4343e8188c649322b68f201c83241a4029a42d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('FWD_py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
