{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences from Wikipedia\n",
    "+ This notebook is used for collecting sentences that tell relationship between two entities from wikipedia using some dependency path pattern\n",
    "+ **This notebook is fully valid under Owl3 machine (using the /scratch/data/wikipedia/full_text-2021-03-20 data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki sub folder example: ../../data/wikipedia/full_text-2021-03-20/BE\n",
      "save sub folder example: data/extract_wiki/wiki_sent_collect/BE\n",
      "wiki file example: ../../data/wikipedia/full_text-2021-03-20/BE/wiki_00\n",
      "save sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.dat\n",
      "save cooccur file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_co.dat\n",
      "save cs sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_cs.dat\n",
      "save selected sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_se.dat\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wikipedia\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "from collections import Counter\n",
    "import bz2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from tools.BasicUtils import my_read, my_write, MyMultiProcessing\n",
    "from tools.TextProcessing import (\n",
    "                normalize_text, remove_brackets, my_sentence_tokenize, build_word_tree_v2, \n",
    "                my_sentence_tokenize, filter_specific_keywords, find_dependency_path_from_tree, find_span, nlp, \n",
    "                sent_lemmatize, exact_match\n",
    "                )\n",
    "from tools.DocProcessing import CoOccurrence\n",
    "\n",
    "from extract_wiki import (\n",
    "    wikipedia_dir, wikipedia_entity_file, wikipedia_entity_norm_file, \n",
    "    wikipedia_keyword_file, wikipedia_token_file, wikipedia_wordtree_file, \n",
    "    save_path, entity_occur_file, keyword_connection_graph_file, w2vec_dump_file, w2vec_keyword_file, w2vec_wordtree_file, w2vec_token_file, \n",
    "    w2vec_keyword2idx_file, path_test_file, test_path, cs_keyword_file, cs_token_file, cs_wordtree_file, cs_path_test_file, cs_raw_keyword_file, \n",
    "    path_pattern_count_file, patterns, w2vec_entity_file, graph_file, \n",
    "    collect_wiki_entity, get_sentence, line2note, note2line,\n",
    "    feature_columns, feature_process, gen_pattern, cal_coverage\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create the multiprocessing object for future use\n",
    "p = MyMultiProcessing(10)\n",
    "\n",
    "# Generate the save dir\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.mkdir(test_path)\n",
    "\n",
    "sub_folders = [sub for sub in os.listdir(wikipedia_dir)]\n",
    "save_sub_folders = [os.path.join(save_path, sub) for sub in sub_folders]\n",
    "wiki_sub_folders = [os.path.join(wikipedia_dir, sub) for sub in sub_folders]\n",
    "\n",
    "wiki_files = []\n",
    "save_sent_files = []\n",
    "save_cooccur_files = []\n",
    "save_cs_sent_files = []\n",
    "save_selected_files = []\n",
    "\n",
    "for save_dir in save_sub_folders:\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "for i in range(len(wiki_sub_folders)):\n",
    "    files = [f for f in os.listdir(wiki_sub_folders[i])]\n",
    "    wiki_files += [os.path.join(wiki_sub_folders[i], f) for f in files]\n",
    "    save_sent_files += [os.path.join(save_sub_folders[i], f+'.dat') for f in files]\n",
    "    save_cooccur_files += [os.path.join(save_sub_folders[i], f+'_co.dat') for f in files]\n",
    "    save_cs_sent_files += [os.path.join(save_sub_folders[i], f+'_cs.dat') for f in files]\n",
    "    save_selected_files += [os.path.join(save_sub_folders[i], f+'_se.dat') for f in files]\n",
    "\n",
    "# Get all files under wikipedia/full_text-2021-03-20\n",
    "\n",
    "print('wiki sub folder example:', wiki_sub_folders[0])\n",
    "print('save sub folder example:', save_sub_folders[0])\n",
    "print('wiki file example:', wiki_files[0])\n",
    "print('save sentence file example:', save_sent_files[0])\n",
    "print('save cooccur file example:', save_cooccur_files[0])\n",
    "print('save cs sentence file example:', save_cs_sent_files[0])\n",
    "print('save selected sentence file example:', save_selected_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect sentences from wikipedia and select good sentences by path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sentences from wiki files (12 min)\n",
    "wiki_sent_pair = [(wiki_files[i], save_sent_files[i]) for i in range(len(wiki_files))]\n",
    "output = p.run(get_sentence, wiki_sent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keruiz2/miniconda3/envs/FWD_py38/lib/python3.8/contextlib.py:113: UserWarning: \"<bz2.BZ2File object at 0x7fdd147b7220>\" is not a raw file, mmap_mode \"c\" flag will be ignored.\n",
      "  return next(self.gen)\n"
     ]
    }
   ],
   "source": [
    "# [Load] wikipedia2vec\n",
    "with bz2.open(w2vec_dump_file) as f_in:\n",
    "    w2vec = Wikipedia2Vec.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] wikipedia2vec\n",
    "\n",
    "# Find similar words or entities\n",
    "# ent1 = 'Python (programming language)'\n",
    "# w2vec.most_similar_by_vector(w2vec.get_entity_vector(ent1), 20)\n",
    "\n",
    "# Get similarity between two entities\n",
    "ent1 = 'Python (programming language)'\n",
    "ent2 = 'Java (programming language)'\n",
    "cosine_similarity(w2vec.get_entity_vector(ent1).reshape(1, -1), w2vec.get_entity_vector(ent2).reshape(1, -1))\n",
    "\n",
    "# Check the entity count and document count\n",
    "# ent1 = 'The World (radio program)'\n",
    "# e = w2vec.get_entity(ent1)\n",
    "# print(e.count)\n",
    "# print(e.doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] my_mention_dict, mapping keyword mention to wikipedia2vec entities\n",
    "w2vec_entity = [w for w in w2vec.dictionary.entities() if w.count >= 50]\n",
    "w2vec_entity_title = [w.title for w in w2vec_entity]\n",
    "my_write(w2vec_entity_file, w2vec_entity_title)\n",
    "my_mention_dict = {}\n",
    "for ent in w2vec_entity:\n",
    "    kw = remove_brackets(normalize_text(ent.title))\n",
    "    if kw not in my_mention_dict:\n",
    "        my_mention_dict[kw] = [ent.index]\n",
    "    else:\n",
    "        my_mention_dict[kw].append(ent.index)\n",
    "w2vec_kws = filter_specific_keywords(list(my_mention_dict.keys()))\n",
    "my_write(w2vec_keyword_file, w2vec_kws)\n",
    "build_word_tree_v2(w2vec_keyword_file, w2vec_wordtree_file, w2vec_token_file)\n",
    "filter_keyword_from_w2vec = set(w2vec_kws)\n",
    "my_mention_dict = {k:v for k, v in my_mention_dict.items() if k in filter_keyword_from_w2vec}\n",
    "with open(w2vec_keyword2idx_file, 'wb') as f_out:\n",
    "    pickle.dump(my_mention_dict, f_out)\n",
    "len(my_mention_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] my_mention_dict\n",
    "with open(w2vec_keyword2idx_file, 'rb') as f_in:\n",
    "    my_mention_dict = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<Entity Database>\n"
     ]
    }
   ],
   "source": [
    "# [Test] my_mention_dict\n",
    "kw = 'database'\n",
    "kw_in_mention = kw in my_mention_dict\n",
    "print(kw_in_mention)\n",
    "if kw_in_mention:\n",
    "    for idx in my_mention_dict[kw]:\n",
    "        print(w2vec.dictionary.get_item_by_index(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] cs keyword that can be mapped to keyword mention based on cs_raw_keyword.txt\n",
    "cs_raw_keyword = my_read(cs_raw_keyword_file)\n",
    "cs_keyword = [kw for kw in cs_raw_keyword if kw in my_mention_dict]\n",
    "my_write(cs_keyword_file, cs_keyword)\n",
    "build_word_tree_v2(cs_keyword_file, cs_wordtree_file, cs_token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] cs keyword\n",
    "cs_keyword = my_read(cs_keyword_file)\n",
    "'smalltalk' in cs_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sentences from pages that have cs keywords as title ['collect_cs_pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect keyword cooccurance from sentence files (2 hours) ['collect_kw_occur_from_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] CoOccurrence model\n",
    "co = CoOccurrence(w2vec_wordtree_file, w2vec_token_file)\n",
    "cs_co = CoOccurrence(cs_wordtree_file, cs_token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General collection framework\n",
    "def collect_test_paths_all(test_file:str, co:CoOccurrence, process_func, columns:list, posfix:str='.dat', disable_pbar:bool=False):\n",
    "    # Build test data\n",
    "    with open(test_file) as f_in:\n",
    "        data = []\n",
    "        for line_idx, line in enumerate(tqdm.tqdm(f_in.readlines(), disable=disable_pbar)):\n",
    "            sent_note = line2note(test_file, line_idx, posfix=posfix)\n",
    "            line = line.strip()\n",
    "            co_kws = list(co.line_operation(sent_lemmatize(line)))\n",
    "            if len(co_kws) < 2:\n",
    "                continue\n",
    "            certain_ent_list = []\n",
    "            certain_ent_kw_list = []\n",
    "            uncertain_ent_list = []\n",
    "            uncertain_ent_kw_list = []\n",
    "            for kw in co_kws:\n",
    "                idxs = my_mention_dict[kw]\n",
    "                if len(idxs) == 1:\n",
    "                    certain_ent_kw_list.append(kw)\n",
    "                    certain_ent_list.append(w2vec.dictionary.get_entity_by_index(idxs[0]))\n",
    "                else:\n",
    "                    uncertain_ent_kw_list.append(kw)\n",
    "                    uncertain_ent_list.append([w2vec.dictionary.get_entity_by_index(idx) for idx in idxs])\n",
    "            \n",
    "            certain_ent_matrix = np.array([w2vec.get_vector(ent) for ent in certain_ent_list])\n",
    "            uncertain_ent_matrix_list = [np.array([w2vec.get_vector(ent) for ent in ent_list]) for ent_list in uncertain_ent_list]\n",
    "            pairs = []\n",
    "            certain_len = len(certain_ent_list)\n",
    "            uncertain_len = len(uncertain_ent_list)\n",
    "            if certain_len >= 1:\n",
    "                # Collect pairs between certain entities\n",
    "                result = cosine_similarity(certain_ent_matrix, certain_ent_matrix) - np.identity(certain_len)\n",
    "                for i in range(certain_len):\n",
    "                    for j in range(i+1, certain_len):\n",
    "                        pairs.append({'kw1':certain_ent_kw_list[i], 'kw2':certain_ent_kw_list[j], 'sim':float(result[i, j]), 'sent':sent_note, \n",
    "                            'kw1_ent':certain_ent_list[i].title, \n",
    "                            'kw2_ent':certain_ent_list[j].title})\n",
    "                # Collect pairs between certain and uncertain entities\n",
    "                for i in range(uncertain_len):\n",
    "                    result = cosine_similarity(certain_ent_matrix, uncertain_ent_matrix_list[i])\n",
    "                    for j in range(certain_len):\n",
    "                        idx = np.argmax(result[j])\n",
    "                        pairs.append({'kw1':uncertain_ent_kw_list[i], 'kw2':certain_ent_kw_list[j], 'sim':float(result[j, idx]), 'sent':sent_note, \n",
    "                            'kw1_ent':uncertain_ent_list[i][idx].title, \n",
    "                            'kw2_ent':certain_ent_list[j].title})\n",
    "            if uncertain_len >= 2:\n",
    "                # Collect pairs between uncertain entities\n",
    "                for i in range(uncertain_len):\n",
    "                    for j in range(i+1, uncertain_len):\n",
    "                        result = cosine_similarity(uncertain_ent_matrix_list[i], uncertain_ent_matrix_list[j])\n",
    "                        idx = np.argmax(result)\n",
    "                        row = int(idx / result.shape[1])\n",
    "                        col = idx % result.shape[1]\n",
    "                        pairs.append({'kw1':uncertain_ent_kw_list[i], 'kw2':uncertain_ent_kw_list[j], 'sim':float(result[row, col]), 'sent':sent_note, \n",
    "                            'kw1_ent':uncertain_ent_list[i][row].title, \n",
    "                            'kw2_ent':uncertain_ent_list[j][col].title})\n",
    "            doc = nlp(line)\n",
    "            data += process_func(doc, pairs)\n",
    "        if not disable_pbar:\n",
    "            print(len(data))\n",
    "        return pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] Collect sample data using general wikipedia2vec keywords and wiki sent files [collect_dataset]\n",
    "wiki_path_test_df = pd.concat([collect_test_paths_all(file, co, feature_process, feature_columns) for file in save_sent_files[:3]], ignore_index=True)\n",
    "wiki_path_test_df.to_csv(path_test_file, sep='\\t', index=False)\n",
    "print(len(wiki_path_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] Collect sample data using cs keywords and cs sent files\n",
    "cs_path_test_df = pd.concat([collect_test_paths_all(file, cs_co, feature_process, feature_columns, posfix='_cs.dat', disable_pbar=True) for file in tqdm.tqdm(save_cs_sent_files[:1100])], ignore_index=True)\n",
    "cs_path_test_df.to_csv(cs_path_test_file, sep='\\t', index=False)\n",
    "print(len(cs_path_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Load path test data (pd.DataFrame)\n",
    "wiki_path_test_df = pd.read_csv(open(path_test_file), sep='\\t')\n",
    "cs_path_test_df = pd.read_csv(open(cs_path_test_file), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] Pattern frequency generation\n",
    "\n",
    "# sub_df = wiki_path_test_df[wiki_path_test_df['sim'] > 0.5]\n",
    "sub_df = cs_path_test_df[cs_path_test_df['sim'] > 0.5]\n",
    "\n",
    "sub_df['pick'] = sub_df.apply(lambda x: 1 if 'nsubj' in x['path'] else 0, axis=1)\n",
    "sub_df = sub_df[sub_df['pick'] > 0]\n",
    "\n",
    "sub_df['pattern'] = sub_df.apply(lambda x: gen_pattern(x['path']), axis=1)\n",
    "\n",
    "c = Counter(sub_df['pattern'].to_list())\n",
    "\n",
    "max_cnt = c.most_common(1)[0][1]\n",
    "log_max_cnt = np.log(max_cnt+1)\n",
    "\n",
    "with open(path_pattern_count_file, 'wb') as f_out:\n",
    "    pickle.dump(c, f_out)\n",
    "\n",
    "def cal_freq(path:str):\n",
    "    cnt = c.get(path)\n",
    "    cnt = (cnt if cnt else 0.5) + 1\n",
    "    return np.log(cnt) / log_max_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process sample data to analyze and collect pattern\n",
    "'''\n",
    "the iterated logarithm accepts any positive real number and yields an integer.\n",
    "\n",
    "subject: iterated logarithm\n",
    "keyword: logarithm\n",
    "keyword recall: 0.5\n",
    "'''\n",
    "\n",
    "def filter_test_path(df:pd.DataFrame, output_file:str):\n",
    "    sub_df = df[df['sim'] > 0.5]\n",
    "    sub_df['pattern'] = sub_df.apply(lambda x: gen_pattern(x['path']), axis=1)\n",
    "    sub_df['pattern_freq'] = sub_df.apply(lambda x: cal_freq(x['pattern']), axis=1)\n",
    "    sub_df['sent'] = sub_df.apply(lambda x: note2line(x['sent'], '_cs.dat').strip(), axis=1)\n",
    "    sub_df['coverage'] = sub_df.apply(lambda x: cal_coverage(x['sent'], x['kw1'], x['kw2'], x['path']), axis=1)\n",
    "    sub_df['score'] = sub_df.apply(lambda x: (x['pattern_freq'] * (x['kw1_recall'] + x['kw2_recall']) / 2 * x['coverage'])**0.33, axis=1)\n",
    "    sub_df = sub_df[sub_df['score'] > 0.5]\n",
    "    sub_df.to_csv(output_file, columns = feature_columns + ['pattern', 'pattern_freq', 'coverage', 'score'], sep='\\t', index=False)\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] Collect sample data using general wikipedia2vec keywords and wiki sent files\n",
    "sub_df = filter_test_path(cs_path_test_df, 'nsubj.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('full_phrase_check.tsv', columns = ['sent', 'kw1', 'kw1_recall', 'kw1_full_span', 'kw2', 'kw2_recall', 'kw2_full_span'], sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Prepration] Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the graph ['generate_graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the entity occurance ['collect_ent_occur_from_selected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo function: find all the sentences that two keywords co-occur in selected sentences\n",
    "with open(path_pattern_count_file, 'rb') as f_in:\n",
    "    c = pickle.load(f_in)\n",
    "\n",
    "max_cnt = c.most_common(1)[0][1]\n",
    "log_max_cnt = np.log(max_cnt+1)\n",
    "\n",
    "def cal_freq(path:str):\n",
    "    cnt = c.get(path)\n",
    "    cnt = (cnt if cnt else 0.5) + 1\n",
    "    return np.log(cnt) / log_max_cnt\n",
    "        \n",
    "def find_sentences(entity_dict:dict, ent1:str, ent2:str):\n",
    "    kw1_occur = entity_dict.get(ent1)\n",
    "    kw2_occur = entity_dict.get(ent2)\n",
    "    if not kw1_occur or not kw2_occur:\n",
    "        return None\n",
    "    co_occur = kw1_occur & kw2_occur\n",
    "    kw1 = remove_brackets(normalize_text(ent1))\n",
    "    kw2 = remove_brackets(normalize_text(ent2))\n",
    "    sim = cosine_similarity(w2vec.get_entity_vector(ent1).reshape(1, -1), w2vec.get_entity_vector(ent2).reshape(1, -1))\n",
    "    data = []\n",
    "    for occur in co_occur:\n",
    "        sent = note2line(note2line(occur, '_se.dat').split('\\t')[7]).strip()\n",
    "        pairs = [{'sim' : sim[0][0], 'kw1' : kw1, 'kw2' : kw2, 'kw1_ent' : ent1, 'kw2_ent' : ent2, 'sent' : sent}]\n",
    "        doc = nlp(sent)\n",
    "        data += feature_process(doc, pairs)\n",
    "    df = pd.DataFrame(data = data, columns=feature_columns)\n",
    "    df['pattern'] = df.apply(lambda x: gen_pattern(x['path']), axis=1)\n",
    "    df['pattern_freq'] = df.apply(lambda x: cal_freq(x['pattern']), axis=1)\n",
    "    df['coverage'] = df.apply(lambda x: cal_coverage(x['sent'], x['kw1'], x['kw2'], x['path']), axis=1)\n",
    "    df['score'] = df.apply(lambda x: (x['pattern_freq'] * (x['kw1_recall'] + x['kw2_recall']) / 2 * x['coverage'])**0.33, axis=1)\n",
    "    df = df[df['score'] > 0.5]\n",
    "    return df\n",
    "\n",
    "def get_selected_record(entity_dict:dict, ent1:str, ent2:str):\n",
    "    kw1_occur = entity_dict.get(ent1)\n",
    "    kw2_occur = entity_dict.get(ent2)\n",
    "    if not kw1_occur or not kw2_occur:\n",
    "        return None\n",
    "    co_occur = kw1_occur & kw2_occur\n",
    "    data = []\n",
    "    features = ['sim', 'kw1', 'kw1_span', 'kw1_ent', 'kw2', 'kw2_span', 'kw2_ent', 'sent', 'path', 'kw1_full_span', 'kw1_recall', 'kw2_full_span', 'kw2_recall', 'pattern', 'pattern_freq', 'coverage', 'score']\n",
    "    for occur in co_occur:\n",
    "        record = note2line(occur, '_se.dat').strip().split('\\t')\n",
    "        sent = note2line(record[7]).strip()\n",
    "        data_dict = {features[i] : record[i] for i in range(len(record))}\n",
    "        data_dict['sent'] = sent\n",
    "        data.append(data_dict)\n",
    "    \n",
    "    df = pd.DataFrame(data = data, columns=features)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] keyword occur dict which has occurance record for all keywords in selected sentences\n",
    "with open(entity_occur_file, 'rb') as f_in:\n",
    "    entity_occur = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_selected_record(entity_occur, 'Machine learning', 'Pattern recognition')\n",
    "if df is not None:\n",
    "    df.to_csv('sents.tsv', columns=feature_columns + ['pattern', 'pattern_freq', 'coverage', 'score'], sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>kw1</th>\n",
       "      <th>kw1_span</th>\n",
       "      <th>kw1_ent</th>\n",
       "      <th>kw2</th>\n",
       "      <th>kw2_span</th>\n",
       "      <th>kw2_ent</th>\n",
       "      <th>sent</th>\n",
       "      <th>path</th>\n",
       "      <th>kw1_full_span</th>\n",
       "      <th>kw1_recall</th>\n",
       "      <th>kw2_full_span</th>\n",
       "      <th>kw2_recall</th>\n",
       "      <th>pattern</th>\n",
       "      <th>pattern_freq</th>\n",
       "      <th>coverage</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.87694</td>\n",
       "      <td>python</td>\n",
       "      <td>(17, 17)</td>\n",
       "      <td>Python (programming language)</td>\n",
       "      <td>java</td>\n",
       "      <td>(11, 11)</td>\n",
       "      <td>Java (programming language)</td>\n",
       "      <td>users of curly bracket programming languages ,...</td>\n",
       "      <td>i_nsubj i_ccomp i_conj nsubj prep pobj conj</td>\n",
       "      <td>python</td>\n",
       "      <td>1.0</td>\n",
       "      <td>java</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i_nsubj i_ccomp nsubj prep pobj</td>\n",
       "      <td>0.430463</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.519874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.87694</td>\n",
       "      <td>python</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>Python (programming language)</td>\n",
       "      <td>java</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>Java (programming language)</td>\n",
       "      <td>language drivers are available for java ( jdbc...</td>\n",
       "      <td>i_nsubj i_conj acomp prep pobj</td>\n",
       "      <td>python</td>\n",
       "      <td>1.0</td>\n",
       "      <td>java</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i_nsubj acomp prep pobj</td>\n",
       "      <td>0.663981</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.519410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.87694</td>\n",
       "      <td>python</td>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>Python (programming language)</td>\n",
       "      <td>java</td>\n",
       "      <td>(21, 21)</td>\n",
       "      <td>Java (programming language)</td>\n",
       "      <td>in 2018 , python was the third most popular la...</td>\n",
       "      <td>i_nsubj attr prep pobj conj</td>\n",
       "      <td>python</td>\n",
       "      <td>1.0</td>\n",
       "      <td>java</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i_nsubj attr prep pobj</td>\n",
       "      <td>0.850258</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.617365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sim     kw1  kw1_span                        kw1_ent   kw2  kw2_span  \\\n",
       "0  0.87694  python  (17, 17)  Python (programming language)  java  (11, 11)   \n",
       "1  0.87694  python  (10, 10)  Python (programming language)  java    (5, 5)   \n",
       "2  0.87694  python    (3, 3)  Python (programming language)  java  (21, 21)   \n",
       "\n",
       "                       kw2_ent  \\\n",
       "0  Java (programming language)   \n",
       "1  Java (programming language)   \n",
       "2  Java (programming language)   \n",
       "\n",
       "                                                sent  \\\n",
       "0  users of curly bracket programming languages ,...   \n",
       "1  language drivers are available for java ( jdbc...   \n",
       "2  in 2018 , python was the third most popular la...   \n",
       "\n",
       "                                          path kw1_full_span  kw1_recall  \\\n",
       "0  i_nsubj i_ccomp i_conj nsubj prep pobj conj        python         1.0   \n",
       "1               i_nsubj i_conj acomp prep pobj        python         1.0   \n",
       "2                  i_nsubj attr prep pobj conj        python         1.0   \n",
       "\n",
       "  kw2_full_span  kw2_recall                          pattern  pattern_freq  \\\n",
       "0          java         1.0  i_nsubj i_ccomp nsubj prep pobj      0.430463   \n",
       "1          java         1.0          i_nsubj acomp prep pobj      0.663981   \n",
       "2          java         1.0           i_nsubj attr prep pobj      0.850258   \n",
       "\n",
       "   coverage     score  \n",
       "0  0.320000  0.519874  \n",
       "1  0.206897  0.519410  \n",
       "2  0.272727  0.617365  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sentences(entity_occur, 'Python (programming language)', 'Java (programming language)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Machine learning' in entity_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent1 = 'Python (programming language)'\n",
    "ent2 = 'Machine learning'\n",
    "cosine_similarity(w2vec.get_entity_vector(ent1).reshape(1, -1), w2vec.get_entity_vector(ent2).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Graph\n",
    "with open(graph_file, 'rb') as f_in:\n",
    "    graph = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num of nodes:', len(graph.nodes))\n",
    "print('num of edges:', len(graph.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges['Python (programming language)', 'C++']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [d[2]['score'] for d in graph.edges.data()]\n",
    "scores.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[int(len(scores)*0.85)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter edges by score\n",
    "filtered_edges = [(u, v) for u, v, d in graph.edges.data() if d['score'] > 0.65]\n",
    "filtered_graph = graph.edge_subgraph(filtered_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in enumerate(filtered_graph.edges.data()):\n",
    "    if i >= 20:\n",
    "        break\n",
    "    u, v, d = e\n",
    "    sent = d['sent']\n",
    "    print(d['score'])\n",
    "    print(u)\n",
    "    print(v)\n",
    "    print(note2line(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_graphs = list(nx.connected_components(filtered_graph))\n",
    "print(len(sub_graphs))\n",
    "sub_graph_node_num = [len(sub) for sub in sub_graphs]\n",
    "sub_graph_node_num.sort(reverse=True)\n",
    "print(sub_graph_node_num[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of nodes: 171921\n",
      "num of edges: 565916\n"
     ]
    }
   ],
   "source": [
    "connected_graph = filtered_graph.subgraph(sub_graphs[0])\n",
    "print('num of nodes:', len(connected_graph.nodes))\n",
    "print('num of edges:', len(connected_graph.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2neig_cnt = {node : len(list(connected_graph.neighbors(node))) for node in connected_graph.nodes.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "neig_cnt = [v for v in node2neig_cnt.values() if v < 20]\n",
    "plt.title('num of nodes vs num of neighbors each node')\n",
    "plt.hist(neig_cnt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2triangle_num = nx.triangles(connected_graph)\n",
    "with open('node2tri_num.pickle', 'wb') as f_out:\n",
    "    pickle.dump(node2triangle_num, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of triangles: 614165.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('num of triangles:', sum(node2triangle_num.values()) / 3)\n",
    "plt.title('num of nodes vs num of triangles each node')\n",
    "plt.hist([v for v in node2triangle_num.values() if v >= 1 and v < 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_with_10_tri = {n : v for n, v in node2triangle_num.items() if v >= 5}\n",
    "print(len(node_with_10_tri) / len(connected_graph.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_triangles(graph:nx.Graph, node:str):\n",
    "    triangles = set()\n",
    "    neighbors = set(graph.neighbors(node))\n",
    "    for neighbor in neighbors:\n",
    "        second_neighbors = set(graph.neighbors(neighbor))\n",
    "        inter_neighbors = neighbors & second_neighbors\n",
    "        for third_neighbor in inter_neighbors:\n",
    "            triangles.add((node, neighbor, third_neighbor) if neighbor < third_neighbor else (node, third_neighbor, neighbor))\n",
    "    return triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles = list(find_triangles(connected_graph, 'Machine learning'))\n",
    "second_node = 'Pattern recognition'\n",
    "triangles.sort(key=lambda x: x[1])\n",
    "triangle_with_sents = []\n",
    "n_seen = {}\n",
    "for n1, n2, n3 in triangles:\n",
    "    if second_node and n2 != second_node and n3 != second_node:\n",
    "        continue\n",
    "    if n2 not in n_seen:\n",
    "        n_seen[n2] = note2line(graph.get_edge_data(n1, n2)['sent']).strip()\n",
    "    if n3 not in n_seen:\n",
    "        n_seen[n3] = note2line(graph.get_edge_data(n1, n3)['sent']).strip()\n",
    "    sent_1 = n_seen[n2]\n",
    "    sent_2 = note2line(graph.get_edge_data(n2, n3)['sent']).strip()\n",
    "    sent_3 = n_seen[n3]\n",
    "    triangle_with_sents.append((n1, sent_1, n2, sent_2, n3, sent_3))\n",
    "with open('triangles.tsv', 'w') as f_out:\n",
    "    csv_writer = csv.writer(f_out, delimiter='\\t')\n",
    "    csv_writer.writerows(triangle_with_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sentence from file\n",
    "note2line('AA:00:0', '_cs.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentence\n",
    "doc = nlp('these facilities enable a designer to \" simulate \" concurrent processes , each described using plain c + + syntax.')\n",
    "\n",
    "# Check noun phrases in the sentences\n",
    "print(list(doc.noun_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WOE re-write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw1 = 'data mining'\n",
    "kw2 = 'machine learning'\n",
    "doc = nlp('Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.'.lower())\n",
    "kw1_span = find_span(doc, kw1)\n",
    "kw2_span = find_span(doc, kw2)\n",
    "find_dependency_path_from_tree(doc, kw1_span[0], kw2_span[0])\n",
    "# print(len(kw1_span))\n",
    "# print(len(kw2_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keyword_connection_graph.neighbors('decision tree')\n",
    "my_write('neighbors.txt', list(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    remove_list = ['See also', 'References', 'Further reading', 'Sources', 'External links']\n",
    "    dic = {sec : page.section(sec) for sec in page.sections}\n",
    "    dic['summary'] = page.summary\n",
    "    sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    while len(section_list) > 0:\n",
    "        section = section_list.pop()\n",
    "        if section in remove_list:\n",
    "            continue\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        # processed_text = clean_text(section_text)\n",
    "        processed_text = ' '.join(section_text.lower().split())\n",
    "        temp_sents = my_sentence_tokenize(processed_text, True)\n",
    "        sents += temp_sents\n",
    "    return list(sents)\n",
    "\n",
    "def collect_entity_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    return [text.lower() for text in page.links]\n",
    "\n",
    "def collect_keyword_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    soup = BeautifulSoup(page.html(), 'html.parser')\n",
    "    main_block = soup.find('div', class_='mw-parser-output')\n",
    "    keywords = set([l.text.lower() for l in main_block.findAll('a') if re.match(r'^(<a href=\"/wiki/)', str(l))])\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'python'\n",
    "\n",
    "p = wikipedia.page(keyword)\n",
    "if p is not None:\n",
    "    sents = collect_sents_from_wiki_page(p)\n",
    "    keywords = collect_keyword_from_wiki_page(p)\n",
    "    print('sentences collected')\n",
    "    my_write('%s.txt' % keyword, sents)\n",
    "    my_write('%s_kw.txt' % keyword, keywords)\n",
    "    df = filter_by_path(sents)\n",
    "    df.to_csv('%s_out.tsv' % keyword, sep='\\t', index=False)\n",
    "\n",
    "    dff = df[df.apply(lambda x: str(x['head']) in keywords and str(x['tail']) in keywords, axis=1)]\n",
    "    dff.to_csv('%s_out_f.tsv' % keyword, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wanted'] = df.apply(lambda x: str(x['head']) in keywords, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect wikipedia page titles as entities and generate keyword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect wikipedia entities and corresponding id\n",
    "output = p.run(collect_wiki_entity, wiki_files)\n",
    "entity_list = []\n",
    "for l in output:\n",
    "    entity_list += l\n",
    "my_write(wikipedia_entity_file, entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalized wikipedia entities\n",
    "normalized_entity = []\n",
    "for kw in open(wikipedia_entity_file).readlines():\n",
    "    eid, ent = kw.split('\\t')\n",
    "    normalized_entity.append('%s\\t%s' % (eid, normalize_text(ent)))\n",
    "my_write(wikipedia_entity_norm_file, normalized_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate keyword list file\n",
    "keywords = [remove_brackets(line.strip().split('\\t')[1]) for line in open(wikipedia_entity_norm_file)]\n",
    "keywords = [kw for kw in keywords if kw.split()]\n",
    "keywords = filter_specific_keywords(keywords)\n",
    "keywords = list(set(keywords))\n",
    "my_write(wikipedia_keyword_file, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build wordtree\n",
    "build_word_tree_v2(wikipedia_keyword_file, wikipedia_wordtree_file, wikipedia_token_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process selected dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build connected graph from selected sentences(18 min) ['build_graph_from_selected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build connected graph from cooccurance files () ['build_graph_from_cooccur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keyword connection graph in selected sentences\n",
    "with open(keyword_connection_graph_file, 'rb') as f_in:\n",
    "    keyword_connection_graph = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-crafted analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_df = wiki_path_test_df[wiki_path_test_df['sim'] >= 0.0]\n",
    "cs_test_df = cs_path_test_df[cs_path_test_df['sim'] >= 0.0]\n",
    "\n",
    "def match_path_pattern(path:str):\n",
    "    for pp in patterns:\n",
    "        if exact_match(pp, path):\n",
    "            return pp\n",
    "    return ''\n",
    "\n",
    "wiki_test_df['pattern'] = wiki_test_df.apply(lambda x: match_path_pattern(x['path']), axis=1)\n",
    "cs_test_df['pattern'] = cs_test_df.apply(lambda x: match_path_pattern(x['path']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_path_result_sim_based(df:pd.DataFrame, paths:list):\n",
    "    summary_df = pd.DataFrame(columns=['path', 'cnt', 'ratio', 'avg_sim'])\n",
    "    for pp in paths:\n",
    "        sub_df = df[df['pattern'] == pp]\n",
    "        summary_df = summary_df.append({\n",
    "            'path' : pp,\n",
    "            'cnt' : len(sub_df),\n",
    "            'ratio' : len(sub_df) / len(df),\n",
    "            'avg_sim' : sum(sub_df['sim']) / len(sub_df) if len(sub_df) else 0\n",
    "        }, ignore_index=True)\n",
    "    summary_df = summary_df.append({\n",
    "        'path' : 'general',\n",
    "        'cnt' : len(df),\n",
    "        'ratio' : 1,\n",
    "        'avg_sim' : sum(df['sim']) / len(df) if len(df) else 0\n",
    "    }, ignore_index=True)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path_result_sim_based(wiki_test_df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path_result_sim_based(cs_test_df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_example_sent_for_pattern(df:pd.DataFrame, path:str, num:int=30, posfix:str='.dat'):\n",
    "    sub_df = df[df['pattern'] == path]\n",
    "    num = min(len(sub_df), num)\n",
    "    sub_df = sub_df[:num]\n",
    "    sub_df['sent'] = sub_df.apply(lambda x: note2line(x['sent'], posfix=posfix).strip(), axis=1)\n",
    "    return sub_df\n",
    "\n",
    "for patt in patterns:\n",
    "    temp_df = collect_example_sent_for_pattern(wiki_test_df, patt)\n",
    "    temp_df.to_csv('%s.tsv' % (patt[:10] if len(patt) >= 10 else patt), index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9496c91418be784f00ee6456e4343e8188c649322b68f201c83241a4029a42d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('FWD_py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
