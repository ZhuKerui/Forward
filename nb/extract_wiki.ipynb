{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences from Wikipedia\n",
    "+ This notebook is used for collecting sentences that tell relationship between two entities from wikipedia using some dependency path pattern\n",
    "+ **This notebook is fully valid under Owl3 machine (using the /scratch/data/wikipedia/full_text-2021-03-20 data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki sub folder example: ../../data/wikipedia/full_text-2021-03-20/BE\n",
      "save sub folder example: data/extract_wiki/wiki_sent_collect/BE\n",
      "wiki file example: ../../data/wikipedia/full_text-2021-03-20/BE/wiki_00\n",
      "save sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.dat\n",
      "save cooccur file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_co.dat\n",
      "save cs sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_cs.dat\n",
      "save selected sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_se.dat\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wikipedia\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "from collections import Counter\n",
    "import bz2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from tools.BasicUtils import my_read, my_write, MyMultiProcessing, count_line\n",
    "from tools.TextProcessing import (\n",
    "                normalize_text, remove_brackets, my_sentence_tokenize, build_word_tree_v2, \n",
    "                my_sentence_tokenize, filter_specific_keywords, find_dependency_path_from_tree, find_span, nlp, \n",
    "                sent_lemmatize, exact_match\n",
    "                )\n",
    "from tools.DocProcessing import CoOccurrence\n",
    "\n",
    "from extract_wiki import (\n",
    "    wikipedia_dir, wikipedia_entity_file, wikipedia_entity_norm_file, \n",
    "    wikipedia_keyword_file, wikipedia_token_file, wikipedia_wordtree_file, \n",
    "    save_path, entity_occur_file, keyword_connection_graph_file, w2vec_dump_file, w2vec_keyword_file, w2vec_wordtree_file, w2vec_token_file, \n",
    "    w2vec_keyword2idx_file, path_test_file, test_path, cs_keyword_file, cs_token_file, cs_wordtree_file, cs_path_test_file, cs_raw_keyword_file, \n",
    "    path_pattern_count_file, patterns, w2vec_entity_file, \n",
    "    collect_wiki_entity, get_sentence, line2note, note2line, basic_process, basic_columns, \n",
    "    feature_columns, feature_process, gen_pattern, cal_coverage\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create the multiprocessing object for future use\n",
    "p = MyMultiProcessing(10)\n",
    "\n",
    "# Generate the save dir\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.mkdir(test_path)\n",
    "\n",
    "sub_folders = [sub for sub in os.listdir(wikipedia_dir)]\n",
    "save_sub_folders = [os.path.join(save_path, sub) for sub in sub_folders]\n",
    "wiki_sub_folders = [os.path.join(wikipedia_dir, sub) for sub in sub_folders]\n",
    "\n",
    "wiki_files = []\n",
    "save_sent_files = []\n",
    "save_cooccur_files = []\n",
    "save_cs_sent_files = []\n",
    "save_selected_files = []\n",
    "\n",
    "for save_dir in save_sub_folders:\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "for i in range(len(wiki_sub_folders)):\n",
    "    files = [f for f in os.listdir(wiki_sub_folders[i])]\n",
    "    wiki_files += [os.path.join(wiki_sub_folders[i], f) for f in files]\n",
    "    save_sent_files += [os.path.join(save_sub_folders[i], f+'.dat') for f in files]\n",
    "    save_cooccur_files += [os.path.join(save_sub_folders[i], f+'_co.dat') for f in files]\n",
    "    save_cs_sent_files += [os.path.join(save_sub_folders[i], f+'_cs.dat') for f in files]\n",
    "    save_selected_files += [os.path.join(save_sub_folders[i], f+'_se.dat') for f in files]\n",
    "\n",
    "# Get all files under wikipedia/full_text-2021-03-20\n",
    "\n",
    "print('wiki sub folder example:', wiki_sub_folders[0])\n",
    "print('save sub folder example:', save_sub_folders[0])\n",
    "print('wiki file example:', wiki_files[0])\n",
    "print('save sentence file example:', save_sent_files[0])\n",
    "print('save cooccur file example:', save_cooccur_files[0])\n",
    "print('save cs sentence file example:', save_cs_sent_files[0])\n",
    "print('save selected sentence file example:', save_selected_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect sentences from wikipedia and select good sentences by path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sentences from wiki files (12 min)\n",
    "wiki_sent_pair = [(wiki_files[i], save_sent_files[i]) for i in range(len(wiki_files))]\n",
    "output = p.run(get_sentence, wiki_sent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keruiz2/miniconda3/envs/FWD_py38/lib/python3.8/contextlib.py:113: UserWarning: \"<bz2.BZ2File object at 0x7f978c97ac70>\" is not a raw file, mmap_mode \"c\" flag will be ignored.\n",
      "  return next(self.gen)\n"
     ]
    }
   ],
   "source": [
    "# [Load] wikipedia2vec\n",
    "with bz2.open(w2vec_dump_file) as f_in:\n",
    "    w2vec = Wikipedia2Vec.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8769396]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Test] wikipedia2vec\n",
    "\n",
    "# Find similar words or entities\n",
    "# ent1 = 'Python (programming language)'\n",
    "# w2vec.most_similar_by_vector(w2vec.get_entity_vector(ent1), 20)\n",
    "\n",
    "# Get similarity between two entities\n",
    "ent1 = 'Python (programming language)'\n",
    "ent2 = 'Java (programming language)'\n",
    "cosine_similarity(w2vec.get_entity_vector(ent1).reshape(1, -1), w2vec.get_entity_vector(ent2).reshape(1, -1))\n",
    "\n",
    "# Check the entity count and document count\n",
    "# ent1 = 'The World (radio program)'\n",
    "# e = w2vec.get_entity(ent1)\n",
    "# print(e.count)\n",
    "# print(e.doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform keywords into index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330958/330958 [00:01<00:00, 323146.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start building wordtree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330958/330958 [00:00<00:00, 358699.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word tree is accomplished with 330958 words added\n",
      "Total time taken in :  build_word_tree_v2 2.5734145641326904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "330958"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Create] my_mention_dict, mapping keyword mention to wikipedia2vec entities\n",
    "w2vec_entity = [w for w in w2vec.dictionary.entities() if w.count >= 50]\n",
    "w2vec_entity_title = [w.title for w in w2vec_entity]\n",
    "my_write(w2vec_entity_file, w2vec_entity_title)\n",
    "my_mention_dict = {}\n",
    "for ent in w2vec_entity:\n",
    "    kw = remove_brackets(normalize_text(ent.title))\n",
    "    if kw not in my_mention_dict:\n",
    "        my_mention_dict[kw] = [ent.index]\n",
    "    else:\n",
    "        my_mention_dict[kw].append(ent.index)\n",
    "w2vec_kws = filter_specific_keywords(list(my_mention_dict.keys()))\n",
    "my_write(w2vec_keyword_file, w2vec_kws)\n",
    "build_word_tree_v2(w2vec_keyword_file, w2vec_wordtree_file, w2vec_token_file)\n",
    "filter_keyword_from_w2vec = set(w2vec_kws)\n",
    "my_mention_dict = {k:v for k, v in my_mention_dict.items() if k in filter_keyword_from_w2vec}\n",
    "with open(w2vec_keyword2idx_file, 'wb') as f_out:\n",
    "    pickle.dump(my_mention_dict, f_out)\n",
    "len(my_mention_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] my_mention_dict\n",
    "with open(w2vec_keyword2idx_file, 'rb') as f_in:\n",
    "    my_mention_dict = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<Entity C++>\n"
     ]
    }
   ],
   "source": [
    "# [Test] my_mention_dict\n",
    "kw = 'c + +'\n",
    "kw_in_mention = kw in my_mention_dict\n",
    "print(kw_in_mention)\n",
    "if kw_in_mention:\n",
    "    for idx in my_mention_dict[kw]:\n",
    "        print(w2vec.dictionary.get_item_by_index(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] cs keyword that can be mapped to keyword mention based on cs_raw_keyword.txt\n",
    "cs_raw_keyword = my_read(cs_raw_keyword_file)\n",
    "cs_keyword = [kw for kw in cs_raw_keyword if kw in my_mention_dict]\n",
    "my_write(cs_keyword_file, cs_keyword)\n",
    "build_word_tree_v2(cs_keyword_file, cs_wordtree_file, cs_token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] cs keyword\n",
    "cs_keyword = my_read(cs_keyword_file)\n",
    "'smalltalk' in cs_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sentences from pages that have cs keywords as title ['collect_cs_pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect keyword cooccurance from sentence files (2 hours) ['collect_kw_occur_from_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] CoOccurrence model\n",
    "co = CoOccurrence(w2vec_wordtree_file, w2vec_token_file)\n",
    "cs_co = CoOccurrence(cs_wordtree_file, cs_token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General collection framework\n",
    "def collect_test_paths_all(test_file:str, co:CoOccurrence, process_func, columns:list, posfix:str='.dat', disable_pbar:bool=False):\n",
    "    # Build test data\n",
    "    with open(test_file) as f_in:\n",
    "        data = []\n",
    "        for line_idx, line in enumerate(tqdm.tqdm(f_in.readlines(), disable=disable_pbar)):\n",
    "            sent_note = line2note(test_file, line_idx, posfix=posfix)\n",
    "            line = line.strip()\n",
    "            co_kws = list(co.line_operation(sent_lemmatize(line)))\n",
    "            if len(co_kws) < 2:\n",
    "                continue\n",
    "            certain_ent_list = []\n",
    "            certain_ent_kw_list = []\n",
    "            uncertain_ent_list = []\n",
    "            uncertain_ent_kw_list = []\n",
    "            for kw in co_kws:\n",
    "                idxs = my_mention_dict[kw]\n",
    "                if len(idxs) == 1:\n",
    "                    certain_ent_kw_list.append(kw)\n",
    "                    certain_ent_list.append(w2vec.dictionary.get_entity_by_index(idxs[0]))\n",
    "                else:\n",
    "                    uncertain_ent_kw_list.append(kw)\n",
    "                    uncertain_ent_list.append([w2vec.dictionary.get_entity_by_index(idx) for idx in idxs])\n",
    "            \n",
    "            certain_ent_matrix = np.array([w2vec.get_vector(ent) for ent in certain_ent_list])\n",
    "            uncertain_ent_matrix_list = [np.array([w2vec.get_vector(ent) for ent in ent_list]) for ent_list in uncertain_ent_list]\n",
    "            pairs = []\n",
    "            certain_len = len(certain_ent_list)\n",
    "            uncertain_len = len(uncertain_ent_list)\n",
    "            if certain_len >= 1:\n",
    "                # Collect pairs between certain entities\n",
    "                result = cosine_similarity(certain_ent_matrix, certain_ent_matrix) - np.identity(certain_len)\n",
    "                for i in range(certain_len):\n",
    "                    for j in range(i+1, certain_len):\n",
    "                        pairs.append({'kw1':certain_ent_kw_list[i], 'kw2':certain_ent_kw_list[j], 'sim':float(result[i, j]), 'sent':sent_note, \n",
    "                            'kw1_ent':certain_ent_list[i].title, \n",
    "                            'kw2_ent':certain_ent_list[j].title})\n",
    "                # Collect pairs between certain and uncertain entities\n",
    "                for i in range(uncertain_len):\n",
    "                    result = cosine_similarity(certain_ent_matrix, uncertain_ent_matrix_list[i])\n",
    "                    for j in range(certain_len):\n",
    "                        idx = np.argmax(result[j])\n",
    "                        pairs.append({'kw1':uncertain_ent_kw_list[i], 'kw2':certain_ent_kw_list[j], 'sim':float(result[j, idx]), 'sent':sent_note, \n",
    "                            'kw1_ent':uncertain_ent_list[i][idx].title, \n",
    "                            'kw2_ent':certain_ent_list[j].title})\n",
    "            if uncertain_len >= 2:\n",
    "                # Collect pairs between uncertain entities\n",
    "                for i in range(uncertain_len):\n",
    "                    for j in range(i+1, uncertain_len):\n",
    "                        result = cosine_similarity(uncertain_ent_matrix_list[i], uncertain_ent_matrix_list[j])\n",
    "                        idx = np.argmax(result)\n",
    "                        row = int(idx / result.shape[1])\n",
    "                        col = idx % result.shape[1]\n",
    "                        pairs.append({'kw1':uncertain_ent_kw_list[i], 'kw2':uncertain_ent_kw_list[j], 'sim':float(result[row, col]), 'sent':sent_note, \n",
    "                            'kw1_ent':uncertain_ent_list[i][row].title, \n",
    "                            'kw2_ent':uncertain_ent_list[j][col].title})\n",
    "            doc = nlp(line)\n",
    "            data += process_func(doc, pairs)\n",
    "        if not disable_pbar:\n",
    "            print(len(data))\n",
    "        return pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] Collect sample data using general wikipedia2vec keywords and wiki sent files [collect_dataset]\n",
    "wiki_path_test_df = pd.concat([collect_test_paths_all(file, co, feature_process, feature_columns) for file in save_sent_files[:3]], ignore_index=True)\n",
    "wiki_path_test_df.to_csv(path_test_file, sep='\\t', index=False)\n",
    "print(len(wiki_path_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] Collect sample data using cs keywords and cs sent files\n",
    "# line_per_file = [count_line(f) for f in save_cs_sent_files[:1100]]\n",
    "# print(sum(line_per_file))\n",
    "cs_path_test_df = pd.concat([collect_test_paths_all(file, cs_co, feature_process, feature_columns, posfix='_cs.dat', disable_pbar=True) for file in tqdm.tqdm(save_cs_sent_files[:1100])], ignore_index=True)\n",
    "cs_path_test_df.to_csv(cs_path_test_file, sep='\\t', index=False)\n",
    "print(len(cs_path_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Load path test data (pd.DataFrame)\n",
    "wiki_path_test_df = pd.read_csv(open(path_test_file), sep='\\t')\n",
    "cs_path_test_df = pd.read_csv(open(cs_path_test_file), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2354/2813895503.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['pick'] = sub_df.apply(lambda x: 1 if 'nsubj' in x['path'] else 0, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# [Create] Pattern frequency generation\n",
    "\n",
    "sub_df = wiki_path_test_df[wiki_path_test_df['sim'] > 0.5]\n",
    "# sub_df = cs_path_test_df[cs_path_test_df['sim'] > 0.5]\n",
    "\n",
    "sub_df['pick'] = sub_df.apply(lambda x: 1 if 'nsubj' in x['path'] else 0, axis=1)\n",
    "sub_df = sub_df[sub_df['pick'] > 0]\n",
    "\n",
    "sub_df['pattern'] = sub_df.apply(lambda x: gen_pattern(x['path']), axis=1)\n",
    "\n",
    "c = Counter(sub_df['pattern'].to_list())\n",
    "\n",
    "max_cnt = c.most_common(1)[0][1]\n",
    "log_max_cnt = np.log(max_cnt+1)\n",
    "\n",
    "with open(path_pattern_count_file, 'wb') as f_out:\n",
    "    pickle.dump(c, f_out)\n",
    "\n",
    "def cal_freq(path:str):\n",
    "    cnt = c.get(path)\n",
    "    cnt = (cnt if cnt else 0.5) + 1\n",
    "    return np.log(cnt) / log_max_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process sample data to analyze and collect pattern\n",
    "'''\n",
    "the iterated logarithm accepts any positive real number and yields an integer.\n",
    "\n",
    "subject: iterated logarithm\n",
    "keyword: logarithm\n",
    "keyword recall: 0.5\n",
    "'''\n",
    "\n",
    "def filter_test_path(df:pd.DataFrame, output_file:str):\n",
    "    sub_df = df[df['sim'] > 0.5]\n",
    "    sub_df['pattern'] = sub_df.apply(lambda x: gen_pattern(x['path']), axis=1)\n",
    "    sub_df['pattern_freq'] = sub_df.apply(lambda x: cal_freq(x['pattern']), axis=1)\n",
    "    sub_df['sent'] = sub_df.apply(lambda x: note2line(x['sent']).strip(), axis=1)\n",
    "    sub_df['coverage'] = sub_df.apply(lambda x: cal_coverage(x['sent'], x['kw1'], x['kw2'], x['path']), axis=1)\n",
    "    sub_df['score'] = sub_df.apply(lambda x: (x['pattern_freq'] * (x['kw1_recall'] + x['kw2_recall']) / 2 * x['coverage'])**0.33, axis=1)\n",
    "    sub_df = sub_df[sub_df['score'] > 0.5]\n",
    "    sub_df.to_csv(output_file, columns = feature_columns + ['pattern', 'pattern_freq', 'coverage', 'score'], sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18525/4092377352.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['pattern'] = sub_df.apply(lambda x: gen_pattern(x['path']), axis=1)\n",
      "/tmp/ipykernel_18525/4092377352.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['pattern_freq'] = sub_df.apply(lambda x: cal_freq(x['pattern']), axis=1)\n",
      "/tmp/ipykernel_18525/4092377352.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['sent'] = sub_df.apply(lambda x: note2line(x['sent']).strip(), axis=1)\n",
      "/tmp/ipykernel_18525/4092377352.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['coverage'] = sub_df.apply(lambda x: cal_coverage(x['sent'], x['kw1'], x['kw2'], x['path']), axis=1)\n",
      "/tmp/ipykernel_18525/4092377352.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sub_df['score'] = sub_df.apply(lambda x: (x['pattern_freq'] * (x['kw1_recall'] + x['kw2_recall']) / 2 * x['coverage'])**0.33, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# [Create] Collect sample data using general wikipedia2vec keywords and wiki sent files\n",
    "filter_test_path(wiki_path_test_df, 'nsubj.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the entity occurance ['collect_ent_occur_from_selected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo function: find all the sentences that two keywords co-occur in selected sentences\n",
    "def find_sentences(entity_dict:dict, ent1:str, ent2:str):\n",
    "    kw1_occur = entity_dict.get(ent1)\n",
    "    kw2_occur = entity_dict.get(ent2)\n",
    "    if not kw1_occur or not kw2_occur:\n",
    "        return None\n",
    "    co_occur = kw1_occur & kw2_occur\n",
    "    kw1 = remove_brackets(normalize_text(ent1))\n",
    "    kw2 = remove_brackets(normalize_text(ent2))\n",
    "    sim = cosine_similarity(w2vec.get_entity_vector(ent1).reshape(1, -1), w2vec.get_entity_vector(ent2).reshape(1, -1))\n",
    "    data = []\n",
    "    for occur in co_occur:\n",
    "        sent = note2line(occur).strip()\n",
    "        pairs = [{'sim' : sim[0][0], 'kw1' : kw1, 'kw2' : kw2, 'kw1_ent' : ent1, 'kw2_ent' : ent2, 'sent' : sent}]\n",
    "        doc = nlp(sent)\n",
    "        data += feature_process(doc, pairs)\n",
    "    \n",
    "    df = pd.DataFrame(data = data, columns=feature_columns)\n",
    "    df['pattern'] = df.apply(lambda x: gen_pattern(x['path']), axis=1)\n",
    "    df['pattern_freq'] = df.apply(lambda x: cal_freq(x['pattern']), axis=1)\n",
    "    df['coverage'] = df.apply(lambda x: cal_coverage(x['sent'], x['kw1'], x['kw2'], x['path']), axis=1)\n",
    "    df['score'] = df.apply(lambda x: (x['pattern_freq'] * (x['kw1_recall'] + x['kw2_recall']) / 2 * x['coverage'])**0.33, axis=1)\n",
    "    df = df[df['score'] > 0.5]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keyword occur dict which has occurance record for all keywords in selected sentences\n",
    "with open(entity_occur_file, 'rb') as f_in:\n",
    "    entity_occur = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = find_sentences(entity_occur, 'Python (programming language)', 'C++')\n",
    "df.to_csv('sents.tsv', columns=feature_columns + ['pattern', 'pattern_freq', 'coverage', 'score'], sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Machine learning' in entity_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(keyword_connection_graph_file, 'rb') as f_in:\n",
    "    keyword_connection_graph = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360711\n",
      "259337\n"
     ]
    }
   ],
   "source": [
    "edge_cnts = [keyword_connection_graph.get_edge_data(e[0], e[1])['c'] for e in keyword_connection_graph.edges]\n",
    "node_cnts = [keyword_connection_graph.nodes[n]['c'] for n in keyword_connection_graph.nodes]\n",
    "print(len(edge_cnts))\n",
    "print(len(node_cnts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_set = set()\n",
    "for f in save_selected_files:\n",
    "    with open(f) as f_in:\n",
    "        for line in f_in:\n",
    "            sent_set.add(line.split('\\t')[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7690069"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f94922a08e0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_cnt = [n for n in edge_cnts if n < 10]\n",
    "n, bins, patches = plt.hist(temp_cnt)\n",
    "plt.plot(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f94c5dffa60>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_cnt = [n for n in node_cnts if n < 50]\n",
    "n, bins, patches = plt.hist(temp_cnt)\n",
    "plt.plot(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2233613\n",
      "40871\n",
      "218466\n"
     ]
    }
   ],
   "source": [
    "filter_nodes = set([n for n in keyword_connection_graph.nodes if keyword_connection_graph.nodes[n]['c'] <= 2 or keyword_connection_graph.nodes[n]['c'] >= 10000])\n",
    "filtered_edges = [keyword_connection_graph.get_edge_data(e[0], e[1])['c'] for e in keyword_connection_graph.edges if e[0] not in filter_nodes and e[1] not in filter_nodes]\n",
    "print(len(filtered_edges))\n",
    "print(len(filter_nodes))\n",
    "print(len(node_cnts) - len(filter_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child\n",
      "Album\n",
      "9\n",
      "The Game (U.S. TV series)\n",
      "Government\n"
     ]
    }
   ],
   "source": [
    "cnt = 5\n",
    "for n in filter_nodes:\n",
    "    if keyword_connection_graph.nodes[n]['c'] >= 10000:\n",
    "        print(n)\n",
    "        cnt -= 1\n",
    "    if cnt <= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_set = set(keyword_connection_graph.neighbors('Machine learning'))\n",
    "# 'Programming language' in n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic programming\n",
      "Genome-wide association study\n",
      "Inductive reasoning\n",
      "Precision and recall\n",
      "Principal component analysis\n",
      "Feature selection\n"
     ]
    }
   ],
   "source": [
    "for i, n in enumerate(n_set):\n",
    "    print(n)\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5504198]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent1 = 'Python (programming language)'\n",
    "ent2 = 'Machine learning'\n",
    "cosine_similarity(w2vec.get_entity_vector(ent1).reshape(1, -1), w2vec.get_entity_vector(ent2).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anarchism is a political philosophy and political movement that is sceptical of authority and rejects all involuntary , coercive forms of hierarchy.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show sentence from file\n",
    "note2line('AA:00:0', '_cs.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentence\n",
    "doc = nlp('these facilities enable a designer to \" simulate \" concurrent processes , each described using plain c + + syntax.')\n",
    "\n",
    "# Check noun phrases in the sentences\n",
    "print(list(doc.noun_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WOE re-write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw1 = 'data mining'\n",
    "kw2 = 'machine learning'\n",
    "doc = nlp('Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.'.lower())\n",
    "kw1_span = find_span(doc, kw1)\n",
    "kw2_span = find_span(doc, kw2)\n",
    "find_dependency_path_from_tree(doc, kw1_span[0], kw2_span[0])\n",
    "# print(len(kw1_span))\n",
    "# print(len(kw2_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keyword_connection_graph.neighbors('decision tree')\n",
    "my_write('neighbors.txt', list(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    remove_list = ['See also', 'References', 'Further reading', 'Sources', 'External links']\n",
    "    dic = {sec : page.section(sec) for sec in page.sections}\n",
    "    dic['summary'] = page.summary\n",
    "    sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    while len(section_list) > 0:\n",
    "        section = section_list.pop()\n",
    "        if section in remove_list:\n",
    "            continue\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        # processed_text = clean_text(section_text)\n",
    "        processed_text = ' '.join(section_text.lower().split())\n",
    "        temp_sents = my_sentence_tokenize(processed_text, True)\n",
    "        sents += temp_sents\n",
    "    return list(sents)\n",
    "\n",
    "def collect_entity_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    return [text.lower() for text in page.links]\n",
    "\n",
    "def collect_keyword_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    soup = BeautifulSoup(page.html(), 'html.parser')\n",
    "    main_block = soup.find('div', class_='mw-parser-output')\n",
    "    keywords = set([l.text.lower() for l in main_block.findAll('a') if re.match(r'^(<a href=\"/wiki/)', str(l))])\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'python'\n",
    "\n",
    "p = wikipedia.page(keyword)\n",
    "if p is not None:\n",
    "    sents = collect_sents_from_wiki_page(p)\n",
    "    keywords = collect_keyword_from_wiki_page(p)\n",
    "    print('sentences collected')\n",
    "    my_write('%s.txt' % keyword, sents)\n",
    "    my_write('%s_kw.txt' % keyword, keywords)\n",
    "    df = filter_by_path(sents)\n",
    "    df.to_csv('%s_out.tsv' % keyword, sep='\\t', index=False)\n",
    "\n",
    "    dff = df[df.apply(lambda x: str(x['head']) in keywords and str(x['tail']) in keywords, axis=1)]\n",
    "    dff.to_csv('%s_out_f.tsv' % keyword, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wanted'] = df.apply(lambda x: str(x['head']) in keywords, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect wikipedia page titles as entities and generate keyword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect wikipedia entities and corresponding id\n",
    "output = p.run(collect_wiki_entity, wiki_files)\n",
    "entity_list = []\n",
    "for l in output:\n",
    "    entity_list += l\n",
    "my_write(wikipedia_entity_file, entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalized wikipedia entities\n",
    "normalized_entity = []\n",
    "for kw in open(wikipedia_entity_file).readlines():\n",
    "    eid, ent = kw.split('\\t')\n",
    "    normalized_entity.append('%s\\t%s' % (eid, normalize_text(ent)))\n",
    "my_write(wikipedia_entity_norm_file, normalized_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate keyword list file\n",
    "keywords = [remove_brackets(line.strip().split('\\t')[1]) for line in open(wikipedia_entity_norm_file)]\n",
    "keywords = [kw for kw in keywords if kw.split()]\n",
    "keywords = filter_specific_keywords(keywords)\n",
    "keywords = list(set(keywords))\n",
    "my_write(wikipedia_keyword_file, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build wordtree\n",
    "build_word_tree_v2(wikipedia_keyword_file, wikipedia_wordtree_file, wikipedia_token_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process selected dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build connected graph from selected sentences(18 min) ['build_graph_from_selected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build connected graph from cooccurance files () ['build_graph_from_cooccur]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keyword connection graph in selected sentences\n",
    "with open(keyword_connection_graph_file, 'rb') as f_in:\n",
    "    keyword_connection_graph = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-crafted analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_df = wiki_path_test_df[wiki_path_test_df['sim'] >= 0.0]\n",
    "cs_test_df = cs_path_test_df[cs_path_test_df['sim'] >= 0.0]\n",
    "\n",
    "def match_path_pattern(path:str):\n",
    "    for pp in patterns:\n",
    "        if exact_match(pp, path):\n",
    "            return pp\n",
    "    return ''\n",
    "\n",
    "wiki_test_df['pattern'] = wiki_test_df.apply(lambda x: match_path_pattern(x['path']), axis=1)\n",
    "cs_test_df['pattern'] = cs_test_df.apply(lambda x: match_path_pattern(x['path']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_path_result_sim_based(df:pd.DataFrame, paths:list):\n",
    "    summary_df = pd.DataFrame(columns=['path', 'cnt', 'ratio', 'avg_sim'])\n",
    "    for pp in paths:\n",
    "        sub_df = df[df['pattern'] == pp]\n",
    "        summary_df = summary_df.append({\n",
    "            'path' : pp,\n",
    "            'cnt' : len(sub_df),\n",
    "            'ratio' : len(sub_df) / len(df),\n",
    "            'avg_sim' : sum(sub_df['sim']) / len(sub_df) if len(sub_df) else 0\n",
    "        }, ignore_index=True)\n",
    "    summary_df = summary_df.append({\n",
    "        'path' : 'general',\n",
    "        'cnt' : len(df),\n",
    "        'ratio' : 1,\n",
    "        'avg_sim' : sum(df['sim']) / len(df) if len(df) else 0\n",
    "    }, ignore_index=True)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path_result_sim_based(wiki_test_df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path_result_sim_based(cs_test_df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_example_sent_for_pattern(df:pd.DataFrame, path:str, num:int=30, posfix:str='.dat'):\n",
    "    sub_df = df[df['pattern'] == path]\n",
    "    num = min(len(sub_df), num)\n",
    "    sub_df = sub_df[:num]\n",
    "    sub_df['sent'] = sub_df.apply(lambda x: note2line(x['sent'], posfix=posfix).strip(), axis=1)\n",
    "    return sub_df\n",
    "\n",
    "for patt in patterns:\n",
    "    temp_df = collect_example_sent_for_pattern(wiki_test_df, patt)\n",
    "    temp_df.to_csv('%s.tsv' % (patt[:10] if len(patt) >= 10 else patt), index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9496c91418be784f00ee6456e4343e8188c649322b68f201c83241a4029a42d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('FWD_py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
