{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences from Wikipedia\n",
    "+ This notebook is used for collecting sentences that tell relationship between two entities from wikipedia using some dependency path pattern\n",
    "+ **This notebook is fully valid under Owl3 machine (using the /scratch/data/wikipedia/full_text-2021-03-20 data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki sub folder example: ../../data/wikipedia/full_text-2021-03-20/BE\n",
      "save sub folder example: data/extract_wiki/wiki_sent_collect/BE\n",
      "wiki file example: ../../data/wikipedia/full_text-2021-03-20/BE/wiki_00\n",
      "save sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.dat\n",
      "save cooccur file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_co.dat\n",
      "save cs sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_cs.dat\n",
      "save selected sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.tsv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wikipedia\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "import bz2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from tools.BasicUtils import my_read, my_write, MyMultiProcessing, count_line\n",
    "from tools.TextProcessing import (\n",
    "                normalize_text, remove_brackets, my_sentence_tokenize, build_word_tree_v2, \n",
    "                my_sentence_tokenize, filter_specific_keywords, find_dependency_path_from_tree, find_span, nlp, \n",
    "                sent_lemmatize, exact_match\n",
    "                )\n",
    "from tools.DocProcessing import CoOccurrence, graph_load\n",
    "\n",
    "from extract_wiki import SentenceFilter\n",
    "\n",
    "from extract_wiki import (\n",
    "    wikipedia_dir, wikipedia_entity_file, wikipedia_entity_norm_file, \n",
    "    wikipedia_keyword_file, wikipedia_token_file, wikipedia_wordtree_file, wikipedia_keyword_filtered_file, keyword_npmi_graph_file_v2, \n",
    "    save_path, keyword_occur_file, keyword_connection_graph_file, w2vec_dump_file, w2vec_keyword_file, w2vec_wordtree_file, w2vec_token_file, \n",
    "    w2vec_keyword2idx_file, path_test_file, test_path, cs_keyword_file, cs_token_file, cs_wordtree_file, cs_path_test_file, cs_raw_keyword_file, \n",
    "    collect_wiki_entity, get_sentence, keyword_count_file, filter_keyword_by_freq, line2note, note2line\n",
    ")\n",
    "\n",
    "\n",
    "# Create the multiprocessing object for future use\n",
    "p = MyMultiProcessing(10)\n",
    "\n",
    "# Generate the save dir\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.mkdir(test_path)\n",
    "\n",
    "sub_folders = [sub for sub in os.listdir(wikipedia_dir)]\n",
    "save_sub_folders = [os.path.join(save_path, sub) for sub in sub_folders]\n",
    "wiki_sub_folders = [os.path.join(wikipedia_dir, sub) for sub in sub_folders]\n",
    "\n",
    "wiki_files = []\n",
    "save_sent_files = []\n",
    "save_cooccur_files = []\n",
    "save_cs_sent_files = []\n",
    "save_selected_files = []\n",
    "\n",
    "for save_dir in save_sub_folders:\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "for i in range(len(wiki_sub_folders)):\n",
    "    files = [f for f in os.listdir(wiki_sub_folders[i])]\n",
    "    wiki_files += [os.path.join(wiki_sub_folders[i], f) for f in files]\n",
    "    save_sent_files += [os.path.join(save_sub_folders[i], f+'.dat') for f in files]\n",
    "    save_cooccur_files += [os.path.join(save_sub_folders[i], f+'_co.dat') for f in files]\n",
    "    save_cs_sent_files += [os.path.join(save_sub_folders[i], f+'_cs.dat') for f in files]\n",
    "    save_selected_files += [os.path.join(save_sub_folders[i], f+'.tsv') for f in files]\n",
    "\n",
    "# Get all files under wikipedia/full_text-2021-03-20\n",
    "\n",
    "print('wiki sub folder example:', wiki_sub_folders[0])\n",
    "print('save sub folder example:', save_sub_folders[0])\n",
    "print('wiki file example:', wiki_files[0])\n",
    "print('save sentence file example:', save_sent_files[0])\n",
    "print('save cooccur file example:', save_cooccur_files[0])\n",
    "print('save cs sentence file example:', save_cs_sent_files[0])\n",
    "print('save selected sentence file example:', save_selected_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect wikipedia page titles as entities and generate keyword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect wikipedia entities\n",
    "output = p.run(collect_wiki_entity, wiki_files)\n",
    "entity_list = []\n",
    "for l in output:\n",
    "    entity_list += l\n",
    "my_write(wikipedia_entity_file, entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalized wikipedia entities\n",
    "normalized_entity = []\n",
    "for kw in open(wikipedia_entity_file).readlines():\n",
    "    eid, ent = kw.split('\\t')\n",
    "    normalized_entity.append('%s\\t%s' % (eid, normalize_text(ent)))\n",
    "my_write(wikipedia_entity_norm_file, normalized_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate keyword list file\n",
    "keywords = [remove_brackets(line.strip().split('\\t')[1]) for line in open(wikipedia_entity_norm_file)]\n",
    "keywords = [kw for kw in keywords if kw.split()]\n",
    "keywords = filter_specific_keywords(keywords)\n",
    "keywords = list(set(keywords))\n",
    "my_write(wikipedia_keyword_file, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build wordtree\n",
    "build_word_tree_v2(wikipedia_keyword_file, wikipedia_wordtree_file, wikipedia_token_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect sentences from wikipedia and select good sentences by path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the wikipedia page files to sentence only file (12 min)\n",
    "wiki_sent_pair = [(wiki_files[i], save_sent_files[i]) for i in range(len(wiki_files))]\n",
    "output = p.run(get_sentence, wiki_sent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect sentences from pages that have cs keywords as title ['collect_cs_pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sentence filter\n",
    "sf = SentenceFilter(wikipedia_wordtree_file, wikipedia_token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of selecting sentences by path ['collect_sents']\n",
    "def collect_sents(save_sent_file:str, save_selected_file:str):\n",
    "    sents = my_read(save_sent_file)\n",
    "    df = sf.list_operation(sents, use_id=True, keyword_only=True)\n",
    "    df.to_csv(save_selected_file, sep='\\t', index=False)\n",
    "\n",
    "test_list = [(save_sent_files[i], '%d.tsv' % i) for i in range(5)]\n",
    "test_output = p.run(collect_sents, test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the keyword occurance (15 min) ['collect_kw_occur_from_selected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build connected graph from selected sentences(18 min) ['build_graph_from_selected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect keyword cooccurance from sentence files (2 hours) ['collect_kw_occur_from_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the less frequent keywords using cooccurance files (8 min)\n",
    "keyword_count = filter_keyword_by_freq(save_cooccur_file_list=save_cooccur_files)\n",
    "with open(keyword_count_file, 'wb') as f_out:\n",
    "    pickle.dump(keyword_count, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = {k : v for k, v in keyword_count.items() if v >= 300}\n",
    "my_write(wikipedia_keyword_filtered_file, list(f.keys()))\n",
    "print(len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'python' in f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build connected graph from cooccurance files () ['build_graph_from_cooccur]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia2vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wikipedia2vec\n",
    "with bz2.open(w2vec_dump_file) as f_in:\n",
    "    w2vec = Wikipedia2Vec.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create my_mention_dict, mapping keyword mention to wikipedia2vec entities\n",
    "my_mention_dict = {}\n",
    "for ent in w2vec.dictionary.entities():\n",
    "    if ent.count < 50:\n",
    "        continue\n",
    "    kw = remove_brackets(normalize_text(ent.title))\n",
    "    if kw not in my_mention_dict:\n",
    "        my_mention_dict[kw] = [ent.index]\n",
    "    else:\n",
    "        my_mention_dict[kw].append(ent.index)\n",
    "w2vec_kws = filter_specific_keywords(list(my_mention_dict.keys()))\n",
    "my_write(w2vec_keyword_file, w2vec_kws)\n",
    "build_word_tree_v2(w2vec_keyword_file, w2vec_wordtree_file, w2vec_token_file)\n",
    "filter_keyword_from_w2vec = set(w2vec_kws)\n",
    "my_mention_dict = {k:v for k, v in my_mention_dict.items() if k in filter_keyword_from_w2vec}\n",
    "with open(w2vec_keyword2idx_file, 'wb') as f_out:\n",
    "    pickle.dump(my_mention_dict, f_out)\n",
    "len(my_mention_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cs keyword that can be mapped to keyword mention based on cs_raw_keyword.txt\n",
    "cs_raw_keyword = my_read(cs_raw_keyword_file)\n",
    "cs_keyword = [kw for kw in cs_raw_keyword if kw in my_mention_dict]\n",
    "my_write(cs_keyword_file, cs_keyword)\n",
    "build_word_tree_v2(cs_keyword_file, cs_wordtree_file, cs_token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'smalltalk' in cs_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CoOccurrence model\n",
    "co = CoOccurrence(w2vec_wordtree_file, w2vec_token_file)\n",
    "cs_co = CoOccurrence(cs_wordtree_file, cs_token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_test_paths_all(test_file:str, co:CoOccurrence, posfix='.dat', disable_pbar:bool=False):\n",
    "    # Build test data\n",
    "    with open(test_file) as f_in:\n",
    "        data = []\n",
    "        for line_idx, line in enumerate(tqdm.tqdm(f_in.readlines(), disable=disable_pbar)):\n",
    "            sent_note = line2note(test_file, line_idx, posfix=posfix)\n",
    "            line = line.strip()\n",
    "            co_kws = list(co.line_operation(sent_lemmatize(line)))\n",
    "            if len(co_kws) < 2:\n",
    "                continue\n",
    "            certain_ent_list = []\n",
    "            certain_ent_kw_list = []\n",
    "            uncertain_ent_list = []\n",
    "            uncertain_ent_kw_list = []\n",
    "            for kw in co_kws:\n",
    "                idxs = my_mention_dict[kw]\n",
    "                if len(idxs) == 1:\n",
    "                    certain_ent_kw_list.append(kw)\n",
    "                    certain_ent_list.append(w2vec.dictionary.get_entity_by_index(idxs[0]))\n",
    "                else:\n",
    "                    uncertain_ent_kw_list.append(kw)\n",
    "                    uncertain_ent_list.append([w2vec.dictionary.get_entity_by_index(idx) for idx in idxs])\n",
    "            \n",
    "            certain_ent_matrix = np.array([w2vec.get_vector(ent) for ent in certain_ent_list])\n",
    "            uncertain_ent_matrix_list = [np.array([w2vec.get_vector(ent) for ent in ent_list]) for ent_list in uncertain_ent_list]\n",
    "            pairs = []\n",
    "            certain_len = len(certain_ent_list)\n",
    "            uncertain_len = len(uncertain_ent_list)\n",
    "            if certain_len >= 1:\n",
    "                # Collect pairs between certain entities\n",
    "                result = cosine_similarity(certain_ent_matrix, certain_ent_matrix) - np.identity(certain_len)\n",
    "                for i in range(certain_len):\n",
    "                    for j in range(i+1, certain_len):\n",
    "                        pairs.append({'kw1':certain_ent_kw_list[i], 'kw2':certain_ent_kw_list[j], 'sim':float(result[i, j]), 'sent':sent_note, \n",
    "                            'kw1_ent':certain_ent_list[i].title, \n",
    "                            'kw2_ent':certain_ent_list[j].title})\n",
    "                # Collect pairs between certain and uncertain entities\n",
    "                for i in range(uncertain_len):\n",
    "                    result = cosine_similarity(certain_ent_matrix, uncertain_ent_matrix_list[i])\n",
    "                    for j in range(certain_len):\n",
    "                        idx = np.argmax(result[j])\n",
    "                        pairs.append({'kw1':uncertain_ent_kw_list[i], 'kw2':certain_ent_kw_list[j], 'sim':float(result[j, idx]), 'sent':sent_note, \n",
    "                            'kw1_ent':uncertain_ent_list[i][idx].title, \n",
    "                            'kw2_ent':certain_ent_list[j].title})\n",
    "            if uncertain_len >= 2:\n",
    "                # Collect pairs between uncertain entities\n",
    "                for i in range(uncertain_len):\n",
    "                    for j in range(i+1, uncertain_len):\n",
    "                        result = cosine_similarity(uncertain_ent_matrix_list[i], uncertain_ent_matrix_list[j])\n",
    "                        idx = np.argmax(result)\n",
    "                        row = int(idx / result.shape[1])\n",
    "                        col = idx % result.shape[1]\n",
    "                        pairs.append({'kw1':uncertain_ent_kw_list[i], 'kw2':uncertain_ent_kw_list[j], 'sim':float(result[row, col]), 'sent':sent_note, \n",
    "                            'kw1_ent':uncertain_ent_list[i][row].title, \n",
    "                            'kw2_ent':uncertain_ent_list[j][col].title})\n",
    "            doc = nlp(line)\n",
    "            for item in pairs:\n",
    "                kw1_spans = find_span(doc, item['kw1'], True)\n",
    "                kw2_spans = find_span(doc, item['kw2'], True)\n",
    "                for kw1_span in kw1_spans:\n",
    "                    for kw2_span in kw2_spans:\n",
    "                        path = find_dependency_path_from_tree(doc, kw1_span, kw2_span)\n",
    "                        if not path:\n",
    "                            continue\n",
    "                        item['kw1_span'] = (kw1_span[0].i, kw1_span[-1].i)\n",
    "                        item['kw2_span'] = (kw2_span[0].i, kw2_span[-1].i)\n",
    "                        item['path'] = path\n",
    "                        data.append(item.copy())\n",
    "        if not disable_pbar:\n",
    "            print(len(data))\n",
    "        return pd.DataFrame(data=data, columns=['sim', 'kw1', 'kw1_span', 'kw1_ent', 'kw2', 'kw2_span', 'kw2_ent', 'sent', 'path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data using general wikipedia keywords\n",
    "wiki_path_test_df = pd.concat([collect_test_paths_all(file, co) for file in save_sent_files[:3]], ignore_index=True)\n",
    "wiki_path_test_df.to_csv(path_test_file, sep='\\t', index=False)\n",
    "print(len(wiki_path_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data using general wikipedia keywords\n",
    "# line_per_file = [count_line(f) for f in save_cs_sent_files[:1100]]\n",
    "# print(sum(line_per_file))\n",
    "cs_path_test_df = pd.concat([collect_test_paths_all(file, cs_co, posfix='_cs.dat', disable_pbar=True) for file in tqdm.tqdm(save_cs_sent_files[:1100])], ignore_index=True)\n",
    "cs_path_test_df.to_csv(cs_path_test_file, sep='\\t', index=False)\n",
    "print(len(cs_path_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'programming language' in my_mention_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in my_mention_dict['programming language']:\n",
    "    print(w2vec.dictionary.get_item_by_index(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec.most_similar_by_vector(w2vec.get_entity_vector('Python (programming language)'), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(w2vec.get_entity_vector('Political movement').reshape(1, -1), w2vec.get_entity_vector('Hierarchy').reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = w2vec.get_entity('The World (radio program)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e.count)\n",
    "print(e.doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note2line('AA:00:0', posfix='_cs.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_path_test_data_sim_based(df:pd.DataFrame, sim_threshold:float=0.5):\n",
    "    # Find all keyword cooccurrence and keep the ones that are similar in wikipedia2vec\n",
    "    return df[df['sim'] >= sim_threshold]\n",
    "    \n",
    "# def collect_path_test_data_npmi_based(df:pd.DataFrame, npmi_threshold:float=0.1):\n",
    "#     # Find all keyword cooccurrence and keep the ones that are similar in wikipedia2vec\n",
    "#     return df[df['npmi'] >= npmi_threshold]\n",
    "    \n",
    "def map_path_pattern_to_test_data(df:pd.DataFrame, paths:list):\n",
    "    def match_path_pattern(path:str):\n",
    "        for pp in paths:\n",
    "            if exact_match(pp, path):\n",
    "                return pp\n",
    "        return ''\n",
    "    df['pattern'] = df.apply(lambda x: match_path_pattern(x['path']), axis=1)\n",
    "    # For each remained pair, get the dep path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_df = collect_path_test_data_sim_based(wiki_path_test_df, 0.0)\n",
    "cs_test_df = collect_path_test_data_sim_based(cs_path_test_df, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3923/1819995245.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['pattern'] = df.apply(lambda x: match_path_pattern(x['path']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "patterns = [\n",
    "    r'i_nsubj attr( prep pobj)*( compound){0,1}', \n",
    "    # r'i_nsubj attr( prep pobj)*', \n",
    "    r'i_nsubj( conj)* dobj( acl prep pobj( conj)*){0,1}', \n",
    "    r'i_nsubj( prep pobj)+( conj)*', \n",
    "    r'i_nsubj advcl dobj( acl attr){0,1}', \n",
    "    r'appos( conj)*', \n",
    "    r'appos acl prep pobj( conj)*', \n",
    "    r'i_nsubjpass( conj)*( prep pobj)+( conj)*', \n",
    "    r'i_nsubjpass prep pobj acl dobj', \n",
    "    r'i_dobj prep pobj( conj)*'\n",
    "    # 'acl prep pobj( conj)*'\n",
    "]\n",
    "map_path_pattern_to_test_data(wiki_test_df, patterns)\n",
    "map_path_pattern_to_test_data(cs_test_df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_path_result_sim_based(df:pd.DataFrame, paths:list):\n",
    "    summary_df = pd.DataFrame(columns=['path', 'cnt', 'ratio', 'avg_sim'])\n",
    "    for pp in paths:\n",
    "        sub_df = df[df['pattern'] == pp]\n",
    "        summary_df = summary_df.append({\n",
    "            'path' : pp,\n",
    "            'cnt' : len(sub_df),\n",
    "            'ratio' : len(sub_df) / len(df),\n",
    "            'avg_sim' : sum(sub_df['sim']) / len(sub_df) if len(sub_df) else 0\n",
    "        }, ignore_index=True)\n",
    "    summary_df = summary_df.append({\n",
    "        'path' : 'general',\n",
    "        'cnt' : len(df),\n",
    "        'ratio' : 1,\n",
    "        'avg_sim' : sum(df['sim']) / len(df) if len(df) else 0\n",
    "    }, ignore_index=True)\n",
    "    return summary_df\n",
    "\n",
    "# def analysis_path_result_npmi_based(df:pd.DataFrame, paths:list):\n",
    "#     summary_df = pd.DataFrame(columns=['path', 'cnt', 'ratio', 'avg_npmi'])\n",
    "#     for pp in paths:\n",
    "#         sub_df = df[df['pattern'] == pp]\n",
    "#         summary_df = summary_df.append({\n",
    "#             'path' : pp,\n",
    "#             'cnt' : len(sub_df),\n",
    "#             'ratio' : len(sub_df) / len(df),\n",
    "#             'avg_npmi' : sum(sub_df['npmi']) / len(sub_df) if len(sub_df) else 0\n",
    "#         }, ignore_index=True)\n",
    "#     summary_df = summary_df.append({\n",
    "#         'path' : 'general',\n",
    "#         'cnt' : len(df),\n",
    "#         'ratio' : 1,\n",
    "#         'avg_npmi' : sum(df['npmi']) / len(df) if len(df) else 0\n",
    "#     }, ignore_index=True)\n",
    "#     return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>cnt</th>\n",
       "      <th>ratio</th>\n",
       "      <th>avg_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i_nsubj attr( prep pobj)*( compound){0,1}</td>\n",
       "      <td>912</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.345963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i_nsubj( conj)* dobj( acl prep pobj( conj)*){0,1}</td>\n",
       "      <td>1351</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.341431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i_nsubj( prep pobj)+( conj)*</td>\n",
       "      <td>2485</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0.308623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i_nsubj advcl dobj( acl attr){0,1}</td>\n",
       "      <td>191</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.334822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>appos( conj)*</td>\n",
       "      <td>1228</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.369404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>appos acl prep pobj( conj)*</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.323274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i_nsubjpass( conj)*( prep pobj)+( conj)*</td>\n",
       "      <td>1686</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>0.312614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i_nsubjpass prep pobj acl dobj</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.329141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i_dobj prep pobj( conj)*</td>\n",
       "      <td>1769</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>0.306733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>general</td>\n",
       "      <td>426647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.319292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path     cnt     ratio  \\\n",
       "0          i_nsubj attr( prep pobj)*( compound){0,1}     912  0.002138   \n",
       "1  i_nsubj( conj)* dobj( acl prep pobj( conj)*){0,1}    1351  0.003167   \n",
       "2                       i_nsubj( prep pobj)+( conj)*    2485  0.005824   \n",
       "3                 i_nsubj advcl dobj( acl attr){0,1}     191  0.000448   \n",
       "4                                      appos( conj)*    1228  0.002878   \n",
       "5                        appos acl prep pobj( conj)*      30  0.000070   \n",
       "6           i_nsubjpass( conj)*( prep pobj)+( conj)*    1686  0.003952   \n",
       "7                     i_nsubjpass prep pobj acl dobj       9  0.000021   \n",
       "8                           i_dobj prep pobj( conj)*    1769  0.004146   \n",
       "9                                            general  426647  1.000000   \n",
       "\n",
       "    avg_sim  \n",
       "0  0.345963  \n",
       "1  0.341431  \n",
       "2  0.308623  \n",
       "3  0.334822  \n",
       "4  0.369404  \n",
       "5  0.323274  \n",
       "6  0.312614  \n",
       "7  0.329141  \n",
       "8  0.306733  \n",
       "9  0.319292  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_path_result_sim_based(wiki_test_df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>cnt</th>\n",
       "      <th>ratio</th>\n",
       "      <th>avg_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i_nsubj attr( prep pobj)*( compound){0,1}</td>\n",
       "      <td>584</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>0.484567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i_nsubj( conj)* dobj( acl prep pobj( conj)*){0,1}</td>\n",
       "      <td>605</td>\n",
       "      <td>0.005451</td>\n",
       "      <td>0.453143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i_nsubj( prep pobj)+( conj)*</td>\n",
       "      <td>674</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>0.472138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i_nsubj advcl dobj( acl attr){0,1}</td>\n",
       "      <td>76</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.442264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>appos( conj)*</td>\n",
       "      <td>314</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.465873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>appos acl prep pobj( conj)*</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.487080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i_nsubjpass( conj)*( prep pobj)+( conj)*</td>\n",
       "      <td>618</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>0.477143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i_nsubjpass prep pobj acl dobj</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.472865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i_dobj prep pobj( conj)*</td>\n",
       "      <td>355</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.480274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>general</td>\n",
       "      <td>110986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.453584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path     cnt     ratio  \\\n",
       "0          i_nsubj attr( prep pobj)*( compound){0,1}     584  0.005262   \n",
       "1  i_nsubj( conj)* dobj( acl prep pobj( conj)*){0,1}     605  0.005451   \n",
       "2                       i_nsubj( prep pobj)+( conj)*     674  0.006073   \n",
       "3                 i_nsubj advcl dobj( acl attr){0,1}      76  0.000685   \n",
       "4                                      appos( conj)*     314  0.002829   \n",
       "5                        appos acl prep pobj( conj)*       6  0.000054   \n",
       "6           i_nsubjpass( conj)*( prep pobj)+( conj)*     618  0.005568   \n",
       "7                     i_nsubjpass prep pobj acl dobj       6  0.000054   \n",
       "8                           i_dobj prep pobj( conj)*     355  0.003199   \n",
       "9                                            general  110986  1.000000   \n",
       "\n",
       "    avg_sim  \n",
       "0  0.484567  \n",
       "1  0.453143  \n",
       "2  0.472138  \n",
       "3  0.442264  \n",
       "4  0.465873  \n",
       "5  0.487080  \n",
       "6  0.477143  \n",
       "7  0.472865  \n",
       "8  0.480274  \n",
       "9  0.453584  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_path_result_sim_based(cs_test_df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_example_sent_for_pattern(df:pd.DataFrame, path:str, num:int=30, posfix:str='.dat'):\n",
    "    sub_df = df[df['pattern'] == path]\n",
    "    num = min(len(sub_df), num)\n",
    "    sub_df = sub_df[:num]\n",
    "    sub_df['sent'] = sub_df.apply(lambda x: note2line(x['sent'], posfix=posfix).strip(), axis=1)\n",
    "    return sub_df\n",
    "\n",
    "for patt in patterns:\n",
    "    temp_df = collect_example_sent_for_pattern(wiki_test_df, patt)\n",
    "    temp_df.to_csv('%s.tsv' % (patt[:10] if len(patt) >= 10 else patt), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keyword occur dict which has occurance record for all keywords in selected sentences\n",
    "with open(keyword_occur_file, 'rb') as f_in:\n",
    "    keyword_occur = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keyword connection graph in selected sentences\n",
    "with open(keyword_connection_graph_file, 'rb') as f_in:\n",
    "    keyword_connection_graph = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keyword count file\n",
    "with open(keyword_count_file, 'rb') as f_in:\n",
    "    keyword_count = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mention to index file\n",
    "with open(w2vec_keyword2idx_file, 'rb') as f_in:\n",
    "    my_mention_dict = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load path test data (pd.DataFrame)\n",
    "wiki_path_test_df = pd.read_csv(open(path_test_file), sep='\\t')\n",
    "cs_path_test_df = pd.read_csv(open(cs_path_test_file), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def reverse_path(path:str):\n",
    "    path = path.split()\n",
    "    r_path = ' '.join(['i_' + token if token[:2] != 'i_' else token[2:] for token in reversed(path)])\n",
    "    return r_path\n",
    "\n",
    "# sub_df = wiki_path_test_df[wiki_path_test_df['sim'] > 0.5]\n",
    "sub_df = cs_path_test_df[cs_path_test_df['npmi'] > 0.15]\n",
    "sub_df['pick'] = sub_df.apply(lambda x: 1 if 'nsubj' in x['path'] else 0, axis=1)\n",
    "sub_df = sub_df[sub_df['pick'] > 0]\n",
    "sub_df['path'] = sub_df.apply(lambda x: reverse_path(x['path']) if 'i_nsubj' not in x['path'] else x['path'], axis=1)\n",
    "# sub_df['sent'] = sub_df.apply(lambda x: note2line(x['sent']).strip(), axis=1)\n",
    "sub_df['sent'] = sub_df.apply(lambda x: note2line2(x['sent']).strip(), axis=1)\n",
    "c = Counter(sub_df['path'].to_list())\n",
    "# for k in c.keys():\n",
    "#     if c[k] < 5:\n",
    "#         c[k] = 0\n",
    "max_cnt = c.most_common(1)[0][1]\n",
    "log_max_cnt = np.log(max_cnt)\n",
    "def cal_score(path:str):\n",
    "    cnt = max(c.get(path), 1)\n",
    "    return np.log(cnt) / log_max_cnt\n",
    "sub_df['score'] = sub_df.apply(lambda x: cal_score(x['path']), axis=1)\n",
    "# sub_df.to_csv('nsubj.tsv', index=False, sep='\\t', columns=['sim', 'kw1', 'kw1_span', 'kw1_ent', 'kw2', 'kw2_span', 'kw2_ent', 'sent', 'path', 'score'])\n",
    "sub_df.to_csv('nsubj_cs.tsv', index=False, sep='\\t', columns=['npmi', 'kw1', 'kw1_span', 'kw2', 'kw2_span', 'sent', 'path', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keyword connection graph in cooccurance files\n",
    "with open(keyword_npmi_graph_file_v2, 'rb') as f_in:\n",
    "    keyword_npmi_graph = pickle.load(f_in)\n",
    "for k, v in keyword_npmi_graph[1].items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo function: find all the sentences that two keywords co-occur in selected sentences\n",
    "def find_sentences(keyword_dict:dict, kw1:str, kw2:str):\n",
    "    kw1_occur = keyword_dict.get(kw1)\n",
    "    kw2_occur = keyword_dict.get(kw2)\n",
    "    sents = pd.DataFrame(columns=['head', 'head_norm', 'head_span', 'tail', 'tail_norm', 'tail_span', 'sent', 'path'])\n",
    "    if not kw1_occur or not kw2_occur:\n",
    "        return sents\n",
    "    co_occur = kw1_occur & kw2_occur\n",
    "    file_dict = {}\n",
    "    for occur in co_occur:\n",
    "        sub_file, line_idx = occur.rsplit(':', 1)\n",
    "        if sub_file not in file_dict:\n",
    "            file_dict[sub_file] = []\n",
    "        file_dict[sub_file].append(int(line_idx))\n",
    "    for f, lines in file_dict.items():\n",
    "        sentence_in_file = my_read(os.path.join(save_path, f.replace(':', '/wiki_')+'.dat'))\n",
    "        records = my_read(os.path.join(save_path, f.replace(':', '/wiki_')+'.tsv'))\n",
    "        for idx in lines:\n",
    "            record = records[idx].split('\\t')\n",
    "            sent = sentence_in_file[int(record[6])]\n",
    "            sents = sents.append({  'head':record[0],\n",
    "                                    'head_norm':record[1],\n",
    "                                    'head_span':record[2],\n",
    "                                    'tail':record[3],\n",
    "                                    'tail_norm':record[4],\n",
    "                                    'tail_span':record[5],\n",
    "                                    'sent':sent,\n",
    "                                    'path':record[7]}, ignore_index=True)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = find_sentences(keyword_occur, 'python', 'programming language')\n",
    "df.to_csv('sents.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'decision tree' in keyword_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keyword_occur['machine learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw1 = 'data mining'\n",
    "kw2 = 'machine learning'\n",
    "doc = nlp('Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.'.lower())\n",
    "kw1_span = find_span(doc, kw1)\n",
    "kw2_span = find_span(doc, kw2)\n",
    "find_dependency_path_from_tree(doc, kw1_span[0], kw2_span[0])\n",
    "# print(len(kw1_span))\n",
    "# print(len(kw2_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keyword_connection_graph.neighbors('decision tree')\n",
    "my_write('neighbors.txt', list(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    remove_list = ['See also', 'References', 'Further reading', 'Sources', 'External links']\n",
    "    dic = {sec : page.section(sec) for sec in page.sections}\n",
    "    dic['summary'] = page.summary\n",
    "    sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    while len(section_list) > 0:\n",
    "        section = section_list.pop()\n",
    "        if section in remove_list:\n",
    "            continue\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        # processed_text = clean_text(section_text)\n",
    "        processed_text = ' '.join(section_text.lower().split())\n",
    "        temp_sents = my_sentence_tokenize(processed_text, True)\n",
    "        sents += temp_sents\n",
    "    return list(sents)\n",
    "\n",
    "def collect_entity_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    return [text.lower() for text in page.links]\n",
    "\n",
    "def collect_keyword_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    soup = BeautifulSoup(page.html(), 'html.parser')\n",
    "    main_block = soup.find('div', class_='mw-parser-output')\n",
    "    keywords = set([l.text.lower() for l in main_block.findAll('a') if re.match(r'^(<a href=\"/wiki/)', str(l))])\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'python'\n",
    "\n",
    "p = wikipedia.page(keyword)\n",
    "if p is not None:\n",
    "    sents = collect_sents_from_wiki_page(p)\n",
    "    keywords = collect_keyword_from_wiki_page(p)\n",
    "    print('sentences collected')\n",
    "    my_write('%s.txt' % keyword, sents)\n",
    "    my_write('%s_kw.txt' % keyword, keywords)\n",
    "    df = filter_by_path(sents)\n",
    "    df.to_csv('%s_out.tsv' % keyword, sep='\\t', index=False)\n",
    "\n",
    "    dff = df[df.apply(lambda x: str(x['head']) in keywords and str(x['tail']) in keywords, axis=1)]\n",
    "    dff.to_csv('%s_out_f.tsv' % keyword, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wanted'] = df.apply(lambda x: str(x['head']) in keywords, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dff)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9496c91418be784f00ee6456e4343e8188c649322b68f201c83241a4029a42d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('FWD_py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
