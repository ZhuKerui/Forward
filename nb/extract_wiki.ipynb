{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences from Wikipedia\n",
    "+ This notebook is used for collecting sentences that tell relationship between two entities from wikipedia using some dependency path pattern\n",
    "+ **This notebook is fully valid under Owl3 machine (using the /scratch/data/wikipedia/full_text-2021-03-20 data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki sub folder example: ../../data/wikipedia/full_text-2021-03-20/BE\n",
      "save sub folder example: data/extract_wiki/wiki_sent_collect/BE\n",
      "wiki file example: ../../data/wikipedia/full_text-2021-03-20/BE/wiki_00\n",
      "save sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.dat\n",
      "save cooccur file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_co.dat\n",
      "save selected sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.tsv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wikipedia\n",
    "import os\n",
    "import pickle\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "import bz2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from tools.BasicUtils import my_read, my_write, MyMultiProcessing\n",
    "from tools.TextProcessing import (\n",
    "                normalize_text, remove_brackets, my_sentence_tokenize, build_word_tree_v2, \n",
    "                my_sentence_tokenize, filter_specific_keywords, find_dependency_path_from_tree, find_span, nlp, \n",
    "                sent_lemmatize\n",
    "                )\n",
    "from tools.DocProcessing import CoOccurrence\n",
    "\n",
    "from extract_wiki import SentenceFilter\n",
    "\n",
    "from extract_wiki import (\n",
    "    wikipedia_dir, wikipedia_entity_file, wikipedia_entity_norm_file, \n",
    "    wikipedia_keyword_file, wikipedia_token_file, wikipedia_wordtree_file, wikipedia_keyword_filtered_file, keyword_npmi_graph_file_v2, \n",
    "    save_path, keyword_occur_file, keyword_connection_graph_file, w2vec_dump_file, w2vec_keyword_file, w2vec_wordtree_file, w2vec_token_file, w2vec_keyword2idx_file, \n",
    "    collect_wiki_entity, get_sentence, keyword_count_file, filter_keyword_by_freq, line2note, note2line\n",
    ")\n",
    "\n",
    "test_path = 'data/extract_wiki/wiki_sent_test'\n",
    "\n",
    "# Generate the save dir\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.mkdir(test_path)\n",
    "\n",
    "sub_folders = [sub for sub in os.listdir(wikipedia_dir)]\n",
    "save_sub_folders = [os.path.join(save_path, sub) for sub in sub_folders]\n",
    "wiki_sub_folders = [os.path.join(wikipedia_dir, sub) for sub in sub_folders]\n",
    "\n",
    "wiki_files = []\n",
    "save_sent_files = []\n",
    "save_cooccur_files = []\n",
    "save_selected_files = []\n",
    "\n",
    "for save_dir in save_sub_folders:\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "for i in range(len(wiki_sub_folders)):\n",
    "    files = [f for f in os.listdir(wiki_sub_folders[i])]\n",
    "    wiki_files += [os.path.join(wiki_sub_folders[i], f) for f in files]\n",
    "    save_sent_files += [os.path.join(save_sub_folders[i], f+'.dat') for f in files]\n",
    "    save_cooccur_files += [os.path.join(save_sub_folders[i], f+'_co.dat') for f in files]\n",
    "    save_selected_files += [os.path.join(save_sub_folders[i], f+'.tsv') for f in files]\n",
    "\n",
    "# Get all files under wikipedia/full_text-2021-03-20\n",
    "\n",
    "print('wiki sub folder example:', wiki_sub_folders[0])\n",
    "print('save sub folder example:', save_sub_folders[0])\n",
    "print('wiki file example:', wiki_files[0])\n",
    "print('save sentence file example:', save_sent_files[0])\n",
    "print('save cooccur file example:', save_cooccur_files[0])\n",
    "print('save selected sentence file example:', save_selected_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect wikipedia page titles as entities and generate keyword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect wikipedia entities\n",
    "p = MyMultiProcessing(10)\n",
    "output = p.run(collect_wiki_entity, wiki_files)\n",
    "entity_list = []\n",
    "for l in output:\n",
    "    entity_list += l\n",
    "my_write(wikipedia_entity_file, entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get normalized wikipedia entities\n",
    "normalized_entity = []\n",
    "for kw in open(wikipedia_entity_file).readlines():\n",
    "    eid, ent = kw.split('\\t')\n",
    "    normalized_entity.append('%s\\t%s' % (eid, normalize_text(ent)))\n",
    "my_write(wikipedia_entity_norm_file, normalized_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate keyword list file\n",
    "keywords = [remove_brackets(line.strip().split('\\t')[1]) for line in open(wikipedia_entity_norm_file)]\n",
    "keywords = [kw for kw in keywords if kw.split()]\n",
    "keywords = filter_specific_keywords(keywords)\n",
    "keywords = list(set(keywords))\n",
    "my_write(wikipedia_keyword_file, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build wordtree\n",
    "build_word_tree_v2(wikipedia_keyword_file, wikipedia_wordtree_file, wikipedia_token_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect sentences from wikipedia and select good sentences by path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the wikipedia page files to sentence only file (12 min)\n",
    "p = MyMultiProcessing(10)\n",
    "wiki_sent_pair = [(wiki_files[i], save_sent_files[i]) for i in range(len(wiki_files))]\n",
    "output = p.run(get_sentence, wiki_sent_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sentence filter\n",
    "sf = SentenceFilter(wikipedia_wordtree_file, wikipedia_token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sentence filter\n",
    "sf = SentenceFilter(wikipedia_wordtree_file, wikipedia_token_file)\n",
    "def collect_sents(save_sent_file:str, save_selected_file:str):\n",
    "    sents = my_read(save_sent_file)\n",
    "    df = sf.list_operation(sents, use_id=True, keyword_only=True)\n",
    "    df.to_csv(save_selected_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of selecting sentences by path ['collect_sents']\n",
    "test_list = [(save_sent_files[i], '%d.tsv' % i) for i in range(20)]\n",
    "test_output = p.run(collect_sents, test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the keyword occurance (15 min) ['collect_kw_occur_from_selected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build connected graph from selected sentences(18 min) ['build_graph_from_selected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect keyword cooccurance from sentence files (2 hours) ['collect_kw_occur_from_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the less frequent keywords using cooccurance files (8 min)\n",
    "keyword_count = filter_keyword_by_freq(save_cooccur_file_list=save_cooccur_files)\n",
    "with open(keyword_count_file, 'wb') as f_out:\n",
    "    pickle.dump(keyword_count, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = {k : v for k, v in keyword_count.items() if v >= 300}\n",
    "my_write(wikipedia_keyword_filtered_file, list(f.keys()))\n",
    "print(len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'python' in f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build connected graph from cooccurance files () ['build_graph_from_cooccur]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia2vec implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keruiz2/miniconda3/envs/FWD_py38/lib/python3.8/contextlib.py:113: UserWarning: \"<bz2.BZ2File object at 0x7fab981cad30>\" is not a raw file, mmap_mode \"c\" flag will be ignored.\n",
      "  return next(self.gen)\n"
     ]
    }
   ],
   "source": [
    "with bz2.open(w2vec_dump_file) as f_in:\n",
    "    w2vec = Wikipedia2Vec.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform keywords into index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330958/330958 [00:00<00:00, 349055.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start building wordtree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330958/330958 [00:01<00:00, 296610.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word tree is accomplished with 330958 words added\n",
      "Total time taken in :  build_word_tree_v2 2.7751083374023438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "330958"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mention_dict = {}\n",
    "for ent in w2vec.dictionary.entities():\n",
    "    if ent.count < 50:\n",
    "        continue\n",
    "    kw = remove_brackets(normalize_text(ent.title))\n",
    "    if kw not in my_mention_dict:\n",
    "        my_mention_dict[kw] = [ent.index]\n",
    "    else:\n",
    "        my_mention_dict[kw].append(ent.index)\n",
    "w2vec_kws = filter_specific_keywords(list(my_mention_dict.keys()))\n",
    "my_write(w2vec_keyword_file, w2vec_kws)\n",
    "build_word_tree_v2(w2vec_keyword_file, w2vec_wordtree_file, w2vec_token_file)\n",
    "filter_keyword_from_w2vec = set(w2vec_kws)\n",
    "my_mention_dict = {k:v for k, v in my_mention_dict.items() if k in filter_keyword_from_w2vec}\n",
    "with open(w2vec_keyword2idx_file, 'wb') as f_out:\n",
    "    pickle.dump(my_mention_dict, f_out)\n",
    "len(my_mention_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = CoOccurrence(w2vec_wordtree_file, w2vec_token_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anarchism',\n",
       " 'authority',\n",
       " 'hierarchy',\n",
       " 'political movement',\n",
       " 'political philosophy'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.line_operation(sent_lemmatize('anarchism is a political philosophy and political movement that is sceptical of authority and rejects all involuntary , coercive forms of hierarchy.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_test_paths_all(test_file:str):\n",
    "    # Build test data\n",
    "    with open(test_file) as f_in:\n",
    "        data = []\n",
    "        for line_idx, line in enumerate(tqdm.tqdm(f_in.readlines())):\n",
    "            sent_note = line2note(test_file, line_idx)\n",
    "            line = line.strip()\n",
    "            co_kws = list(co.line_operation(sent_lemmatize(line)))\n",
    "            if len(co_kws) < 2:\n",
    "                continue\n",
    "            certain_ent_list = []\n",
    "            certain_ent_kw_list = []\n",
    "            uncertain_ent_list = []\n",
    "            uncertain_ent_kw_list = []\n",
    "            for kw in co_kws:\n",
    "                idxs = my_mention_dict[kw]\n",
    "                if len(idxs) == 1:\n",
    "                    certain_ent_kw_list.append(kw)\n",
    "                    certain_ent_list.append(w2vec.dictionary.get_entity_by_index(idxs[0]))\n",
    "                else:\n",
    "                    uncertain_ent_kw_list.append(kw)\n",
    "                    uncertain_ent_list.append([w2vec.dictionary.get_entity_by_index(idx) for idx in idxs])\n",
    "            \n",
    "            certain_ent_matrix = np.array([w2vec.get_vector(ent) for ent in certain_ent_list])\n",
    "            uncertain_ent_matrix_list = [np.array([w2vec.get_vector(ent) for ent in ent_list]) for ent_list in uncertain_ent_list]\n",
    "            pairs = []\n",
    "            certain_len = len(certain_ent_list)\n",
    "            uncertain_len = len(uncertain_ent_list)\n",
    "            if certain_len >= 1:\n",
    "                # Collect pairs between certain entities\n",
    "                result = cosine_similarity(certain_ent_matrix, certain_ent_matrix) - np.identity(certain_len)\n",
    "                for i in range(certain_len):\n",
    "                    for j in range(i+1, certain_len):\n",
    "                        pairs.append({'kw1':certain_ent_kw_list[i], 'kw2':certain_ent_kw_list[j], 'sim':float(result[i, j]), 'sent':sent_note, \n",
    "                            'kw1_ent':certain_ent_list[i].title, \n",
    "                            'kw2_ent':certain_ent_list[j].title})\n",
    "                # Collect pairs between certain and uncertain entities\n",
    "                for i in range(uncertain_len):\n",
    "                    result = cosine_similarity(certain_ent_matrix, uncertain_ent_matrix_list[i])\n",
    "                    for j in range(certain_len):\n",
    "                        idx = np.argmax(result[j])\n",
    "                        pairs.append({'kw1':uncertain_ent_kw_list[i], 'kw2':certain_ent_kw_list[j], 'sim':float(result[j, idx]), 'sent':sent_note, \n",
    "                            'kw1_ent':uncertain_ent_list[i][idx].title, \n",
    "                            'kw2_ent':certain_ent_list[j].title})\n",
    "            if uncertain_len >= 2:\n",
    "                # Collect pairs between uncertain entities\n",
    "                for i in range(uncertain_len):\n",
    "                    for j in range(i+1, uncertain_len):\n",
    "                        result = cosine_similarity(uncertain_ent_matrix_list[i], uncertain_ent_matrix_list[j])\n",
    "                        idx = np.argmax(result)\n",
    "                        row = int(idx / result.shape[1])\n",
    "                        col = idx % result.shape[1]\n",
    "                        # print(row)\n",
    "                        # print(col)\n",
    "                        pairs.append({'kw1':uncertain_ent_kw_list[i], 'kw2':uncertain_ent_kw_list[j], 'sim':float(result[row, col]), 'sent':sent_note, \n",
    "                            'kw1_ent':uncertain_ent_list[i][row].title, \n",
    "                            'kw2_ent':uncertain_ent_list[j][col].title})\n",
    "            doc = nlp(line)\n",
    "            for item in pairs:\n",
    "                kw1_spans = find_span(doc, item['kw1'], True)\n",
    "                kw2_spans = find_span(doc, item['kw2'], True)\n",
    "                for kw1_span in kw1_spans:\n",
    "                    for kw2_span in kw2_spans:\n",
    "                        path = find_dependency_path_from_tree(doc, kw1_span, kw2_span)\n",
    "                        item['kw1_span'] = (kw1_span[0].i, kw1_span[-1].i)\n",
    "                        item['kw2_span'] = (kw2_span[0].i, kw2_span[-1].i)\n",
    "                        item['path'] = path\n",
    "                        data.append(item.copy())\n",
    "        \n",
    "        return pd.DataFrame(data=data, columns=['sim', 'kw1', 'kw1_span', 'kw1_ent', 'kw2', 'kw2_span', 'kw2_ent', 'sent', 'path'])\n",
    "    # Find all keyword cooccurrence and keep the ones that are similar in wikipedia2vec\n",
    "    # For each remained pair, get the dep path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5797/5797 [01:23<00:00, 69.19it/s]\n"
     ]
    }
   ],
   "source": [
    "df = collect_test_paths_all(save_path+'/AA/wiki_00.dat')\n",
    "df.to_csv(save_path + '/test.tsv', sep='\\t', index=False)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>kw1</th>\n",
       "      <th>kw1_span</th>\n",
       "      <th>kw1_ent</th>\n",
       "      <th>kw2</th>\n",
       "      <th>kw2_span</th>\n",
       "      <th>kw2_ent</th>\n",
       "      <th>sent</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.556184</td>\n",
       "      <td>political movement</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>Political movement</td>\n",
       "      <td>political philosophy</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Political philosophy</td>\n",
       "      <td>AA:00:0</td>\n",
       "      <td>i_conj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.420070</td>\n",
       "      <td>political movement</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>Political movement</td>\n",
       "      <td>authority</td>\n",
       "      <td>(12, 12)</td>\n",
       "      <td>Authority</td>\n",
       "      <td>AA:00:0</td>\n",
       "      <td>i_conj relcl acomp prep pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.437531</td>\n",
       "      <td>political movement</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>Political movement</td>\n",
       "      <td>hierarchy</td>\n",
       "      <td>(21, 21)</td>\n",
       "      <td>Hierarchy</td>\n",
       "      <td>AA:00:0</td>\n",
       "      <td>i_conj relcl conj dobj prep pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.533027</td>\n",
       "      <td>political movement</td>\n",
       "      <td>(6, 7)</td>\n",
       "      <td>Political movement</td>\n",
       "      <td>anarchism</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>AA:00:0</td>\n",
       "      <td>i_conj i_attr nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.558961</td>\n",
       "      <td>political philosophy</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Political philosophy</td>\n",
       "      <td>authority</td>\n",
       "      <td>(12, 12)</td>\n",
       "      <td>Authority</td>\n",
       "      <td>AA:00:0</td>\n",
       "      <td>relcl acomp prep pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.380747</td>\n",
       "      <td>political philosophy</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Political philosophy</td>\n",
       "      <td>hierarchy</td>\n",
       "      <td>(21, 21)</td>\n",
       "      <td>Hierarchy</td>\n",
       "      <td>AA:00:0</td>\n",
       "      <td>relcl conj dobj prep pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.619777</td>\n",
       "      <td>political philosophy</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Political philosophy</td>\n",
       "      <td>anarchism</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>AA:00:0</td>\n",
       "      <td>i_attr nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.576076</td>\n",
       "      <td>authority</td>\n",
       "      <td>(12, 12)</td>\n",
       "      <td>Authority</td>\n",
       "      <td>hierarchy</td>\n",
       "      <td>(21, 21)</td>\n",
       "      <td>Hierarchy</td>\n",
       "      <td>AA:00:0</td>\n",
       "      <td>i_pobj i_prep i_acomp conj dobj prep pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.444457</td>\n",
       "      <td>authority</td>\n",
       "      <td>(12, 12)</td>\n",
       "      <td>Authority</td>\n",
       "      <td>anarchism</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>AA:00:0</td>\n",
       "      <td>i_pobj i_prep i_acomp i_relcl i_attr nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.408588</td>\n",
       "      <td>hierarchy</td>\n",
       "      <td>(21, 21)</td>\n",
       "      <td>Hierarchy</td>\n",
       "      <td>anarchism</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>Anarchism</td>\n",
       "      <td>AA:00:0</td>\n",
       "      <td>i_pobj i_prep i_dobj i_conj i_relcl i_attr nsubj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sim                   kw1  kw1_span               kw1_ent  \\\n",
       "0  0.556184    political movement    (6, 7)    Political movement   \n",
       "1  0.420070    political movement    (6, 7)    Political movement   \n",
       "2  0.437531    political movement    (6, 7)    Political movement   \n",
       "3  0.533027    political movement    (6, 7)    Political movement   \n",
       "4  0.558961  political philosophy    (3, 4)  Political philosophy   \n",
       "5  0.380747  political philosophy    (3, 4)  Political philosophy   \n",
       "6  0.619777  political philosophy    (3, 4)  Political philosophy   \n",
       "7  0.576076             authority  (12, 12)             Authority   \n",
       "8  0.444457             authority  (12, 12)             Authority   \n",
       "9  0.408588             hierarchy  (21, 21)             Hierarchy   \n",
       "\n",
       "                    kw2  kw2_span               kw2_ent     sent  \\\n",
       "0  political philosophy    (3, 4)  Political philosophy  AA:00:0   \n",
       "1             authority  (12, 12)             Authority  AA:00:0   \n",
       "2             hierarchy  (21, 21)             Hierarchy  AA:00:0   \n",
       "3             anarchism    (0, 0)             Anarchism  AA:00:0   \n",
       "4             authority  (12, 12)             Authority  AA:00:0   \n",
       "5             hierarchy  (21, 21)             Hierarchy  AA:00:0   \n",
       "6             anarchism    (0, 0)             Anarchism  AA:00:0   \n",
       "7             hierarchy  (21, 21)             Hierarchy  AA:00:0   \n",
       "8             anarchism    (0, 0)             Anarchism  AA:00:0   \n",
       "9             anarchism    (0, 0)             Anarchism  AA:00:0   \n",
       "\n",
       "                                               path  \n",
       "0                                            i_conj  \n",
       "1                      i_conj relcl acomp prep pobj  \n",
       "2                  i_conj relcl conj dobj prep pobj  \n",
       "3                               i_conj i_attr nsubj  \n",
       "4                             relcl acomp prep pobj  \n",
       "5                         relcl conj dobj prep pobj  \n",
       "6                                      i_attr nsubj  \n",
       "7         i_pobj i_prep i_acomp conj dobj prep pobj  \n",
       "8        i_pobj i_prep i_acomp i_relcl i_attr nsubj  \n",
       "9  i_pobj i_prep i_dobj i_conj i_relcl i_attr nsubj  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df[df['sim'] < 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>kw1</th>\n",
       "      <th>kw1_span</th>\n",
       "      <th>kw1_ent</th>\n",
       "      <th>kw2</th>\n",
       "      <th>kw2_span</th>\n",
       "      <th>kw2_ent</th>\n",
       "      <th>sent</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>-0.038170</td>\n",
       "      <td>france</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>France</td>\n",
       "      <td>the world</td>\n",
       "      <td>(24, 25)</td>\n",
       "      <td>The World (radio program)</td>\n",
       "      <td>AA:00:94</td>\n",
       "      <td>i_pobj i_prep nsubj prep pobj prep pobj conj p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>-0.018074</td>\n",
       "      <td>1920</td>\n",
       "      <td>(10, 10)</td>\n",
       "      <td>1920</td>\n",
       "      <td>current</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>Current (stream)</td>\n",
       "      <td>AA:00:222</td>\n",
       "      <td>i_conj i_pobj i_prep i_conj i_relcl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4709</th>\n",
       "      <td>-0.041801</td>\n",
       "      <td>pupil</td>\n",
       "      <td>(18, 18)</td>\n",
       "      <td>Pupil</td>\n",
       "      <td>attendance</td>\n",
       "      <td>(28, 28)</td>\n",
       "      <td>Attendance</td>\n",
       "      <td>AA:00:239</td>\n",
       "      <td>i_dative dobj prep pobj prep pcomp dobj conj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4711</th>\n",
       "      <td>-0.016279</td>\n",
       "      <td>attendance</td>\n",
       "      <td>(28, 28)</td>\n",
       "      <td>Attendance</td>\n",
       "      <td>autonomy</td>\n",
       "      <td>(22, 22)</td>\n",
       "      <td>Autonomy</td>\n",
       "      <td>AA:00:239</td>\n",
       "      <td>i_conj i_dobj i_pcomp i_prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5175</th>\n",
       "      <td>-0.039343</td>\n",
       "      <td>brighton</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>Brighton (UK Parliament constituency)</td>\n",
       "      <td>name</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>Name</td>\n",
       "      <td>AA:00:251</td>\n",
       "      <td>i_pobj i_prep i_appos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sim         kw1  kw1_span                                kw1_ent  \\\n",
       "1612 -0.038170      france    (1, 1)                                 France   \n",
       "4339 -0.018074        1920  (10, 10)                                   1920   \n",
       "4709 -0.041801       pupil  (18, 18)                                  Pupil   \n",
       "4711 -0.016279  attendance  (28, 28)                             Attendance   \n",
       "5175 -0.039343    brighton  (40, 40)  Brighton (UK Parliament constituency)   \n",
       "\n",
       "             kw2  kw2_span                    kw2_ent       sent  \\\n",
       "1612   the world  (24, 25)  The World (radio program)   AA:00:94   \n",
       "4339     current    (2, 2)           Current (stream)  AA:00:222   \n",
       "4709  attendance  (28, 28)                 Attendance  AA:00:239   \n",
       "4711    autonomy  (22, 22)                   Autonomy  AA:00:239   \n",
       "5175        name    (2, 2)                       Name  AA:00:251   \n",
       "\n",
       "                                                   path  \n",
       "1612  i_pobj i_prep nsubj prep pobj prep pobj conj p...  \n",
       "4339                i_conj i_pobj i_prep i_conj i_relcl  \n",
       "4709       i_dative dobj prep pobj prep pcomp dobj conj  \n",
       "4711                       i_conj i_dobj i_pcomp i_prep  \n",
       "5175                              i_pobj i_prep i_appos  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'programming language' in my_mention_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Entity Programming language>\n"
     ]
    }
   ],
   "source": [
    "for idx in my_mention_dict['programming language']:\n",
    "    print(w2vec.dictionary.get_item_by_index(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec.most_similar_by_vector(w2vec.get_entity_vector('Python (programming language)'), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43753132]], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(w2vec.get_entity_vector('Political movement').reshape(1, -1), w2vec.get_entity_vector('Hierarchy').reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = w2vec.get_entity('The World (radio program)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "print(e.count)\n",
    "print(e.doc_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keyword occur dict which has occurance record for all keywords in selected sentences\n",
    "with open(keyword_occur_file, 'rb') as f_in:\n",
    "    keyword_occur = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keyword connection graph in selected sentences\n",
    "with open(keyword_connection_graph_file, 'rb') as f_in:\n",
    "    keyword_connection_graph = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keyword count file\n",
    "with open(keyword_count_file, 'rb') as f_in:\n",
    "    keyword_count = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mention to index file\n",
    "with open(w2vec_keyword2idx_file, 'rb') as f_in:\n",
    "    my_mention_dict = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load keyword connection graph in cooccurance files\n",
    "with open(keyword_npmi_graph_file_v2, 'rb') as f_in:\n",
    "    keyword_npmi_graph = pickle.load(f_in)\n",
    "for k, v in keyword_npmi_graph[1].items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo function: find all the sentences that two keywords co-occur in selected sentences\n",
    "def find_sentences(keyword_dict:dict, kw1:str, kw2:str):\n",
    "    kw1_occur = keyword_dict.get(kw1)\n",
    "    kw2_occur = keyword_dict.get(kw2)\n",
    "    sents = pd.DataFrame(columns=['head', 'head_norm', 'head_span', 'tail', 'tail_norm', 'tail_span', 'sent', 'path'])\n",
    "    if not kw1_occur or not kw2_occur:\n",
    "        return sents\n",
    "    co_occur = kw1_occur & kw2_occur\n",
    "    file_dict = {}\n",
    "    for occur in co_occur:\n",
    "        sub_file, line_idx = occur.rsplit(':', 1)\n",
    "        if sub_file not in file_dict:\n",
    "            file_dict[sub_file] = []\n",
    "        file_dict[sub_file].append(int(line_idx))\n",
    "    for f, lines in file_dict.items():\n",
    "        sentence_in_file = my_read(os.path.join(save_path, f.replace(':', '/wiki_')+'.dat'))\n",
    "        records = my_read(os.path.join(save_path, f.replace(':', '/wiki_')+'.tsv'))\n",
    "        for idx in lines:\n",
    "            record = records[idx].split('\\t')\n",
    "            sent = sentence_in_file[int(record[6])]\n",
    "            sents = sents.append({  'head':record[0],\n",
    "                                    'head_norm':record[1],\n",
    "                                    'head_span':record[2],\n",
    "                                    'tail':record[3],\n",
    "                                    'tail_norm':record[4],\n",
    "                                    'tail_span':record[5],\n",
    "                                    'sent':sent,\n",
    "                                    'path':record[7]}, ignore_index=True)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = find_sentences(keyword_occur, 'python', 'programming language')\n",
    "df.to_csv('sents.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'decision tree' in keyword_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keyword_occur['machine learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw1 = 'data mining'\n",
    "kw2 = 'machine learning'\n",
    "doc = nlp('Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.'.lower())\n",
    "kw1_span = find_span(doc, kw1)\n",
    "kw2_span = find_span(doc, kw2)\n",
    "find_dependency_path_from_tree(doc, kw1_span[0], kw2_span[0])\n",
    "# print(len(kw1_span))\n",
    "# print(len(kw2_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keyword_connection_graph.neighbors('decision tree')\n",
    "my_write('neighbors.txt', list(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    remove_list = ['See also', 'References', 'Further reading', 'Sources', 'External links']\n",
    "    dic = {sec : page.section(sec) for sec in page.sections}\n",
    "    dic['summary'] = page.summary\n",
    "    sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    while len(section_list) > 0:\n",
    "        section = section_list.pop()\n",
    "        if section in remove_list:\n",
    "            continue\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        # processed_text = clean_text(section_text)\n",
    "        processed_text = ' '.join(section_text.lower().split())\n",
    "        temp_sents = my_sentence_tokenize(processed_text, True)\n",
    "        sents += temp_sents\n",
    "    return list(sents)\n",
    "\n",
    "def collect_entity_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    return [text.lower() for text in page.links]\n",
    "\n",
    "def collect_keyword_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    soup = BeautifulSoup(page.html(), 'html.parser')\n",
    "    main_block = soup.find('div', class_='mw-parser-output')\n",
    "    keywords = set([l.text.lower() for l in main_block.findAll('a') if re.match(r'^(<a href=\"/wiki/)', str(l))])\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'python'\n",
    "\n",
    "p = wikipedia.page(keyword)\n",
    "if p is not None:\n",
    "    sents = collect_sents_from_wiki_page(p)\n",
    "    keywords = collect_keyword_from_wiki_page(p)\n",
    "    print('sentences collected')\n",
    "    my_write('%s.txt' % keyword, sents)\n",
    "    my_write('%s_kw.txt' % keyword, keywords)\n",
    "    df = filter_by_path(sents)\n",
    "    df.to_csv('%s_out.tsv' % keyword, sep='\\t', index=False)\n",
    "\n",
    "    dff = df[df.apply(lambda x: str(x['head']) in keywords and str(x['tail']) in keywords, axis=1)]\n",
    "    dff.to_csv('%s_out_f.tsv' % keyword, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wanted'] = df.apply(lambda x: str(x['head']) in keywords, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dff)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9496c91418be784f00ee6456e4343e8188c649322b68f201c83241a4029a42d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('FWD_py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
