{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Extract Sentences from Wikipedia\n",
    "+ This notebook is used for collecting sentences that tell relationship between two entities from wikipedia using some dependency path pattern\n",
    "+ **This notebook is fully valid under Owl3 machine (using the /scratch/data/wikipedia/full_text-2021-03-20 data)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wikipedia\n",
    "import os\n",
    "import pickle\n",
    "sys.path.append('..')\n",
    "\n",
    "from tools.BasicUtils import my_read, my_write, MyMultiProcessing\n",
    "from tools.TextProcessing import (\n",
    "                normalize_text, remove_brackets, my_sentence_tokenize, build_word_tree_v2, \n",
    "                my_sentence_tokenize, filter_specific_keywords, find_dependency_path_from_tree, find_span, nlp\n",
    "                )\n",
    "from tools.DocProcessing import SentenceFilter\n",
    "\n",
    "from extract_wiki import (\n",
    "    wikipedia_dir, wikipedia_entity_file, wikipedia_entity_norm_file, \n",
    "    wikipedia_keyword_file, wikipedia_token_file, wikipedia_wordtree_file, \n",
    "    save_path, keyword_occur_file, keyword_connection_graph_file,\n",
    "    collect_wiki_entity, get_sentence\n",
    ")\n",
    "\n",
    "\n",
    "# Generate the save dir\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "sub_folders = [sub for sub in os.listdir(wikipedia_dir)]\n",
    "save_sub_folders = [os.path.join(save_path, sub) for sub in sub_folders]\n",
    "wiki_sub_folders = [os.path.join(wikipedia_dir, sub) for sub in sub_folders]\n",
    "\n",
    "wiki_files = []\n",
    "save_sent_files = []\n",
    "save_selected_files = []\n",
    "for save_dir in save_sub_folders:\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "for i in range(len(wiki_sub_folders)):\n",
    "    files = [f for f in os.listdir(wiki_sub_folders[i])]\n",
    "    wiki_files += [os.path.join(wiki_sub_folders[i], f) for f in files]\n",
    "    save_sent_files += [os.path.join(save_sub_folders[i], f+'.dat') for f in files]\n",
    "    save_selected_files += [os.path.join(save_sub_folders[i], f+'.tsv') for f in files]\n",
    "\n",
    "# Get all files under wikipedia/full_text-2021-03-20\n",
    "\n",
    "print('wiki sub folder example:', wiki_sub_folders[0])\n",
    "print('save sub folder example:', save_sub_folders[0])\n",
    "print('wiki file example:', wiki_files[0])\n",
    "print('save sentence file example:', save_sent_files[0])\n",
    "print('save selected sentence file example:', save_selected_files[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "wiki sub folder example: ../../data/wikipedia/full_text-2021-03-20/BE\n",
      "save sub folder example: data/extract_wiki/wiki_sent_collect/BE\n",
      "wiki file example: ../../data/wikipedia/full_text-2021-03-20/BE/wiki_00\n",
      "save sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.dat\n",
      "save selected sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.tsv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collect wikipedia page titles as entities and generate keyword list"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect wikipedia entities\n",
    "p = MyMultiProcessing(10)\n",
    "output = p.run(collect_wiki_entity, wiki_files)\n",
    "entity_list = []\n",
    "for l in output:\n",
    "    entity_list += l\n",
    "my_write(wikipedia_entity_file, entity_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get normalized wikipedia entities\n",
    "normalized_entity = []\n",
    "for kw in open(wikipedia_entity_file).readlines():\n",
    "    eid, ent = kw.split('\\t')\n",
    "    normalized_entity.append('%s\\t%s' % (eid, normalize_text(ent)))\n",
    "my_write(wikipedia_entity_norm_file, normalized_entity)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate keyword list file\n",
    "keywords = [remove_brackets(line.strip().split('\\t')[1]) for line in open(wikipedia_entity_norm_file)]\n",
    "keywords = [kw for kw in keywords if kw.split()]\n",
    "keywords = filter_specific_keywords(keywords)\n",
    "my_write(wikipedia_keyword_file, keywords)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build wordtree\n",
    "build_word_tree_v2(wikipedia_keyword_file, wikipedia_wordtree_file, wikipedia_token_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collect sentences from wikipedia and select good sentences by path"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Process the wikipedia page files to sentence only file (10 min)\n",
    "p = MyMultiProcessing(10)\n",
    "wiki_sent_pair = [(wiki_files[i], save_sent_files[i]) for i in range(len(wiki_files))]\n",
    "output = p.run(get_sentence, wiki_sent_pair)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create the sentence filter\n",
    "sf = SentenceFilter(wikipedia_wordtree_file, wikipedia_token_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create the sentence filter\n",
    "sf = SentenceFilter(wikipedia_wordtree_file, wikipedia_token_file)\n",
    "def collect_sents(save_sent_file:str, save_selected_file:str):\n",
    "    sents = my_read(save_sent_file)\n",
    "    df = sf.list_operation(sents, use_id=True, keyword_only=True)\n",
    "    df.to_csv(save_selected_file, sep='\\t', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test of selecting sentences by path ['collect_sents']\n",
    "test_list = [(save_sent_files[i], '%d.tsv' % i) for i in range(20)]\n",
    "test_output = p.run(collect_sents, test_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the keyword occurance (15 min) ['collect_kw_occur_from_selected']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build connected graph (18 min) ['build_graph']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from tools.DocProcessing import CoOccurrence\n",
    "# co = CoOccurrence(wikipedia_wordtree_file, wikipedia_token_file)\n",
    "# def collect_occur()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Demo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load keyword occur dict which has occurance record for all keywords in selected sentences\n",
    "with open(keyword_occur_file, 'rb') as f_in:\n",
    "    keyword_occur = pickle.load(f_in)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load keyword connection graph in selected sentences\n",
    "with open(keyword_connection_graph_file, 'rb') as f_in:\n",
    "    keyword_connection_graph = pickle.load(f_in)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Demo function: find all the sentences that two keywords co-occur in selected sentences\n",
    "def find_sentences(keyword_dict:dict, kw1:str, kw2:str):\n",
    "    kw1_occur = keyword_dict.get(kw1)\n",
    "    kw2_occur = keyword_dict.get(kw2)\n",
    "    sents = pd.DataFrame(columns=['head', 'head_norm', 'head_span', 'tail', 'tail_norm', 'tail_span', 'sent', 'path'])\n",
    "    if not kw1_occur or not kw2_occur:\n",
    "        return sents\n",
    "    co_occur = kw1_occur & kw2_occur\n",
    "    file_dict = {}\n",
    "    for occur in co_occur:\n",
    "        sub_file, line_idx = occur.rsplit(':', 1)\n",
    "        if sub_file not in file_dict:\n",
    "            file_dict[sub_file] = []\n",
    "        file_dict[sub_file].append(int(line_idx))\n",
    "    for f, lines in file_dict.items():\n",
    "        sentence_in_file = my_read(os.path.join(save_path, f.replace(':', '/wiki_')+'.dat'))\n",
    "        records = my_read(os.path.join(save_path, f.replace(':', '/wiki_')+'.tsv'))\n",
    "        for idx in lines:\n",
    "            record = records[idx].split('\\t')\n",
    "            sent = sentence_in_file[int(record[6])]\n",
    "            sents = sents.append({  'head':record[0],\n",
    "                                    'head_norm':record[1],\n",
    "                                    'head_span':record[2],\n",
    "                                    'tail':record[3],\n",
    "                                    'tail_norm':record[4],\n",
    "                                    'tail_span':record[5],\n",
    "                                    'sent':sent,\n",
    "                                    'path':record[7]}, ignore_index=True)\n",
    "    return sents"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = find_sentences(keyword_occur, 'python', 'programming language')\n",
    "df.to_csv('sents.tsv', sep='\\t', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'decision tree' in keyword_occur"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(keyword_occur['machine learning'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "kw1 = 'data mining'\n",
    "kw2 = 'machine learning'\n",
    "doc = nlp('Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.'.lower())\n",
    "kw1_span = find_span(doc, kw1)\n",
    "kw2_span = find_span(doc, kw2)\n",
    "find_dependency_path_from_tree(doc, kw1_span[0], kw2_span[0])\n",
    "# print(len(kw1_span))\n",
    "# print(len(kw2_span))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = keyword_connection_graph.neighbors('decision tree')\n",
    "my_write('neighbors.txt', list(data))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Online operations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def collect_sents_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    remove_list = ['See also', 'References', 'Further reading', 'Sources', 'External links']\n",
    "    dic = {sec : page.section(sec) for sec in page.sections}\n",
    "    dic['summary'] = page.summary\n",
    "    sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    while len(section_list) > 0:\n",
    "        section = section_list.pop()\n",
    "        if section in remove_list:\n",
    "            continue\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        # processed_text = clean_text(section_text)\n",
    "        processed_text = ' '.join(section_text.lower().split())\n",
    "        temp_sents = my_sentence_tokenize(processed_text, True)\n",
    "        sents += temp_sents\n",
    "    return list(sents)\n",
    "\n",
    "def collect_entity_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    return [text.lower() for text in page.links]\n",
    "\n",
    "def collect_keyword_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    soup = BeautifulSoup(page.html(), 'html.parser')\n",
    "    main_block = soup.find('div', class_='mw-parser-output')\n",
    "    keywords = set([l.text.lower() for l in main_block.findAll('a') if re.match(r'^(<a href=\"/wiki/)', str(l))])\n",
    "    return keywords\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "keyword = 'python'\n",
    "\n",
    "p = wikipedia.page(keyword)\n",
    "if p is not None:\n",
    "    sents = collect_sents_from_wiki_page(p)\n",
    "    keywords = collect_keyword_from_wiki_page(p)\n",
    "    print('sentences collected')\n",
    "    my_write('%s.txt' % keyword, sents)\n",
    "    my_write('%s_kw.txt' % keyword, keywords)\n",
    "    df = filter_by_path(sents)\n",
    "    df.to_csv('%s_out.tsv' % keyword, sep='\\t', index=False)\n",
    "\n",
    "    dff = df[df.apply(lambda x: str(x['head']) in keywords and str(x['tail']) in keywords, axis=1)]\n",
    "    dff.to_csv('%s_out_f.tsv' % keyword, sep='\\t', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['wanted'] = df.apply(lambda x: str(x['head']) in keywords, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dff.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(dff)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('FWD_pip': conda)"
  },
  "interpreter": {
   "hash": "5479462052bd6ecbded73107b69bede395ffae09f7ec13a4a64bf5809dc60a3f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}