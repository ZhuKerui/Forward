{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train Score Function Using 1st Sentences in Wikipedia Page"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BatchEncoding, AdamW\n",
    "import torch\n",
    "from typing import Iterable, List\n",
    "import tqdm\n",
    "from torch.nn import Softmax\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.BasicUtils import my_read, my_json_read, my_csv_read, my_write, ntopidx\n",
    "from tools.TextProcessing import clean_text, sent_lemmatize, find_dependency_path_from_tree, nlp\n",
    "\n",
    "file_description = [\n",
    "    \"wiki_cs_ent.txt ---- CS wikipedia entities from data/kw_ent_map.json which have records in 1st-sents-new.json\",\n",
    "    \"*.pt ---- Training result\",\n",
    "    \"train.csv ---- The training dataset\",\n",
    "    \"valid.csv ---- The validation dataset\",\n",
    "    \"wrong_prediction.tsv ---- The sentences that are wrongly predicted by the model\"\n",
    "]\n",
    "\n",
    "if not os.path.exists('../data/temp/1st_sent'):\n",
    "    os.mkdir('../data/temp/1st_sent')\n",
    "    my_write('../data/temp/1st_sent/readme.txt', file_description)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Update readme.txt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "my_write('../data/temp/1st_sent/readme.txt', file_description)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load two main data files: 1st-sents-new.json and kw_ent_map.json"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Load 1st-sents-new.json with all words lower-cased\n",
    "first_sents_dict = json.loads(open('../data/raw_data/1st-sents-new.json', 'r').read().lower())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Load kw_ent_map.json\n",
    "kw_ent_map = my_json_read('../data/corpus/kw_ent_map.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collect CS Wikipedia entities that have records in 1st-sents-new.json"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Get cs terms that have wikipedia page\n",
    "wiki_cs_ent_in_1st = [item.lower() for item in set(kw_ent_map.values()) if item.lower() in first_sents_dict]\n",
    "my_write('../data/temp/1st_sent/wiki_cs_ent.txt', wiki_cs_ent_in_1st)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collect dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "wiki_cs_ent_in_1st = my_read('../data/temp/1st_sent/wiki_cs_ent.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "wiki_pages = my_json_read('../data/corpus/wiki_pages.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Collect negative sentences\n",
    "remove_list = ['See also', 'References', 'Further reading', 'summary', 'title']\n",
    "\n",
    "def collect_neg_sents_from_term(dic:dict, n:int=5):\n",
    "    term = clean_text(dic['title'])\n",
    "    neg_sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    while len(neg_sents) < n and len(section_list) != 0:\n",
    "        section = section_list.pop()\n",
    "        if section in remove_list:\n",
    "            continue\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        processed_text = clean_text(section_text)\n",
    "        if term not in processed_text:\n",
    "            continue\n",
    "        temp_sents = sent_tokenize(processed_text)\n",
    "        for sent in temp_sents:\n",
    "            if term in sent:\n",
    "                neg_sents.append('%s\\t%s' % (term, sent))\n",
    "    return neg_sents if neg_sents else None\n",
    "\n",
    "neg_sents = []\n",
    "for dic in wiki_pages:\n",
    "    temp = collect_neg_sents_from_term(dic)\n",
    "    if temp:\n",
    "        neg_sents += temp\n",
    "\n",
    "my_write('../data/test/neg_sents.txt', neg_sents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Collect positive sentences\n",
    "my_write('../data/test/pos_sents.txt', ['%s\\t%s' % (clean_text(term), clean_text(first_sents_dict[term]['sentence'])) for term in wiki_cs_ent_in_1st])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class Occurrence:\n",
    "    def __init__(self, wordtree_file:str, keyword_file:str):\n",
    "        self.wordtree = my_json_read(wordtree_file)\n",
    "        self.keyword_list = my_read(keyword_file)\n",
    "        self.keywords_dict = {word : i for i, word in enumerate(self.keyword_list)}\n",
    "\n",
    "    def line_operation(self, reformed_sent:list):\n",
    "        i = 0\n",
    "        kw_set_for_line = set()\n",
    "        while i < len(reformed_sent):\n",
    "            if reformed_sent[i] in self.wordtree: # If the word is the start word of a keyword\n",
    "                phrase_buf = []\n",
    "                it = self.wordtree\n",
    "                j = i\n",
    "                while j < len(reformed_sent) and reformed_sent[j] in it:\n",
    "                    # Add the word to the wait list\n",
    "                    phrase_buf.append(reformed_sent[j])\n",
    "                    if \"\" in it[reformed_sent[j]]: # If the word could be the last word of a keyword, update the list\n",
    "                        # self.line_record[self.keywords_dict[' '.join(phrase_buf).replace(' - ', '-')]].add(int(line_idx) - 1)\n",
    "                        kw_set_for_line.add(' '.join(phrase_buf).replace(' - ', '-'))\n",
    "                    # Go down the tree to the next child\n",
    "                    it = it[reformed_sent[j]]\n",
    "                    j += 1\n",
    "                    i = j - 1\n",
    "            i += 1\n",
    "        return kw_set_for_line if kw_set_for_line else None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "o = Occurrence('../data/corpus/wordtree.json', '../data/corpus/keyword_f.txt')\n",
    "o.line_operation(sent_lemmatize('a hidden markov model is a markov chain for which the state is only partially observable.'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'hidden markov model', 'markov chain', 'partially observable'}"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# Generate positive samples\n",
    "r = my_csv_read('../data/test/pos_sents.txt', delimiter='\\t')\n",
    "target_list = []\n",
    "for item in r:\n",
    "    reformed_list = sent_lemmatize(item[1])\n",
    "    reformed_sent = ' '.join(reformed_list)\n",
    "    temp_kw_set = o.line_operation(reformed_list)\n",
    "    if temp_kw_set is None:\n",
    "        continue\n",
    "    if len(temp_kw_set) < 2:\n",
    "        continue\n",
    "    if item[0] not in temp_kw_set:\n",
    "        continue\n",
    "    temp_kw_set.remove(item[0])\n",
    "    for kw in temp_kw_set:\n",
    "        target_list.append((item[0], kw, reformed_sent))\n",
    "        target_list.append((kw, item[0], reformed_sent))\n",
    "with open('../data/test/pos_samples.tsv', 'w') as f_out:\n",
    "    w = csv.writer(f_out, delimiter='\\t')\n",
    "    w.writerows(target_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# Generate negative samples\n",
    "neg = my_csv_read('../data/test/neg_sents.txt', delimiter='\\t')\n",
    "pos = my_csv_read('../data/test/pos_sents.txt', delimiter='\\t')\n",
    "keyword_list = my_read('../data/corpus/keyword_f.txt')\n",
    "target_list = []\n",
    "\n",
    "for item in pos:\n",
    "    reformed_list = sent_lemmatize(item[1])\n",
    "    reformed_sent = ' '.join(reformed_list)\n",
    "    temp_kw_set = o.line_operation(reformed_list)\n",
    "    if temp_kw_set is None:\n",
    "        continue\n",
    "    random_false_kw = random.sample(keyword_list, 1)[0]\n",
    "    random_true_kw = random.sample(temp_kw_set, 1)[0]\n",
    "    target_list.append((random_false_kw, random_true_kw, reformed_sent))\n",
    "    target_list.append((random_true_kw, random_false_kw, reformed_sent))\n",
    "\n",
    "for i, item in enumerate(neg):\n",
    "    reformed_list = sent_lemmatize(item[1])\n",
    "    reformed_sent = ' '.join(reformed_list)\n",
    "    temp_kw_set = o.line_operation(reformed_list)\n",
    "    if temp_kw_set is None:\n",
    "        continue\n",
    "    if len(temp_kw_set) < 2:\n",
    "        continue\n",
    "    if item[0] not in temp_kw_set:\n",
    "        continue\n",
    "    temp_kw_set.remove(item[0])\n",
    "    for kw in temp_kw_set:\n",
    "        target_list.append((item[0], kw, reformed_sent))\n",
    "        target_list.append((kw, item[0], reformed_sent))\n",
    "    if i >= 10000:\n",
    "        break\n",
    "with open('../data/test/neg_samples.tsv', 'w') as f_out:\n",
    "    w = csv.writer(f_out, delimiter='\\t')\n",
    "    w.writerows(target_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# Generate training data\n",
    "\n",
    "# # Positive samples\n",
    "pos = pd.DataFrame(my_csv_read('../data/test/pos_samples.tsv', delimiter='\\t'), columns=['head_ent', 'tail_ent', 'sent'])\n",
    "pos['label'] = 'T'\n",
    "\n",
    "# Negative samples 1\n",
    "neg = pd.DataFrame(my_csv_read('../data/test/neg_samples.tsv', delimiter='\\t'), columns=['head_ent', 'tail_ent', 'sent'])\n",
    "neg['label'] = 'F'\n",
    "\n",
    "\n",
    "# df = pos.append(neg, ignore_index=True).sample(frac=1.0).reset_index(drop=True)\n",
    "df = pd.concat([pos, neg], axis=0, ignore_index=True).sample(frac=1.0).reset_index(drop=True)\n",
    "df['pair'] = df.apply(lambda x: '<HEAD_ENT> %s <TAIL_ENT> %s' % (x.head_ent, x.tail_ent), axis=1)\n",
    "\n",
    "split_line = int(len(df) * 0.8)\n",
    "train_df = df[:split_line].reset_index(drop=True)\n",
    "valid_df = df[split_line:].reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv('../data/temp/1st_sent/train.csv', index=False)\n",
    "valid_df.to_csv('../data/temp/1st_sent/valid.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "train_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                      head_ent                  tail_ent  \\\n",
       "0                 flash memory       write amplification   \n",
       "1      maximum independent set                 map graph   \n",
       "2  inductive logic programming     structured prediction   \n",
       "3                    syn flood  denial-of-service attack   \n",
       "4                 file sharing                visual art   \n",
       "\n",
       "                                                sent label  \\\n",
       "0  write amplification is an undesirable phenomen...     T   \n",
       "1  however , the high exponent of the algorithm t...     F   \n",
       "2  other algorithm and model for structured predi...     F   \n",
       "3  a syn flood is a form of denial - of - service...     T   \n",
       "4  the visual art are art form such as painting ,...     F   \n",
       "\n",
       "                                                pair  \n",
       "0  <HEAD_ENT> flash memory <TAIL_ENT> write ampli...  \n",
       "1  <HEAD_ENT> maximum independent set <TAIL_ENT> ...  \n",
       "2  <HEAD_ENT> inductive logic programming <TAIL_E...  \n",
       "3  <HEAD_ENT> syn flood <TAIL_ENT> denial-of-serv...  \n",
       "4      <HEAD_ENT> file sharing <TAIL_ENT> visual art  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_ent</th>\n",
       "      <th>tail_ent</th>\n",
       "      <th>sent</th>\n",
       "      <th>label</th>\n",
       "      <th>pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flash memory</td>\n",
       "      <td>write amplification</td>\n",
       "      <td>write amplification is an undesirable phenomen...</td>\n",
       "      <td>T</td>\n",
       "      <td>&lt;HEAD_ENT&gt; flash memory &lt;TAIL_ENT&gt; write ampli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maximum independent set</td>\n",
       "      <td>map graph</td>\n",
       "      <td>however , the high exponent of the algorithm t...</td>\n",
       "      <td>F</td>\n",
       "      <td>&lt;HEAD_ENT&gt; maximum independent set &lt;TAIL_ENT&gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>inductive logic programming</td>\n",
       "      <td>structured prediction</td>\n",
       "      <td>other algorithm and model for structured predi...</td>\n",
       "      <td>F</td>\n",
       "      <td>&lt;HEAD_ENT&gt; inductive logic programming &lt;TAIL_E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>syn flood</td>\n",
       "      <td>denial-of-service attack</td>\n",
       "      <td>a syn flood is a form of denial - of - service...</td>\n",
       "      <td>T</td>\n",
       "      <td>&lt;HEAD_ENT&gt; syn flood &lt;TAIL_ENT&gt; denial-of-serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file sharing</td>\n",
       "      <td>visual art</td>\n",
       "      <td>the visual art are art form such as painting ,...</td>\n",
       "      <td>F</td>\n",
       "      <td>&lt;HEAD_ENT&gt; file sharing &lt;TAIL_ENT&gt; visual art</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "len(df)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "30780"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Load training and validation data\n",
    "train_df = pd.read_csv('../data/temp/1st_sent/train.csv')\n",
    "valid_df = pd.read_csv('../data/temp/1st_sent/valid.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens' : ['<HEAD_ENT>', '<TAIL_ENT>', '<DEP_PATH>']})"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# Load model for training\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# model = BertForSequenceClassification.from_pretrained('temp2.pt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Embedding(30525, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Function for batch generation\n",
    "def batch(sents:Iterable, n:int):\n",
    "    l = len(sents)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield sents[ndx:min(ndx + n, l)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "batch_list = [item for item in batch(train_df, 32)]\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = 0\n",
    "    batch_num = 0\n",
    "    for batch_df in tqdm.tqdm(batch_list):\n",
    "        optim.zero_grad()\n",
    "        labels = torch.tensor([1 if i == 'T' else 0 for i in batch_df.label.to_list()]).unsqueeze(1).to(device)\n",
    "        inputs = BatchEncoding(tokenizer(batch_df.sent.to_list(), batch_df.pair.to_list(), padding=True, truncation=True, max_length=80, return_tensors=\"pt\")).to(device)\n",
    "        output = model(**inputs, labels=labels)\n",
    "        loss += output.loss\n",
    "        output.loss.backward()\n",
    "        optim.step()\n",
    "    print(loss / len(batch_list))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save trained model\n",
    "tokenizer.save_pretrained('../data/temp/1st_sent/test1.pt')\n",
    "model.save_pretrained('../data/temp/1st_sent/test1.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Reload trained model\n",
    "reload_model = BertForSequenceClassification.from_pretrained('../data/temp/1st_sent/test1.pt')\n",
    "tokenizer = BertTokenizer.from_pretrained('../data/temp/1st_sent/test1.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Validation check\n",
    "reload_model.to('cpu')\n",
    "reload_model.eval()\n",
    "eval_loss = 0\n",
    "eval_batch_num = 0\n",
    "eval_batch_list = [item for item in batch(valid_df, 16)]\n",
    "with torch.no_grad():\n",
    "    for batch_df in tqdm.tqdm(eval_batch_list):\n",
    "        labels = torch.tensor([1 if i == 'T' else 0 for i in batch_df.label.to_list()]).unsqueeze(1)\n",
    "        inputs = BatchEncoding(tokenizer(batch_df.sent.to_list(), batch_df.pair.to_list(), padding=True, truncation=True, max_length=80, return_tensors='pt'))\n",
    "        output = reload_model(**inputs, labels=labels)\n",
    "        eval_loss += output.loss\n",
    "    print(eval_loss / len(eval_batch_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 385/385 [03:26<00:00,  1.87it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0309)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Function that help generate score\n",
    "def get_score(sents:List[str], pairs:List[str]):\n",
    "    with torch.no_grad():\n",
    "        inputs = BatchEncoding(tokenizer(sents, pairs, padding=True, truncation=True, max_length=80, return_tensors='pt'))\n",
    "        output = reload_model(**inputs)\n",
    "        s = Softmax(1)\n",
    "        return s(output.logits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Get logits score\n",
    "val_output = get_score(valid_df.sent.to_list(), valid_df.pair.to_list())\n",
    "# Get prediction label\n",
    "cls_result = np.argmax(val_output.numpy(), axis=1)\n",
    "# Get prediction score\n",
    "cls_score = val_output.numpy()[:, 1]\n",
    "# Get ground truth\n",
    "val_label = np.array([1 if l == 'T' else 0 for l in valid_df.label.to_list()])\n",
    "# Get correct ones\n",
    "correct_prediction = val_label == cls_result\n",
    "# Sum the number of correct ones\n",
    "correct_num = np.sum(correct_prediction)\n",
    "# Get the wrong prediction idx\n",
    "wrong_prediction_idx = np.arange(0, len(val_label))[val_label != cls_result]\n",
    "# Get the wrong ones\n",
    "wrong_samples = [(cls_result[idx], valid_df.label[idx], valid_df.pair[idx], valid_df.sent[idx]) for idx in wrong_prediction_idx]\n",
    "# Write the wrong ones to file\n",
    "with open('../data/temp/1st_sent/wrong_prediction.tsv', 'w') as f_out:\n",
    "    w = csv.writer(f_out, delimiter='\\t')\n",
    "    w.writerows(wrong_samples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test_sents = my_read('all_occurance.txt')\n",
    "# test_pairs = ['<HEAD_ENT> %s <TAIL_ENT> %s' % ('python', 'programming language')] * len(test_sents)\n",
    "\n",
    "# test_result = get_score(test_sents, test_pairs)\n",
    "# test_cls_score = test_result.numpy()[:, 1]\n",
    "# test_idx = ntopidx(len(test_cls_score), test_cls_score)\n",
    "# test_sentences = [('%.8f' % test_cls_score[i], test_sents[i]) for i in test_idx]\n",
    "# with open('test.tsv', 'w') as f_out:\n",
    "#     w = csv.writer(f_out, delimiter='\\t')\n",
    "#     w.writerows(test_sentences)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24deccfc9cfcdf51c4ee2bca89f156543ccae2e435d3cceed46c01114d4ab00d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('forward': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}