{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train Score Function Using 1st Sentences in Wikipedia Page"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BatchEncoding, AdamW\n",
    "import torch\n",
    "from typing import Iterable, List\n",
    "import tqdm\n",
    "from torch.nn import Softmax\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.BasicUtils import my_read, my_json_read, my_csv_read, MultiThreading, my_write, get_wiki_page_from_kw, clean_sent\n",
    "from py_1st_sent import collect_neg_sents_from_term"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate json file with all strings lowercased\n",
    "!cat ../data/raw_data/1st-sents-new.json | tr '[:upper:]' '[:lower:]' > ../data/corpus/1st-sents-lowercase.json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load json file\n",
    "first_sents_dict = my_json_read('../data/corpus/1st-sents-lowercase.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "terms_cs_cfl = my_csv_read('../data/raw_data/terms-cs-cfl-epoch200.txt', delimiter='\\t')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get cs terms that have wikipedia page\n",
    "wiki_cs_terms = []\n",
    "for item in terms_cs_cfl:\n",
    "    kw = item[0]\n",
    "    if kw in first_sents_dict:\n",
    "        wiki_cs_terms.append(kw)\n",
    "        if len(wiki_cs_terms) >= 5000:\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_write('wiki_cs_terms.txt', wiki_cs_terms)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wiki_cs_terms = my_read('wiki_cs_terms.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect negative sentences\n",
    "mt = MultiThreading()\n",
    "my_write('neg_sents.txt', mt.run(collect_neg_sents_from_term, wiki_cs_terms[:3000], 10).split('\\n'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect positive sentences\n",
    "my_write('pos_sents.txt', ['%s\\t%s' % (term, clean_sent(first_sents_dict[term]['sentence'])) for term in wiki_cs_terms[:3000]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Generate training data\n",
    "\n",
    "# # Positive samples\n",
    "pos = pd.DataFrame(my_csv_read('pos_sents.txt', delimiter='\\t'), columns=['head', 'sent'])\n",
    "pos['label'] = 'T'\n",
    "\n",
    "# Negative samples\n",
    "neg = pd.DataFrame(my_csv_read('neg_sents.txt', delimiter='\\t'), columns=['head', 'sent'])\n",
    "neg['label'] = 'F'\n",
    "\n",
    "df = pos.append(neg, ignore_index=True).sample(frac=1.0).reset_index(drop=True)\n",
    "\n",
    "split_line = int(len(df) * 0.8)\n",
    "train_df = df[:split_line].reset_index(drop=True)\n",
    "valid_df = df[split_line:].reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "valid_df.to_csv('valid.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     head                                               sent  \\\n",
       "0       anomaly detection  anomaly detection benchmark data repository of...   \n",
       "1  cholesky decomposition  c programming language the gnu scientific libr...   \n",
       "2               smoothing  in statistics and image processing , to smooth...   \n",
       "3       visual perception  another type of the unconscious inference hypo...   \n",
       "4       texture synthesis  since then, the field of texture synthesis has...   \n",
       "\n",
       "  label  \n",
       "0     F  \n",
       "1     F  \n",
       "2     T  \n",
       "3     F  \n",
       "4     F  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>sent</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anomaly detection</td>\n",
       "      <td>anomaly detection benchmark data repository of...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cholesky decomposition</td>\n",
       "      <td>c programming language the gnu scientific libr...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>smoothing</td>\n",
       "      <td>in statistics and image processing , to smooth...</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>visual perception</td>\n",
       "      <td>another type of the unconscious inference hypo...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>texture synthesis</td>\n",
       "      <td>since then, the field of texture synthesis has...</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load training and validation data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "valid_df = pd.read_csv('valid.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load model for training\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('temp2.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function for batch generation\n",
    "def batch(sents:Iterable, n:int):\n",
    "    l = len(sents)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield sents[ndx:min(ndx + n, l)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "batch_list = [item for item in batch(train_df, 32)]\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = 0\n",
    "    batch_num = 0\n",
    "    for batch_df in tqdm.tqdm(batch_list):\n",
    "        optim.zero_grad()\n",
    "        labels = torch.tensor([1 if i == 'T' else 0 for i in batch_df.label.to_list()]).unsqueeze(1).to(device)\n",
    "        inputs = BatchEncoding(tokenizer(batch_df.sent.to_list(), padding=True, truncation=True, max_length=80, return_tensors='pt')).to(device)\n",
    "        output = model(**inputs, labels=labels)\n",
    "        loss += output.loss\n",
    "        output.loss.backward()\n",
    "        optim.step()\n",
    "    print(loss / len(batch_list))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save trained model\n",
    "model.save_pretrained('temp2.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reload trained model\n",
    "reload_model = BertForSequenceClassification.from_pretrained('temp2.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Validation check\n",
    "reload_model.to('cpu')\n",
    "reload_model.eval()\n",
    "eval_loss = 0\n",
    "eval_batch_num = 0\n",
    "eval_batch_list = [item for item in batch(valid_df, 16)]\n",
    "with torch.no_grad():\n",
    "    for batch_df in tqdm.tqdm(eval_batch_list):\n",
    "        labels = torch.tensor([1 if i == 'T' else 0 for i in batch_df.label.to_list()]).unsqueeze(1)\n",
    "        inputs = BatchEncoding(tokenizer(batch_df.sent.to_list(), padding=True, truncation=True, max_length=80, return_tensors='pt'))\n",
    "        output = reload_model(**inputs, labels=labels)\n",
    "        eval_loss += output.loss\n",
    "    print(eval_loss / len(eval_batch_list))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "temp.pt: 0.0526\n",
    "\n",
    "temp2.pt: 0.0511"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function that help generate score\n",
    "def get_score(sents:List[str]):\n",
    "    with torch.no_grad():\n",
    "        inputs = BatchEncoding(tokenizer(sents, padding=True, truncation=True, max_length=80, return_tensors='pt'))\n",
    "        output = reload_model(**inputs)\n",
    "        s = Softmax(1)\n",
    "        return s(output.logits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_output = get_score(valid_df.sent.to_list())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cls_result = np.argmax(val_output.numpy(), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cls_result.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_label = np.array([1 if l == 'T' else 0 for l in valid_df.label.to_list()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "correct_prediction = val_label == cls_result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.sum(correct_prediction)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 10 sentences from 1st_wiki\n",
    "sents = ['andre kirk agassi is an american retired professional tennis player and former world no.',\n",
    "'the austroasiatic languages , also known as mon–khmer , are a large language family of mainland southeast asia , also scattered throughout parts of india , bangladesh , nepal , and southern china .',\n",
    "'afroasiatic , also known as afrasian or hamito - semitic or semito - hamitic, is a large language family of about 300 languages that are spoken predominantly in west asia , north africa , the horn of africa and parts of the sahel .',\n",
    "'andorra , officially the principality of andorra , is a sovereign landlocked microstate on the iberian peninsula , in the eastern pyrenees , bordered by france to the north and spain to the south.',\n",
    "'in mathematics and statistics , the arithmetic mean , or simply the mean or the average , is the sum of a collection of numbers divided by the count of numbers in the collection.',\n",
    "'the american football conference is one of the two conferences of the national football league , the highest professional level of american football in the united states.',\n",
    "'animal farm is an allegorical novella by george orwell , first published in england on 17 august 1945.',\n",
    "'amphibians are ectotherm ic, tetrapod vertebrate s of the class amphibia.',\n",
    "'alaska is a u.',\n",
    "'agriculture is the science, art and practice of cultivating plants and livestock.']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 10 sentences from small_sent.txt\n",
    "sents = ['we describe a new algorithm, the - pebble game with colors, and use it obtain a characterization of the family of - sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs.',\n",
    "'special instances of sparse graphs appear in rigidity theory and have received increased attention in recent years.',\n",
    "'in particular, our colored pebbles generalize and strengthen the previous results of lee and streinu and give a new proof of the tutte - nash - williams characterization of arboricity.',\n",
    "'we also present a new decomposition that certifies sparsity based on the - pebble game with colors.',\n",
    "'our work also exposes connections between pebble game algorithms and previous sparse graph algorithms by gabow, gabow and westermann and hendrickson.',\n",
    "'in a quantum mechanical model, diosi, feldmann and kosloff arrived at a conjecture stating that the limit of the entropy of certain mixtures is the relative entropy as system size goes to infinity.',\n",
    "'the conjecture is proven in this paper for density matrices.',\n",
    "'the first proof is analytic and uses the quantum law of large numbers.',\n",
    "'the second one clarifies the relation to channel capacity per unit cost for classical - quantum channels.',\n",
    "'both proofs lead to generalization of the conjecture.']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "get_score(sents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect 1st_sentence like sentences\n",
    "all_sents = open('../data/corpus/small_sent.txt', 'r').read().strip().split('\\n')\n",
    "random.shuffle(all_sents)\n",
    "sents = all_sents[:2000]\n",
    "output = get_score(sents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score = output[:, 1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sum(score > 0.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score = score.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx = np.arange(len(score))[score > 0.5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "good_sents = [sents[i] for i in idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "good_sents"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score[score > 0.5]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "947ccf1d8baae4b0b3c7136017192ad9c9ad48a2268b8759d45f6c7f995c7f83"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('imojie_env': virtualenvwrapper)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}