{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train Score Function Using 1st Sentences in Wikipedia Page"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BatchEncoding, AdamW\n",
    "import torch\n",
    "from typing import Iterable, List\n",
    "import tqdm\n",
    "from torch.nn import Softmax\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.BasicUtils import my_read, my_json_read, my_csv_read, my_write, ntopidx\n",
    "from tools.TextProcessing import clean_text, sent_lemmatize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load json file\n",
    "first_sents_dict = my_json_read('../data/corpus/1st-sents-lowercase.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "terms_cs_cfl = my_csv_read('../data/raw_data/terms-cs-cfl-epoch200.txt', delimiter='\\t')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Get cs terms that have wikipedia page\n",
    "wiki_cs_terms = []\n",
    "for item in terms_cs_cfl:\n",
    "    kw = item[0]\n",
    "    if kw in first_sents_dict:\n",
    "        wiki_cs_terms.append(kw)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "my_write('wiki_cs_terms.txt', wiki_cs_terms)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "wiki_cs_terms = my_read('wiki_cs_terms.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "wiki_pages = my_json_read('../data/temp/wiki_pages.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Collect negative sentences\n",
    "remove_list = ['See also', 'References', 'Further reading', 'summary']\n",
    "\n",
    "def collect_neg_sents_from_term(dic:dict, n:int=5):\n",
    "    term = clean_text(dic['title'])\n",
    "    neg_sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    for item in remove_list:\n",
    "        if item in section_list:\n",
    "            section_list.remove(item)\n",
    "    while len(neg_sents) < n and len(section_list) != 0:\n",
    "        section = section_list.pop()\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        processed_text = clean_text(section_text)\n",
    "        if term not in processed_text:\n",
    "            continue\n",
    "        temp_sents = sent_tokenize(processed_text)\n",
    "        for sent in temp_sents:\n",
    "            if term in sent:\n",
    "                neg_sents.append('%s\\t%s' % (term, sent))\n",
    "    return neg_sents if neg_sents else None\n",
    "\n",
    "neg_sents = []\n",
    "for dic in wiki_pages:\n",
    "    temp = collect_neg_sents_from_term(dic)\n",
    "    if temp:\n",
    "        neg_sents += temp\n",
    "\n",
    "my_write('../data/test/neg_sents.txt', neg_sents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Collect positive sentences\n",
    "my_write('../data/test/pos_sents.txt', ['%s\\t%s' % (term, clean_text(first_sents_dict[term]['sentence'])) for term in wiki_cs_terms])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Occurrence:\n",
    "    def __init__(self, wordtree_file:str, keyword_file:str):\n",
    "        self.wordtree = my_json_read(wordtree_file)\n",
    "        self.keyword_list = my_read(keyword_file)\n",
    "        self.keywords_dict = {word : i for i, word in enumerate(self.keyword_list)}\n",
    "\n",
    "    def line_operation(self, reformed_sent:list):\n",
    "        i = 0\n",
    "        kw_set_for_line = set()\n",
    "        while i < len(reformed_sent):\n",
    "            if reformed_sent[i] in self.wordtree: # If the word is the start word of a keyword\n",
    "                phrase_buf = []\n",
    "                it = self.wordtree\n",
    "                j = i\n",
    "                while j < len(reformed_sent) and reformed_sent[j] in it:\n",
    "                    # Add the word to the wait list\n",
    "                    phrase_buf.append(reformed_sent[j])\n",
    "                    if \"\" in it[reformed_sent[j]]: # If the word could be the last word of a keyword, update the list\n",
    "                        # self.line_record[self.keywords_dict[' '.join(phrase_buf).replace(' - ', '-')]].add(int(line_idx) - 1)\n",
    "                        kw_set_for_line.add(' '.join(phrase_buf).replace(' - ', '-'))\n",
    "                    # Go down the tree to the next child\n",
    "                    it = it[reformed_sent[j]]\n",
    "                    j += 1\n",
    "                    i = j - 1\n",
    "            i += 1\n",
    "        return kw_set_for_line if kw_set_for_line else None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Generate samples\n",
    "o = Occurrence('../data/corpus/wordtree.json', '../data/corpus/keyword_f.txt')\n",
    "r = my_csv_read('../data/test/neg_sents.txt', delimiter='\\t')\n",
    "target_list = []\n",
    "target_file = '../data/test/neg_samples.tsv'\n",
    "for item in r:\n",
    "    reformed_list = sent_lemmatize(item[1])\n",
    "    reformed_sent = ' '.join(reformed_list)\n",
    "    temp_kw_set = o.line_operation(reformed_list)\n",
    "    if temp_kw_set is None:\n",
    "        continue\n",
    "    temp_kw_list = list(temp_kw_set)\n",
    "    length = len(temp_kw_list)\n",
    "    if length > 1:\n",
    "        for i in range(length):\n",
    "            for j in range(length):\n",
    "                if i != j:\n",
    "                    target_list.append((temp_kw_list[i], temp_kw_list[j], reformed_sent))\n",
    "with open(target_file, 'w') as f_out:\n",
    "    w = csv.writer(f_out, delimiter='\\t')\n",
    "    w.writerows(target_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "o = Occurrence('../data/corpus/wordtree.json', '../data/corpus/keyword_f.txt')\n",
    "o.line_operation(sent_lemmatize('a hidden markov model is a markov chain for which the state is only partially observable.'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'hidden markov model', 'markov chain', 'partially observable'}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Generate training data\n",
    "\n",
    "# # Positive samples\n",
    "pos = pd.DataFrame(my_csv_read('../data/test/pos_samples.tsv', delimiter='\\t'), columns=['head_ent', 'tail_ent', 'sent'])\n",
    "pos['label'] = 'T'\n",
    "\n",
    "# Negative samples 1\n",
    "neg_1 = pd.DataFrame(my_csv_read('../data/test/neg_samples.tsv', delimiter='\\t'), columns=['head_ent', 'tail_ent', 'sent'])\n",
    "neg_1['label'] = 'F'\n",
    "\n",
    "# Negative samples 2\n",
    "neg_2 = pd.concat([pos.sent.to_frame(), \n",
    "                    pos.head_ent.sample(frac=1).reset_index(drop=True).to_frame(), \n",
    "                    pos.tail_ent.to_frame()], axis=1)\n",
    "neg_2['label'] = 'F'\n",
    "\n",
    "# Negative samples 3\n",
    "neg_3 = pd.concat([pos.sent.to_frame(), \n",
    "                    pos.head_ent.to_frame(), \n",
    "                    pos.tail_ent.sample(frac=1).reset_index(drop=True).to_frame()], axis=1)\n",
    "neg_3['label'] = 'F'\n",
    "\n",
    "# Negative samples 4\n",
    "neg_4 = pd.concat([pos.sent.to_frame(), \n",
    "                    pos.head_ent.sample(frac=1).reset_index(drop=True).to_frame(), \n",
    "                    pos.tail_ent.sample(frac=1).reset_index(drop=True).to_frame()], axis=1)\n",
    "neg_4['label'] = 'F'\n",
    "\n",
    "# df = pos.append(neg, ignore_index=True).sample(frac=1.0).reset_index(drop=True)\n",
    "df = pd.concat([pos, neg_1, neg_2, neg_3, neg_4], axis=0, ignore_index=True).sample(frac=1.0).reset_index(drop=True)\n",
    "df['pair'] = df.apply(lambda x: '<HEAD_ENT> %s <TAIL_ENT> %s' % (x.head_ent, x.tail_ent), axis=1)\n",
    "\n",
    "split_line = int(len(df) * 0.8)\n",
    "train_df = df[:split_line].reset_index(drop=True)\n",
    "valid_df = df[split_line:].reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "valid_df.to_csv('valid.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "train_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            head_ent               tail_ent  \\\n",
       "0      turing degree  theory of computation   \n",
       "1  engineered system             root cause   \n",
       "2     system biology    pseudorandom number   \n",
       "3        standard ml           kernel space   \n",
       "4   internet service        optimal control   \n",
       "\n",
       "                                                sent label  \\\n",
       "0  computability theory , also known as recursion...     T   \n",
       "1  in science and engineering , root cause analys...     F   \n",
       "2  a confusion network is a natural language proc...     F   \n",
       "3  a finite - state transducer is a finite - stat...     F   \n",
       "4  an internet exchange point is the physical inf...     F   \n",
       "\n",
       "                                                pair  \n",
       "0  <HEAD_ENT> turing degree <TAIL_ENT> theory of ...  \n",
       "1  <HEAD_ENT> engineered system <TAIL_ENT> root c...  \n",
       "2  <HEAD_ENT> system biology <TAIL_ENT> pseudoran...  \n",
       "3     <HEAD_ENT> standard ml <TAIL_ENT> kernel space  \n",
       "4  <HEAD_ENT> internet service <TAIL_ENT> optimal...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head_ent</th>\n",
       "      <th>tail_ent</th>\n",
       "      <th>sent</th>\n",
       "      <th>label</th>\n",
       "      <th>pair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>turing degree</td>\n",
       "      <td>theory of computation</td>\n",
       "      <td>computability theory , also known as recursion...</td>\n",
       "      <td>T</td>\n",
       "      <td>&lt;HEAD_ENT&gt; turing degree &lt;TAIL_ENT&gt; theory of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>engineered system</td>\n",
       "      <td>root cause</td>\n",
       "      <td>in science and engineering , root cause analys...</td>\n",
       "      <td>F</td>\n",
       "      <td>&lt;HEAD_ENT&gt; engineered system &lt;TAIL_ENT&gt; root c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>system biology</td>\n",
       "      <td>pseudorandom number</td>\n",
       "      <td>a confusion network is a natural language proc...</td>\n",
       "      <td>F</td>\n",
       "      <td>&lt;HEAD_ENT&gt; system biology &lt;TAIL_ENT&gt; pseudoran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>standard ml</td>\n",
       "      <td>kernel space</td>\n",
       "      <td>a finite - state transducer is a finite - stat...</td>\n",
       "      <td>F</td>\n",
       "      <td>&lt;HEAD_ENT&gt; standard ml &lt;TAIL_ENT&gt; kernel space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>internet service</td>\n",
       "      <td>optimal control</td>\n",
       "      <td>an internet exchange point is the physical inf...</td>\n",
       "      <td>F</td>\n",
       "      <td>&lt;HEAD_ENT&gt; internet service &lt;TAIL_ENT&gt; optimal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "len(df)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "162206"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load training and validation data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "valid_df = pd.read_csv('valid.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens' : ['<HEAD_ENT>', '<TAIL_ENT>', '<DEP_PATH>']})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/keruiz2/Envs/forward/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9000). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load model for training\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# model = BertForSequenceClassification.from_pretrained('temp2.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function for batch generation\n",
    "def batch(sents:Iterable, n:int):\n",
    "    l = len(sents)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield sents[ndx:min(ndx + n, l)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train the model\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "batch_list = [item for item in batch(train_df, 32)]\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = 0\n",
    "    batch_num = 0\n",
    "    for batch_df in tqdm.tqdm(batch_list):\n",
    "        optim.zero_grad()\n",
    "        labels = torch.tensor([1 if i == 'T' else 0 for i in batch_df.label.to_list()]).unsqueeze(1).to(device)\n",
    "        inputs = BatchEncoding(tokenizer(batch_df.sent.to_list(), batch_df.pair.to_list(), padding=True, truncation=True, max_length=80, return_tensors=\"pt\")).to(device)\n",
    "        output = model(**inputs, labels=labels)\n",
    "        loss += output.loss\n",
    "        output.loss.backward()\n",
    "        optim.step()\n",
    "    print(loss / len(batch_list))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Save trained model\n",
    "tokenizer.save_pretrained('temp3.pt')\n",
    "model.save_pretrained('temp3.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reload trained model\n",
    "reload_model = BertForSequenceClassification.from_pretrained('temp3.pt')\n",
    "tokenizer = BertTokenizer.from_pretrained('temp3.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Validation check\n",
    "reload_model.to('cpu')\n",
    "reload_model.eval()\n",
    "eval_loss = 0\n",
    "eval_batch_num = 0\n",
    "eval_batch_list = [item for item in batch(valid_df, 16)]\n",
    "with torch.no_grad():\n",
    "    for batch_df in tqdm.tqdm(eval_batch_list):\n",
    "        labels = torch.tensor([1 if i == 'T' else 0 for i in batch_df.label.to_list()]).unsqueeze(1)\n",
    "        inputs = BatchEncoding(tokenizer(batch_df.sent.to_list(), batch_df.pair.to_list(), padding=True, truncation=True, max_length=80, return_tensors='pt'))\n",
    "        output = reload_model(**inputs, labels=labels)\n",
    "        eval_loss += output.loss\n",
    "    print(eval_loss / len(eval_batch_list))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function that help generate score\n",
    "def get_score(sents:List[str], pairs:List[str]):\n",
    "    with torch.no_grad():\n",
    "        inputs = BatchEncoding(tokenizer(sents, pairs, padding=True, truncation=True, max_length=80, return_tensors='pt'))\n",
    "        output = reload_model(**inputs)\n",
    "        s = Softmax(1)\n",
    "        return s(output.logits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get logits score\n",
    "val_output = get_score(valid_df.sent.to_list(), valid_df.pair.to_list())\n",
    "# Get prediction label\n",
    "cls_result = np.argmax(val_output.numpy(), axis=1)\n",
    "# Get prediction score\n",
    "cls_score = val_output.numpy()[:, 1]\n",
    "# Get ground truth\n",
    "val_label = np.array([1 if l == 'T' else 0 for l in valid_df.label.to_list()])\n",
    "# Get correct ones\n",
    "correct_prediction = val_label == cls_result\n",
    "# Sum the number of correct ones\n",
    "correct_num = np.sum(correct_prediction)\n",
    "# Get the wrong prediction idx\n",
    "wrong_prediction_idx = np.arange(0, len(val_label))[val_label != cls_result]\n",
    "# Get the wrong ones\n",
    "wrong_samples = [(valid_df.sent[idx], valid_df.pair[idx], valid_df.label[idx], cls_result[idx]) for idx in wrong_prediction_idx]\n",
    "# Write the wrong ones to file\n",
    "with open('wrong_prediction.tsv', 'w') as f_out:\n",
    "    w = csv.writer(f_out, delimiter='\\t')\n",
    "    w.writerows(wrong_samples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_sents = my_read('all_occurance.txt')\n",
    "test_pairs = ['<HEAD_ENT> %s <TAIL_ENT> %s' % ('python', 'programming language')] * len(test_sents)\n",
    "\n",
    "test_result = get_score(test_sents, test_pairs)\n",
    "test_cls_score = test_result.numpy()[:, 1]\n",
    "test_idx = ntopidx(len(test_cls_score), test_cls_score)\n",
    "test_sentences = [('%.8f' % test_cls_score[i], test_sents[i]) for i in test_idx]\n",
    "with open('test.tsv', 'w') as f_out:\n",
    "    w = csv.writer(f_out, delimiter='\\t')\n",
    "    w.writerows(test_sentences)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect 1st_sentence like sentences\n",
    "all_sents = open('../data/corpus/small_sent.txt', 'r').read().strip().split('\\n')\n",
    "random.shuffle(all_sents)\n",
    "sents = all_sents[:2000]\n",
    "output = get_score(sents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score = output[:, 1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sum(score > 0.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score = score.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx = np.arange(len(score))[score > 0.5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "good_sents = [sents[i] for i in idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "good_sents"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score[score > 0.5]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "24deccfc9cfcdf51c4ee2bca89f156543ccae2e435d3cceed46c01114d4ab00d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('forward': virtualenvwrapper)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}