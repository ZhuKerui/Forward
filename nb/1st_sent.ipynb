{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train Score Function Using 1st Sentences in Wikipedia Page"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BatchEncoding, AdamW\n",
    "import torch\n",
    "from typing import Iterable, List\n",
    "import tqdm\n",
    "from torch.nn import Softmax\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.BasicUtils import my_read, my_json_read, my_csv_read, MultiThreading, my_write, clean_sent, get_wiki_page_from_kw, ntopidx, nsmallidx\n",
    "from py_1st_sent import collect_neg_sents_from_term"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import json\n",
    "from tools.TextProcessing import sent_lemmatize\n",
    "\n",
    "class Occurrence:\n",
    "    def __init__(self, wordtree_file:str, keyword_file:str):\n",
    "        self.wordtree = json.load(open(wordtree_file, 'r'))\n",
    "        self.keyword_list = open(keyword_file, 'r').read().strip().split('\\n')\n",
    "        self.keywords_dict = {word : i for i, word in enumerate(self.keyword_list)}\n",
    "\n",
    "    def line_operation(self, line:str):\n",
    "        reformed_sent = sent_lemmatize(line)\n",
    "        i = 0\n",
    "        kw_set_for_line = set()\n",
    "        while i < len(reformed_sent):\n",
    "            if reformed_sent[i] in self.wordtree: # If the word is the start word of a keyword\n",
    "                phrase_buf = []\n",
    "                it = self.wordtree\n",
    "                j = i\n",
    "                while j < len(reformed_sent) and reformed_sent[j] in it:\n",
    "                    # Add the word to the wait list\n",
    "                    phrase_buf.append(reformed_sent[j])\n",
    "                    if \"\" in it[reformed_sent[j]]: # If the word could be the last word of a keyword, update the list\n",
    "                        # self.line_record[self.keywords_dict[' '.join(phrase_buf).replace(' - ', '-')]].add(int(line_idx) - 1)\n",
    "                        kw_set_for_line.add(' '.join(phrase_buf).replace(' - ', '-'))\n",
    "                    # Go down the tree to the next child\n",
    "                    it = it[reformed_sent[j]]\n",
    "                    j += 1\n",
    "                    i = j - 1\n",
    "            i += 1\n",
    "        return kw_set_for_line if kw_set_for_line else None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate json file with all strings lowercased\n",
    "!cat ../data/raw_data/1st-sents-new.json | tr '[:upper:]' '[:lower:]' > ../data/corpus/1st-sents-lowercase.json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load json file\n",
    "first_sents_dict = my_json_read('../data/corpus/1st-sents-lowercase.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "terms_cs_cfl = my_csv_read('../data/raw_data/terms-cs-cfl-epoch200.txt', delimiter='\\t')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get cs terms that have wikipedia page\n",
    "wiki_cs_terms = []\n",
    "for item in terms_cs_cfl:\n",
    "    kw = item[0]\n",
    "    if kw in first_sents_dict:\n",
    "        wiki_cs_terms.append(kw)\n",
    "        if len(wiki_cs_terms) >= 5000:\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "my_write('wiki_cs_terms.txt', wiki_cs_terms)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wiki_cs_terms = my_read('wiki_cs_terms.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect negative sentences\n",
    "mt = MultiThreading()\n",
    "my_write('../data/test/neg_sents.txt', mt.run(collect_neg_sents_from_term, wiki_cs_terms[:3000], 10).split('\\n'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect positive sentences\n",
    "my_write('../data/test/pos_sents.txt', ['%s\\t%s' % (term, clean_sent(first_sents_dict[term]['sentence'])) for term in wiki_cs_terms[:3000]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Generate samples\n",
    "o = Occurrence('../data/corpus/wordtree.json', '../data/corpus/keyword_f.txt')\n",
    "r = my_csv_read('../data/test/pos_sents.txt', delimiter='\\t')\n",
    "target_list = []\n",
    "target_file = '../data/test/pos_samples.tsv'\n",
    "for item in r:\n",
    "    temp_kw_set = o.line_operation(item[1])\n",
    "    if temp_kw_set is None:\n",
    "        continue\n",
    "    if item[0] in temp_kw_set:\n",
    "        temp_kw_set.remove(item[0])\n",
    "    if temp_kw_set:\n",
    "        for kw in temp_kw_set:\n",
    "            target_list.append((item[0], kw, item[1]))\n",
    "with open(target_file, 'w') as f_out:\n",
    "    w = csv.writer(f_out, delimiter='\\t')\n",
    "    w.writerows(target_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "o = Occurrence('../data/corpus/wordtree.json', '../data/corpus/keyword_f.txt')\n",
    "o.line_operation('a hidden markov model is a markov chain for which the state is only partially observable.')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'hidden markov model', 'markov chain'}"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Generate training data\n",
    "\n",
    "# # Positive samples\n",
    "pos = pd.DataFrame(my_csv_read('../data/test/pos_samples.tsv', delimiter='\\t'), columns=['head_ent', 'tail_ent', 'sent'])\n",
    "pos['label'] = 'T'\n",
    "\n",
    "# Negative samples\n",
    "neg = pd.DataFrame(my_csv_read('../data/test/neg_samples.tsv', delimiter='\\t'), columns=['head_ent', 'tail_ent', 'sent'])\n",
    "neg['label'] = 'F'\n",
    "\n",
    "df = pos.append(neg, ignore_index=True).sample(frac=1.0).reset_index(drop=True)\n",
    "df['pair'] = df.apply(lambda x: '<HEAD_ENT> %s <TAIL_ENT> %s' % (x.head_ent, x.tail_ent), axis=1)\n",
    "\n",
    "split_line = int(len(df) * 0.8)\n",
    "train_df = df[:split_line].reset_index(drop=True)\n",
    "valid_df = df[split_line:].reset_index(drop=True)\n",
    "\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "valid_df.to_csv('valid.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Load training and validation data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "valid_df = pd.read_csv('valid.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens' : ['<HEAD_ENT>', '<TAIL_ENT>', '<DEP_PATH>']})"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Load model for training\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# model = BertForSequenceClassification.from_pretrained('temp2.pt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Embedding(30525, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Function for batch generation\n",
    "def batch(sents:Iterable, n:int):\n",
    "    l = len(sents)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield sents[ndx:min(ndx + n, l)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Train the model\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "batch_list = [item for item in batch(train_df, 32)]\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = 0\n",
    "    batch_num = 0\n",
    "    for batch_df in tqdm.tqdm(batch_list):\n",
    "        optim.zero_grad()\n",
    "        labels = torch.tensor([1 if i == 'T' else 0 for i in batch_df.label.to_list()]).unsqueeze(1).to(device)\n",
    "        inputs = BatchEncoding(tokenizer(batch_df.sent.to_list(), batch_df.pair.to_list(), padding=True, truncation=True, max_length=80, return_tensors=\"pt\")).to(device)\n",
    "        output = model(**inputs, labels=labels)\n",
    "        loss += output.loss\n",
    "        output.loss.backward()\n",
    "        optim.step()\n",
    "    print(loss / len(batch_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 201/201 [00:49<00:00,  4.07it/s]\n",
      "  0%|          | 0/201 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.1808, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 201/201 [00:50<00:00,  3.98it/s]\n",
      "  0%|          | 0/201 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 201/201 [00:50<00:00,  4.01it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0359, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Save trained model\n",
    "tokenizer.save_pretrained('temp3.pt')\n",
    "model.save_pretrained('temp3.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Reload trained model\n",
    "reload_model = BertForSequenceClassification.from_pretrained('temp3.pt')\n",
    "tokenizer = BertTokenizer.from_pretrained('temp3.pt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Validation check\n",
    "reload_model.to('cpu')\n",
    "reload_model.eval()\n",
    "eval_loss = 0\n",
    "eval_batch_num = 0\n",
    "eval_batch_list = [item for item in batch(valid_df, 16)]\n",
    "with torch.no_grad():\n",
    "    for batch_df in tqdm.tqdm(eval_batch_list):\n",
    "        labels = torch.tensor([1 if i == 'T' else 0 for i in batch_df.label.to_list()]).unsqueeze(1)\n",
    "        inputs = BatchEncoding(tokenizer(batch_df.sent.to_list(), batch_df.pair.to_list(), padding=True, truncation=True, max_length=80, return_tensors='pt'))\n",
    "        output = reload_model(**inputs, labels=labels)\n",
    "        eval_loss += output.loss\n",
    "    print(eval_loss / len(eval_batch_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 101/101 [00:59<00:00,  1.70it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.1032)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Function that help generate score\n",
    "def get_score(sents:List[str], pairs:List[str]):\n",
    "    with torch.no_grad():\n",
    "        inputs = BatchEncoding(tokenizer(sents, pairs, padding=True, truncation=True, max_length=80, return_tensors='pt'))\n",
    "        output = reload_model(**inputs)\n",
    "        s = Softmax(1)\n",
    "        return s(output.logits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Get logits score\n",
    "val_output = get_score(valid_df.sent.to_list(), valid_df.pair.to_list())\n",
    "# Get prediction label\n",
    "cls_result = np.argmax(val_output.numpy(), axis=1)\n",
    "# Get prediction score\n",
    "cls_score = val_output.numpy()[:, 1]\n",
    "# Get ground truth\n",
    "val_label = np.array([1 if l == 'T' else 0 for l in valid_df.label.to_list()])\n",
    "# Get correct ones\n",
    "correct_prediction = val_label == cls_result\n",
    "# Sum the number of correct ones\n",
    "correct_num = np.sum(correct_prediction)\n",
    "# Get the wrong prediction idx\n",
    "wrong_prediction_idx = np.arange(0, len(val_label))[val_label != cls_result]\n",
    "# Get the wrong ones\n",
    "wrong_samples = [(valid_df.sent[idx], valid_df.pair[idx], valid_df.label[idx], cls_result[idx]) for idx in wrong_prediction_idx]\n",
    "# Write the wrong ones to file\n",
    "with open('wrong_prediction.tsv', 'w') as f_out:\n",
    "    w = csv.writer(f_out, delimiter='\\t')\n",
    "    w.writerows(wrong_samples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "test_sents = my_read('all_occurance.txt')\n",
    "test_pairs = ['<HEAD_ENT> %s <TAIL_ENT> %s' % ('python', 'programming language')] * len(test_sents)\n",
    "\n",
    "test_result = get_score(test_sents, test_pairs)\n",
    "test_cls_score = test_result.numpy()[:, 1]\n",
    "test_idx = ntopidx(len(test_cls_score), test_cls_score)\n",
    "test_sentences = [('%.8f' % test_cls_score[i], test_sents[i]) for i in test_idx]\n",
    "with open('test.tsv', 'w') as f_out:\n",
    "    w = csv.writer(f_out, delimiter='\\t')\n",
    "    w.writerows(test_sentences)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect 1st_sentence like sentences\n",
    "all_sents = open('../data/corpus/small_sent.txt', 'r').read().strip().split('\\n')\n",
    "random.shuffle(all_sents)\n",
    "sents = all_sents[:2000]\n",
    "output = get_score(sents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score = output[:, 1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sum(score > 0.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score = score.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "idx = np.arange(len(score))[score > 0.5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "good_sents = [sents[i] for i in idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "good_sents"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "score[score > 0.5]"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "947ccf1d8baae4b0b3c7136017192ad9c9ad48a2268b8759d45f6c7f995c7f83"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('imojie_env': virtualenvwrapper)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}