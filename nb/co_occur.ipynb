{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence notebook\n",
    "+ This notebook is used for handling keyword co-occurrence related work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.BasicUtils import my_read, my_write\n",
    "from tools.TextProcessing import sent_lemmatize, nlp\n",
    "from tools.DocProcessing import build_graph, graph_load, graph_dump, get_subgraph, co_occur_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fundamental data (50 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = my_read('../data/corpus/small_sent.txt')\n",
    "keyword_list = my_read('../data/corpus/keyword.txt')\n",
    "word2idx_dict = {word:i for i, word in enumerate(keyword_list)}\n",
    "with open('../data/corpus/occur.pickle', 'rb') as f_in:\n",
    "    occur_dict = pickle.load(f_in)\n",
    "co_occur_list = co_occur_load('../data/corpus/co_occur.txt')\n",
    "pair_graph = graph_load('../data/corpus/pair.gpickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate co-occurrence list and occurrence dictionary if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to py folder and run followings gen_co_occur.py and gen_occur.py in the backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate co-occurrence graph if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Co-occurrence lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000566/2000566 [00:36<00:00, 54841.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading Done! NPMI analysis starts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2362837/2362837 [00:07<00:00, 328481.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPMI analysis Done\n"
     ]
    }
   ],
   "source": [
    "# Generate pair graph (about 5 minutes)\n",
    "pair_graph = build_graph(co_occur_list, keyword_list)\n",
    "graph_dump(pair_graph, '../data/corpus/pair.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 82, 'npmi': 0.2327354320347479}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_graph.edges['database', 'data mining']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play around in the below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of highly related pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def mark_sent_in_html(sent:str, keyword_list:List[str], is_entity:bool=True):\n",
    "    reformed_sent = sent.split() if is_entity else sent_lemmatize(sent.replace('-', ' - '))\n",
    "    reformed_keywords = [[k] for k in keyword_list] if is_entity else [k.replace('-', ' - ').split() for k in keyword_list]\n",
    "    mask = np.zeros(len(reformed_sent), dtype=np.bool)\n",
    "    for k in reformed_keywords:\n",
    "        begin_idx = 0\n",
    "        while reformed_sent[begin_idx:].count(k[0]) > 0:\n",
    "            begin_idx = reformed_sent.index(k[0], begin_idx)\n",
    "            is_good = True\n",
    "            i = 0\n",
    "            for i in range(1, len(k)):\n",
    "                if begin_idx + i >= len(reformed_sent) or reformed_sent[begin_idx + i] != k[i]:\n",
    "                    is_good = False\n",
    "                    break\n",
    "            if is_good:\n",
    "                mask[begin_idx:begin_idx+i+1] = True\n",
    "            begin_idx += (i+1)\n",
    "    i = 0\n",
    "    insert_idx = 0\n",
    "    while i < len(mask):\n",
    "        if mask[i] and (i == 0 or mask[i-1] == False):\n",
    "            reformed_sent.insert(insert_idx, '<font style=\\\"color:red;\\\">')\n",
    "            insert_idx += 2\n",
    "            i += 1\n",
    "            while i < len(mask) and mask[i]:\n",
    "                i += 1\n",
    "                insert_idx += 1\n",
    "            reformed_sent.insert(insert_idx, '</font>')\n",
    "            insert_idx += 1\n",
    "        insert_idx += 1\n",
    "        i += 1\n",
    "    return ' '.join(reformed_sent)\n",
    "\n",
    "def gen_co_occur_report(report_file:str, g:nx.Graph, keyword:str, word2idx_dict:Dict[str, int], keyword_list:List[str], occur_dict:Dict[str, set], sent_list:List[str], is_entity:bool=True, kw_dist_max:int=6):\n",
    "    neighbors = g.neighbors(word2idx_dict[keyword])\n",
    "    related_kws = [keyword_list[idx] for idx in neighbors]\n",
    "    content = ['<a href=\\\"#%s__%s\\\">%s, %s</a><br>' % (keyword, kw, keyword, kw) for kw in related_kws]\n",
    "    for kw in related_kws:\n",
    "        content.append('<a id=\\\"%s__%s\\\"><h1>%s, %s</h1></a> ' % (keyword, kw, keyword, kw))\n",
    "        sents = [sent_list[i] for i in occur_dict[keyword] & occur_dict[kw]]\n",
    "        if is_entity:\n",
    "            sents = [sent.split() for sent in sents]\n",
    "            sents = [' '.join(sent) for sent in sents if sent.count(keyword) == 1 and sent.count(kw) == 1 and abs(sent.index(keyword) - sent.index(kw)) <= kw_dist_max]\n",
    "        content += ['%s<br><br>' % mark_sent_in_html(sent, [keyword, kw], is_entity=is_entity) for sent in sents]\n",
    "    \n",
    "    my_write(report_file, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate highly related subgraph\n",
    "sub_g = get_subgraph(pair_graph, 0.3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor_test_pairs = [('python', 'java'), ('stack', 'queue')]\n",
    "\n",
    "title = []\n",
    "content = []\n",
    "for pair in neighbor_test_pairs:\n",
    "    mid_set = set(sub_g.neighbors(word2idx_dict[pair[0]])) & set(sub_g.neighbors(word2idx_dict[pair[1]]))\n",
    "    if not mid_set:\n",
    "        print('%s and %s fail in one hop relation' % (pair[0], pair[1]))\n",
    "        continue\n",
    "    title.append('<a href=\\\"#%s__%s\\\">%s, %s</a><br>' % (pair[0], pair[1], pair[0], pair[1]))\n",
    "    content.append('<a id=\\\"%s__%s\\\"></a> <h1>%s, %s</h1>' % (pair[0], pair[1], pair[0], pair[1]))\n",
    "    for mid in mid_set:\n",
    "        mid_text = keyword_list[mid]\n",
    "        content.append('<h2>%s, %s</h2>' % (pair[0], mid_text))\n",
    "        temp_sents = [sent_list[i].split() for i in occur_dict[pair[0]] & occur_dict[mid_text]]\n",
    "        temp_sents = [' '.join(sent) for sent in temp_sents if sent.count(pair[0]) == 1 and sent.count(mid_text) == 1 and abs(sent.index(pair[0]) - sent.index(mid_text)) <= 6]\n",
    "        mark_sents = []\n",
    "        for sent in temp_sents:\n",
    "            doc = nlp(sent)\n",
    "            tokens = [word.text for word in doc]\n",
    "            try:\n",
    "                idx1, idx2 = tokens.index(pair[0]), tokens.index(mid_text)\n",
    "            except:\n",
    "                continue\n",
    "            if doc[idx1].dep_ == 'nsubj' or doc[idx1].dep_ == 'dobj' or doc[idx2].dep_ == 'nsubj' or doc[idx2].dep_ == 'dobj':\n",
    "                mark_sents.append(sent)\n",
    "        content += ['%s<br><br>' % mark_sent_in_html(sent, [pair[0], mid_text]) for sent in mark_sents]\n",
    "        \n",
    "        content.append('<h2>%s, %s</h2>' % (pair[1], mid_text))\n",
    "        temp_sents = [sent_list[i].split() for i in occur_dict[pair[1]] & occur_dict[mid_text]]\n",
    "        temp_sents = [' '.join(sent) for sent in temp_sents if sent.count(pair[1]) == 1 and sent.count(mid_text) == 1 and abs(sent.index(pair[1]) - sent.index(mid_text)) <= 6]\n",
    "        mark_sents = []\n",
    "        for sent in temp_sents:\n",
    "            doc = nlp(sent)\n",
    "            tokens = [word.text for word in doc]\n",
    "            try:\n",
    "                idx1, idx2 = tokens.index(pair[1]), tokens.index(mid_text)\n",
    "            except:\n",
    "                continue\n",
    "            if doc[idx1].dep_ == 'nsubj' or doc[idx1].dep_ == 'dobj' or doc[idx2].dep_ == 'nsubj' or doc[idx2].dep_ == 'dobj':\n",
    "                mark_sents.append(sent)\n",
    "        content += ['%s<br><br>' % mark_sent_in_html(sent, [pair[1], mid_text]) for sent in mark_sents]\n",
    "\n",
    "        # content.append('<h2>%s, %s, %s</h2>' % (pair[0], mid_text, pair[1]))\n",
    "        # temp_sents = [sent_list[i].split() for i in occur_dict[pair[0]] & occur_dict[mid_text] & occur_dict[pair[1]]]\n",
    "        # temp_sents = [' '.join(sent) for sent in temp_sents if sent.count(pair[0]) == 1 and sent.count(pair[1]) == 1 and sent.count(mid_text) == 1]\n",
    "        # content += ['%s<br><br>' % mark_sent_in_html(sent, [pair[0], mid_text, pair[1]]) for sent in temp_sents]\n",
    "my_write('overlap_test.html', title + content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_co_occur_report('ds_co_occur.html', sub_g, 'data_structure', word2idx_dict, keyword_list, occur_dict, sent_list, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate highly related subgraph\n",
    "sub_g = get_subgraph(pair_graph, 0.2, 3, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = []\n",
    "for idx, item in enumerate(sub_g.edges()):\n",
    "    kw0 = keyword_list[item[0]]\n",
    "    kw1 = keyword_list[item[1]]\n",
    "    test_sents += [[sent_list[i], kw0, kw1] for i in occur_dict[kw0] & occur_dict[kw1]]\n",
    "    if idx >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(test_sents, columns=['sent', 'kw0', 'kw1']).sample(frac=1).reset_index(drop=True)[:500]).to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the sentences with OLLIE, Stanford OpenIE or OpenIE5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = my_read('../data/test/co_occur_test.txt')\n",
    "# test_data = [data.split(',') for data in test_data]\n",
    "# test_dict = {data[0] : data[1:] for data in test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sent_dict = {central_kw : set() for central_kw in test_dict}\n",
    "# for central_kw, kws in test_dict.items():\n",
    "#     for kw in kws:\n",
    "#         test_sent_dict[central_kw] |= occur_dict[kw]\n",
    "#     test_sent_dict[central_kw] &= occur_dict[central_kw]\n",
    "\n",
    "# for central_kw, sents in test_sent_dict.items():\n",
    "#     content = [sent_list[i] for i in sents]\n",
    "#     my_write('../data/temp/%s_wiki.txt' % central_kw.replace(' ', '_'), content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_lines = occur_dict['python'] & (occur_dict['java'] | occur_dict['ruby'])\n",
    "# my_write('python_java_ruby.txt', [sent_list[i] for i in test_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openie_data = my_read('../data/temp/pl_wiki_ollie_triple.txt')\n",
    "# # openie_data = my_read('pjr_ollie_triple.txt')\n",
    "# # keywords = set(['data structure', 'binary tree', 'hash table', 'linked list'])\n",
    "# keywords = set(['programming language', 'python', 'java', 'javascript', 'lua', 'scala', 'lisp', 'php', 'ruby', 'smalltalk'])\n",
    "# # keywords = set(['python', 'java', 'ruby'])\n",
    "\n",
    "# qualified_triples = []\n",
    "# for data in openie_data:\n",
    "#     if data:\n",
    "#         arg1, rel, arg2 = data.split(';')\n",
    "#         for kw in keywords:\n",
    "#             if kw in arg1:\n",
    "#                 for kw in keywords:\n",
    "#                     if kw in arg2:\n",
    "#                         qualified_triples.append(data)\n",
    "#                         break\n",
    "#                 break\n",
    "# my_write('pl_wiki_ollie_triple_f.txt', qualified_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_structure_idx = occur_dict['data structure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(co_occur_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co_occur_set = {}\n",
    "# for keyword, idx_set in occur_dict.items():\n",
    "#     intersection = idx_set & data_structure_idx\n",
    "#     if intersection:\n",
    "#         co_occur_set[keyword] = list(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_co_occur_list = sorted(co_occur_set.items(), key=lambda x: len(x[1]), reverse=True)[:100]\n",
    "# sorted_co_occur_count = [(word, len(idx)) for word, idx in sorted_co_occur_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_co_occur_count[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'b-tree' in co_occur_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sent_list[co_occur_set['b-tree'][0]]\n",
    "# temp_list = [sent_list[idx] for idx in co_occur_set['b-tree']]\n",
    "# my_write('ds_bt_sent.txt', temp_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9496c91418be784f00ee6456e4343e8188c649322b68f201c83241a4029a42d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('FWD_py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
