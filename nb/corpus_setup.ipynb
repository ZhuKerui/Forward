{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\n",
    "import sys\n",
    "import wikipedia\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.TextProcessing import build_word_tree, process_keywords\n",
    "from tools.BasicUtils import my_write, my_csv_read, MultiThreading, get_wikipedia_entity"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Select keyword file and generate keyword list"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# unknown keyword file for now\n",
    "stable_kw = []\n",
    "unstable_kw = []\n",
    "f_in = my_csv_read('../data/raw_data/keyword.csv', delimiter='\\t')\n",
    "keywords = [line[0] for line in f_in]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect keywords that can be linked to a wikipedia page\n",
    "stable_kw = []\n",
    "unstable_kw = []\n",
    "r = my_csv_read('../data/raw_data/terms-cs-cfl-epoch200.txt', delimiter='\\t')\n",
    "candidate_kw_list = [item[0] for item in r]\n",
    "stable_kw, unstable_kw = process_keywords(candidate_kw_list)\n",
    "\n",
    "mt = MultiThreading()\n",
    "ret = mt.run(lambda x: (x, get_wikipedia_entity(x)), stable_kw, thread_num=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Process keyword list and generate word tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Process keyword list and store the results\n",
    "stable_kw, unstable_kw = process_keywords(keywords)\n",
    "my_write('../data/corpus/keyword_f.txt', stable_kw)\n",
    "my_write('../data/temp/unstable_keyword.txt', unstable_kw)\n",
    "# Generate word tree (25 seconds)\n",
    "build_word_tree('../data/corpus/keyword_f.txt', '../data/corpus/wordtree.json', '../data/corpus/entity.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check stable keywords\n",
    "'-' in stable_kw"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Process sentences"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Extract sentence from small corpus (2 minute)\n",
    "j = json.load(open('../data/raw_data/small_arxiv.json', 'r'))\n",
    "sents = [i['abstract'].replace('\\n', ' ').strip() for i in j]\n",
    "with open('../data/corpus/pre_small_sent.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(sents))\n",
    "\n",
    "!sed 's/\\$//g;s/\\\\/ /g;s/---*/, /g;s/([^)]*)//g;s/{[^)]*}//g;s/-/ - /g' ../data/corpus/pre_small_sent.txt | tr -s [:space:] | tr '[:upper:]' '[:lower:]' > ../data/corpus/small_sent.txt\n",
    "\n",
    "!python ../py/sent_tokenize.py ../data/corpus/small_sent.txt ../data/corpus/small_sent.txt\n",
    "\n",
    "!rm ../data/corpus/pre_small_sent.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Extract sentence from 1st_sents_new.json\n",
    "file_dir = '../data/raw_data/1st-sents-new.json'\n",
    "\n",
    "j = json.load(open(file_dir, 'r'))\n",
    "sents = [i['sentence'].strip() for i in j.values()]\n",
    "with open('../data/corpus/pre_1st_sent.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(sents))\n",
    "\n",
    "!sed 's/\\$//g;s/\\\\/ /g;s/---*/, /g;s/([^)]*)//g;s/{[^)]*}//g;s/-/ - /g' ../data/corpus/pre_1st_sent.txt | tr -s [:space:] | tr '[:upper:]' '[:lower:]' > ../data/corpus/1st_sent.txt\n",
    "\n",
    "!rm ../data/corpus/pre_1st_sent.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate Reformed sentences (3 minutes)\n",
    "# To run the code in the backend, use the gen_reform.py in the \"py\" folder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keyword process using terms-cs-cfl-epoch200.txt and 1st-sents-new.json"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "wiki_kw_dict = json.load(open('../data/corpus/1st-sents-lowercase.json', 'r'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "wiki_kw_dict['python']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sentence': 'python was a cold war contingency plan of the british government for the continuity of government in the event of nuclear war .',\n",
       " 'entity': {'cold war': 'cold war',\n",
       "  'british government': 'government of the united kingdom',\n",
       "  'continuity of government': 'continuity of government',\n",
       "  'nuclear war': 'nuclear warfare'}}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "'convolutional neural network' in candidate_kw_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "with open('../data/corpus/keyword_f.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(stable_kw))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}