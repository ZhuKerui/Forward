{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.TextProcessing import sent_lemmatize, build_word_tree"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Process keyword file (20 seconds)\n",
    "\n",
    "filtered_words = set(['can', 'it', 'work', 'in', 'parts', 'is', 'its', 'or', 'and', 'a','b','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z', ''])\n",
    "\n",
    "stable_kw = []\n",
    "unstable_kw = []\n",
    "with open('../data/raw_data/keyword.csv', 'r') as f_in:\n",
    "    for line in f_in:\n",
    "        kw = line.split(',')[0]\n",
    "        if '- ' in kw:\n",
    "            continue\n",
    "        splited = kw.replace('-', ' - ')\n",
    "        reformed = ' '.join(sent_lemmatize(splited))\n",
    "        if reformed in filtered_words:\n",
    "            continue\n",
    "        if reformed == splited:\n",
    "            stable_kw.append(kw)\n",
    "        else:\n",
    "            unstable_kw.append('%s\\t%s' % (kw, reformed))\n",
    "\n",
    "with open('../data/corpus/keyword_f.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(stable_kw))\n",
    "\n",
    "with open('../data/temp/unstable_keyword.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(unstable_kw))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check stable keywords\n",
    "'-' in stable_kw"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Generate word tree (25 seconds)\n",
    "build_word_tree('../data/corpus/keyword_f.txt', '../data/corpus/wordtree.json', '../data/corpus/entity.txt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building word tree is accomplished with 12699 words added\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Extract sentence from small corpus (2 minute)\n",
    "j = json.load(open('../data/raw_data/small_arxiv.json', 'r'))\n",
    "sents = [i['abstract'].replace('\\n', ' ').strip() for i in j]\n",
    "with open('../data/corpus/pre_small_sent.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(sents))\n",
    "\n",
    "!sed 's/\\$//g;s/\\\\/ /g;s/---*/, /g;s/([^)]*)//g;s/{[^)]*}//g;s/-/ - /g' ../data/corpus/pre_small_sent.txt | tr -s [:space:] | tr '[:upper:]' '[:lower:]' > ../data/corpus/small_sent.txt\n",
    "\n",
    "!python ../py/sent_tokenize.py ../data/corpus/small_sent.txt ../data/corpus/small_sent.txt\n",
    "\n",
    "!rm ../data/corpus/pre_small_sent.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Extract sentence from 1st_sents_new.json\n",
    "file_dir = '../data/raw_data/1st-sents-new.json'\n",
    "\n",
    "j = json.load(open(file_dir, 'r'))\n",
    "sents = [i['sentence'].strip() for i in j.values()]\n",
    "with open('../data/corpus/pre_1st_sent.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(sents))\n",
    "\n",
    "!sed 's/\\$//g;s/\\\\/ /g;s/---*/, /g;s/([^)]*)//g;s/{[^)]*}//g;s/-/ - /g' ../data/corpus/pre_1st_sent.txt | tr -s [:space:] | tr '[:upper:]' '[:lower:]' > ../data/corpus/1st_sent.txt\n",
    "\n",
    "!rm ../data/corpus/pre_1st_sent.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate Reformed sentences (3 minutes)\n",
    "# To run the code in the backend, use the gen_reform.py in the \"py\" folder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keyword process using terms-cs-cfl-epoch200.txt and 1st-sents-new.json"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "filtered_words = set(['can', 'it', 'work', 'in', 'parts', 'is', 'its', 'or', 'and', 'a','b','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z', ''])\n",
    "\n",
    "wiki_kw_dict = json.load(open('../data/corpus/1st-sents-lowercase.json', 'r'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "stable_kw = []\n",
    "unstable_kw = []\n",
    "with open('../data/raw_data/terms-cs-cfl-epoch200.txt', 'r') as f_in:\n",
    "    r = csv.reader(f_in, delimiter='\\t')\n",
    "    candidate_kw_list = [item[0] for item in r if item[0] in wiki_kw_dict]\n",
    "    print('convolutional neural network' in candidate_kw_list)\n",
    "    for kw in candidate_kw_list:\n",
    "        if '- ' in kw:\n",
    "            continue\n",
    "        splited = kw.replace('-', ' - ')\n",
    "        reformed = ' '.join(sent_lemmatize(splited))\n",
    "        if reformed in filtered_words:\n",
    "            continue\n",
    "        if reformed == splited:\n",
    "            stable_kw.append(kw)\n",
    "        else:\n",
    "            unstable_kw.append('%s\\t%s' % (kw, reformed))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "wiki_kw_dict['python']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'sentence': 'python was a cold war contingency plan of the british government for the continuity of government in the event of nuclear war .',\n",
       " 'entity': {'cold war': 'cold war',\n",
       "  'british government': 'government of the united kingdom',\n",
       "  'continuity of government': 'continuity of government',\n",
       "  'nuclear war': 'nuclear warfare'}}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "'convolutional neural network' in candidate_kw_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "with open('../data/corpus/keyword_f.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(stable_kw))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}