{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.BasicUtils import my_read, my_write, MultiProcessing\n",
    "from tools.TextProcessing import sent_lemmatize, build_word_tree\n",
    "from tools.DocProcessing.SentenceReformer import SentenceReformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process keyword file (20 seconds)\n",
    "\n",
    "filtered_words = set(['can', 'it', 'work', 'in', 'parts', 'is', 'its', 'a','b','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z', ''])\n",
    "\n",
    "stable_kw = []\n",
    "unstable_kw = []\n",
    "with open('../data/raw_data/keyword.csv', 'r') as f_in:\n",
    "    for line in f_in:\n",
    "        kw = line.split(',')[0]\n",
    "        if '- ' in kw:\n",
    "            continue\n",
    "        splited = kw.replace('-', ' - ')\n",
    "        reformed = ' '.join(sent_lemmatize(splited))\n",
    "        if reformed in filtered_words:\n",
    "            continue\n",
    "        if reformed == splited:\n",
    "            stable_kw.append(kw)\n",
    "        else:\n",
    "            unstable_kw.append('%s\\t%s' % (kw, reformed))\n",
    "\n",
    "with open('../data/corpus/keyword_f.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(stable_kw))\n",
    "\n",
    "with open('../data/temp/unstable_keyword.txt', 'w') as f_out:\n",
    "    f_out.write('\\n'.join(unstable_kw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check stable keywords\n",
    "'-' in stable_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word tree (25 seconds)\n",
    "build_word_tree('../data/corpus/keyword_f.txt', '../data/corpus/wordtree.json', '../data/corpus/entity.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentence from small corpus (2 minute)\n",
    "!sed -n '/^ *\"abstract/p' ../data/raw_data/small_arxiv.json | sed 's/.*\\\"abstract\\\": \\\"  \\(.*\\)\\\\n\\\",.*/\\1/;s/\\\\n/ /g;s/\\$//g;s/\\\\/ /g;s/---*/, /g;s/([^)]*)//g;s/{[^)]*}//g;s/-/ - /g' | tr -s [:space:] | tr '[:upper:]' '[:lower:]' > ../data/corpus/small_sent.txt\n",
    "\n",
    "!python ../py/sent_tokenize.py ../data/corpus/small_sent.txt ../data/corpus/small_sent.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Reformed sentences (3 minutes)\n",
    "# To run the code in the backend, use the gen_reform.py in the \"py\" folder"
   ]
  }
 ]
}