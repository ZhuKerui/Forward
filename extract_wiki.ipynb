{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences from Wikipedia\n",
    "+ This notebook is used for collecting sentences that tell relationship between two entities from wikipedia using some dependency path pattern\n",
    "+ **This notebook is fully valid under Owl3 machine (using the /scratch/data/wikipedia/full_text-2021-03-20 data)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki sub folder example: ../data/wikipedia/full_text-2021-03-20/BE\n",
      "save sub folder example: data/extract_wiki/wiki_sent_collect/BE\n",
      "wiki file example: ../data/wikipedia/full_text-2021-03-20/BE/wiki_00\n",
      "save sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.dat\n",
      "save cooccur file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_co.dat\n",
      "save selected sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_se.dat\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wikipedia\n",
    "import os\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "from collections import Counter\n",
    "import bz2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from typing import List\n",
    "from nltk.corpus import stopwords\n",
    "self_define_stopwords = set(['-', ',', '.'])\n",
    "sw = set(stopwords.words('english'))\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "random.seed(0)\n",
    "import torch\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from tools.BasicUtils import my_write, my_read_pickle, my_write_pickle\n",
    "from tools.TextProcessing import (\n",
    "                my_sentence_tokenize,\n",
    "                my_sentence_tokenize, filter_specific_keywords, nlp, \n",
    "                exact_match\n",
    "                )\n",
    "\n",
    "from extract_wiki import (\n",
    "    save_path, entity_occur_from_cooccur_file, graph_file, single_sent_graph_file, \n",
    "    w2vec_dump_file, sub_path_pattern_count_file, \n",
    "    w2vec_keyword2idx_file, \n",
    "    test_path, path_test_file, \n",
    "    path_pattern_count_file, \n",
    "    save_sub_folders, wiki_sub_folders, \n",
    "    wiki_files, save_sent_files, save_cooccur_files, save_selected_files, \n",
    "    p, patterns, FeatureProcess, wikipedia_entity_file, CalFreq, \n",
    "    note2line, cal_score_from_df, cal_freq_from_df, \n",
    "    gen_kw_from_wiki_ent, get_entity_page, gen_corepath_pattern, find_triangles, find_path_between_pair, \n",
    "    generate_sample,\n",
    "    sample_to_neo4j, get_sentence, informativeness_demo, process_list\n",
    ")\n",
    "\n",
    "# Generate the save dir\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.mkdir(test_path)\n",
    "\n",
    "for save_dir in save_sub_folders:\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "# Get all files under wikipedia/full_text-2021-03-20\n",
    "\n",
    "print('wiki sub folder example:', wiki_sub_folders[0])\n",
    "print('save sub folder example:', save_sub_folders[0])\n",
    "print('wiki file example:', wiki_files[0])\n",
    "print('save sentence file example:', save_sent_files[0])\n",
    "print('save cooccur file example:', save_cooccur_files[0])\n",
    "print('save selected sentence file example:', save_selected_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] wikipedia2vec\n",
    "with bz2.open(w2vec_dump_file) as f_in:\n",
    "    w2vec = Wikipedia2Vec.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] wikipedia2vec\n",
    "\n",
    "# Find similar words or entities\n",
    "# ent1 = 'Python (programming language)'\n",
    "# w2vec.most_similar_by_vector(w2vec.get_entity_vector(ent1), 20)\n",
    "\n",
    "# Get similarity between two entities\n",
    "# ent1 = 'Machine learning'\n",
    "# ent2 = 'Information science'\n",
    "# cosine_similarity(w2vec.get_entity_vector(ent1).reshape(1, -1), w2vec.get_entity_vector(ent2).reshape(1, -1))[0, 0]\n",
    "\n",
    "# Check the entity count and document count\n",
    "# ent1 = 'Hidden Markov model'\n",
    "# e = w2vec.get_entity(ent1)\n",
    "# print(e.count)\n",
    "# print(e.doc_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect sentences, entities, entities co-occurrances, titles from wikipedia dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roughly collect sentences, entity co-occurrances, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] Test get_sentence function\n",
    "get_sentence(wiki_files[0], 'sent.txt', 'cooccur.txt', 'title.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py collect_sent_and_cooccur (8 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct entity mapping in co-occurrance files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py correct_mapping_in_cooccur (6 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect cooccur similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py cal_cooccur_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Information retrieval is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Test]\n",
    "lines = get_entity_page('Information retrieval')\n",
    "sents = [note2line(note) for note in lines]\n",
    "occurs = [note2line(note, '_co_.dat') for note in lines]\n",
    "ori_occurs = [note2line(note, '_co.dat') for note in lines]\n",
    "pairs_list = [note2line(note, '_pr.dat') for note in lines]\n",
    "my_write('sent_check.txt', sents)\n",
    "my_write('occur_check.txt', occurs)\n",
    "my_write('ori_occur_check.txt', ori_occurs)\n",
    "my_write('pair_check.txt', pairs_list)\n",
    "note2line(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate entity occurrance from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the entity occurrance dict from co-occurrance info [collect_ent_occur_from_cooccur]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Not necessary] Mapping keyword mention to wikipedia2vec entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wikipedia_entity_file) as f_in:\n",
    "    wikipedia_entity = set(f_in.read().split('\\n'))\n",
    "w2vec\n",
    "\n",
    "w2vec_keyword2idx = {}\n",
    "\n",
    "for entity in tqdm.tqdm(wikipedia_entity):\n",
    "    w2vec_entity = w2vec.get_entity(entity)\n",
    "    if w2vec_entity is None:\n",
    "        continue\n",
    "    kw = gen_kw_from_wiki_ent(entity)\n",
    "    if kw not in w2vec_keyword2idx:\n",
    "        w2vec_keyword2idx[kw] = [w2vec_entity.index]\n",
    "    else:\n",
    "        if w2vec_entity.index not in w2vec_keyword2idx[kw]:\n",
    "            w2vec_keyword2idx[kw].append(w2vec_entity.index)\n",
    "w2vec_kws = filter_specific_keywords(list(w2vec_keyword2idx.keys()))\n",
    "filter_keyword_from_w2vec = set(w2vec_kws)\n",
    "w2vec_keyword2idx = {k:v for k, v in w2vec_keyword2idx.items() if k in filter_keyword_from_w2vec}\n",
    "my_write_pickle(w2vec_keyword2idx_file, w2vec_keyword2idx)\n",
    "len(w2vec_keyword2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] w2vec_keyword2idx\n",
    "w2vec_keyword2idx = my_read_pickle(w2vec_keyword2idx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] w2vec_keyword2idx\n",
    "kw = 'feature engineering'\n",
    "kw_in_mention = kw in w2vec_keyword2idx\n",
    "print(kw_in_mention)\n",
    "if kw_in_mention:\n",
    "    for idx in w2vec_keyword2idx[kw]:\n",
    "        print(w2vec.dictionary.get_item_by_index(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect pattern frequency counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py collect_subpath_pattern_freq\n",
    "# python extract_wiki.py collect_pattern_freq (12 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i_nsubj attr prep pobj', 150),\n",
       " ('i_nsubj prep pobj', 146),\n",
       " ('i_nsubj attr', 89),\n",
       " ('i_nsubj attr prep pobj prep pobj', 81),\n",
       " ('i_nsubj dobj', 79),\n",
       " ('i_nsubjpass prep pobj', 72),\n",
       " ('i_nsubj prep pobj prep pobj', 70),\n",
       " ('i_nsubjpass prep pobj prep pobj', 52),\n",
       " ('i_nsubj dobj prep pobj', 47),\n",
       " ('i_nsubjpass agent pobj', 25)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Load] cal_freq function\n",
    "cal_freq = CalFreq(path_pattern_count_file)\n",
    "cal_freq.c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58685962811188"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Test] cal_freq function\n",
    "cal_freq.cal_freq_from_path('i_nsubj prep pobj prep pobj prep pobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>kw1</th>\n",
       "      <th>kw1_span</th>\n",
       "      <th>kw1_ent</th>\n",
       "      <th>kw2</th>\n",
       "      <th>kw2_span</th>\n",
       "      <th>kw2_ent</th>\n",
       "      <th>sent</th>\n",
       "      <th>dep_path</th>\n",
       "      <th>pattern</th>\n",
       "      <th>dep_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.701339</td>\n",
       "      <td>Archbishop of Cologne</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>Archbishop of Cologne</td>\n",
       "      <td>Anno II</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>Anno II</td>\n",
       "      <td>Anno II was Archbishop of Cologne from 1056 un...</td>\n",
       "      <td>i_attr nsubj</td>\n",
       "      <td>i_nsubj attr</td>\n",
       "      <td>0.949983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.747330</td>\n",
       "      <td>The Doobie Brothers</td>\n",
       "      <td>(16, 18)</td>\n",
       "      <td>The Doobie Brothers</td>\n",
       "      <td>Livin ' on the Fault Line</td>\n",
       "      <td>(0, 5)</td>\n",
       "      <td>Livin' on the Fault Line</td>\n",
       "      <td>Livin' on the Fault Line is the seventh studio...</td>\n",
       "      <td>i_appos i_pobj i_prep i_attr nsubj</td>\n",
       "      <td>i_nsubj attr prep pobj</td>\n",
       "      <td>0.990815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.631102</td>\n",
       "      <td>Little Darling</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Little Darling (I Need You)</td>\n",
       "      <td>Marvin Gaye</td>\n",
       "      <td>(11, 12)</td>\n",
       "      <td>Marvin Gaye</td>\n",
       "      <td>The track \"Little Darling \" is a remake of the...</td>\n",
       "      <td>i_appos i_nsubj attr prep pobj</td>\n",
       "      <td>i_nsubj attr prep pobj</td>\n",
       "      <td>0.845042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.614200</td>\n",
       "      <td>Lombardy</td>\n",
       "      <td>(13, 13)</td>\n",
       "      <td>Lombardy</td>\n",
       "      <td>Bagolino</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>Bagolino</td>\n",
       "      <td>Bagolino is a \"comune\" in the province of Bres...</td>\n",
       "      <td>i_pobj i_prep i_attr nsubj</td>\n",
       "      <td>i_nsubj attr prep pobj</td>\n",
       "      <td>0.888238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.806008</td>\n",
       "      <td>Valle Sabbia</td>\n",
       "      <td>(30, 31)</td>\n",
       "      <td>Valle Sabbia</td>\n",
       "      <td>Bagolino</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>Bagolino</td>\n",
       "      <td>Bagolino is a \"comune\" in the province of Bres...</td>\n",
       "      <td>i_pobj i_prep i_pobj i_prep nsubj</td>\n",
       "      <td>i_nsubj prep pobj prep pobj</td>\n",
       "      <td>0.342975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sim                    kw1  kw1_span                      kw1_ent  \\\n",
       "0  0.701339  Archbishop of Cologne    (3, 5)        Archbishop of Cologne   \n",
       "1  0.747330    The Doobie Brothers  (16, 18)          The Doobie Brothers   \n",
       "2  0.631102         Little Darling    (3, 4)  Little Darling (I Need You)   \n",
       "3  0.614200               Lombardy  (13, 13)                     Lombardy   \n",
       "4  0.806008           Valle Sabbia  (30, 31)                 Valle Sabbia   \n",
       "\n",
       "                         kw2  kw2_span                   kw2_ent  \\\n",
       "0                    Anno II    (0, 1)                   Anno II   \n",
       "1  Livin ' on the Fault Line    (0, 5)  Livin' on the Fault Line   \n",
       "2                Marvin Gaye  (11, 12)               Marvin Gaye   \n",
       "3                   Bagolino    (0, 0)                  Bagolino   \n",
       "4                   Bagolino    (0, 0)                  Bagolino   \n",
       "\n",
       "                                                sent  \\\n",
       "0  Anno II was Archbishop of Cologne from 1056 un...   \n",
       "1  Livin' on the Fault Line is the seventh studio...   \n",
       "2  The track \"Little Darling \" is a remake of the...   \n",
       "3  Bagolino is a \"comune\" in the province of Bres...   \n",
       "4  Bagolino is a \"comune\" in the province of Bres...   \n",
       "\n",
       "                             dep_path                      pattern  \\\n",
       "0                        i_attr nsubj                 i_nsubj attr   \n",
       "1  i_appos i_pobj i_prep i_attr nsubj       i_nsubj attr prep pobj   \n",
       "2      i_appos i_nsubj attr prep pobj       i_nsubj attr prep pobj   \n",
       "3          i_pobj i_prep i_attr nsubj       i_nsubj attr prep pobj   \n",
       "4   i_pobj i_prep i_pobj i_prep nsubj  i_nsubj prep pobj prep pobj   \n",
       "\n",
       "   dep_coverage  \n",
       "0      0.949983  \n",
       "1      0.990815  \n",
       "2      0.845042  \n",
       "3      0.888238  \n",
       "4      0.342975  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(path_test_file, sep='\\t')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py collect_dataset (18 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Prepration] Sentence-edged Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py generate_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate sentence graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py generate_sent_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Test] Check the score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Graph\n",
    "graph:nx.Graph = my_read_pickle(graph_file)\n",
    "\n",
    "print('num of nodes:', len(graph.nodes))\n",
    "print('num of edges:', len(graph.edges))\n",
    "\n",
    "single_sent_graph:nx.Graph = my_read_pickle(single_sent_graph_file)\n",
    "print('num of nodes:', len(single_sent_graph.nodes))\n",
    "print('num of edges:', len(single_sent_graph.edges))\n",
    "\n",
    "cal_freq = CalFreq(path_pattern_count_file)\n",
    "d = my_read_pickle(entity_occur_from_cooccur_file)\n",
    "\n",
    "fp = FeatureProcess(sub_path_pattern_count_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] graph\n",
    "ent1 = 'Machine learning'\n",
    "# Check the neighbours of an entity\n",
    "list(graph.neighbors(ent1))\n",
    "\n",
    "# Check the edges of two entities\n",
    "# edges = graph.edges[ent1, 'Hinge loss']\n",
    "# edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect highest scored sentence for pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "edges = list(single_sent_graph.edges)\n",
    "random.Random(0).shuffle(edges)\n",
    "for edge in edges[:100]:\n",
    "    data = single_sent_graph.get_edge_data(*edge)\n",
    "    sents.append({'ent1' : edge[0], \n",
    "                  'ent2' : edge[1], \n",
    "                  'sent' : note2line(data['data'][0]['note']).strip(), \n",
    "                  'score' : data['data'][0]['score']})\n",
    "pd.DataFrame(sents).to_csv('highest_sents.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single sentence significance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = informativeness_demo(\"Many operating systems let a program return a result when its process terminates;\", 'Operating system', 'Process', fp)\n",
    "df.to_csv('temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'kw1_span': (1, 2),\n",
       "  'kw2_span': (11, 11),\n",
       "  'pattern': 'i_nsubj ccomp advcl nsubj',\n",
       "  'dep_path': 'i_nsubj ccomp advcl nsubj',\n",
       "  'dep_coverage': 0.8732452507251984}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.feature_process(nlp(\"Many operating systems let a program return a result when its process terminates;\"), 'Operating system', 'Process')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check score function on sentences from a pair of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent1 = 'Marko Attila Hoare'\n",
    "ent2 = 'Serbs'\n",
    "sents = [note2line(note).strip() for note in d[ent1] & d[ent2]]\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 35.50it/s]\n"
     ]
    }
   ],
   "source": [
    "b = cal_freq_from_df(pd.DataFrame(process_list(sents, [str((0, ent1, ent2))]*len(sents), fp.batched_feature_process)), cal_freq)\n",
    "b = cal_score_from_df(b)\n",
    "b = b.sort_values(by=['score'], ascending=False)\n",
    "b.to_csv('sentences.csv', index=False, columns=['kw1', 'kw1_span', 'kw2', 'kw2_span', 'sent', 'dep_coverage', 'pattern_freq', 'pattern', 'score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check score function on sentences from pairs containing one entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [00:59<00:00,  3.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = []\n",
    "ent = 'Machine learning'\n",
    "for neighbor in tqdm.tqdm(list(graph.neighbors(ent))):\n",
    "    if neighbor not in d:\n",
    "        continue\n",
    "    s = d[ent] & d[neighbor]\n",
    "    s = list(s)\n",
    "    sents = [note2line(note).strip() for note in s]\n",
    "    pairs = [{'kw1' : gen_kw_from_wiki_ent(ent, False), 'kw2' : gen_kw_from_wiki_ent(neighbor, False)}]\n",
    "    temp_list = []\n",
    "    for idx, sent in enumerate(sents):\n",
    "        res = fp.batched_feature_process(sent, pairs)\n",
    "        for i in res:\n",
    "            i['sent'] = sent\n",
    "            i['note'] = s[idx]\n",
    "        temp_list.extend(res)\n",
    "    if not temp_list:\n",
    "        continue\n",
    "    df = pd.DataFrame(temp_list)\n",
    "    df = cal_freq_from_df(df, cal_freq)\n",
    "    df = cal_score_from_df(df)\n",
    "    df = df.sort_values(by=['score'], ascending=False)\n",
    "    examples.append(df)\n",
    "\n",
    "test_df = pd.concat(examples)\n",
    "test_df.to_csv(ent + '.csv', index=False, columns=['kw1', 'kw1_span', 'kw2', 'kw2_span', 'note', 'sent', 'dep_coverage', 'pattern_freq', 'pattern', 'score'])\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py collect_score_function_eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test score function with human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(sent:str, ent1:str, ent2:str):\n",
    "    kw1 = gen_kw_from_wiki_ent(ent1)\n",
    "    kw2 = gen_kw_from_wiki_ent(ent2)\n",
    "    data = fp.batched_feature_process(sent, [{'kw1' : kw1, 'kw2' : kw2}])\n",
    "    if not data:\n",
    "        return -1\n",
    "    data = data[0]\n",
    "    pattern_freq = cal_freq.cal_freq_from_path(gen_corepath_pattern(data['dep_path']))\n",
    "    dep_coverage = data['dep_coverage']\n",
    "    return (2*dep_coverage*pattern_freq) / (pattern_freq+dep_coverage)\n",
    "\n",
    "test_data = pd.read_csv('test.tsv', sep='\\t')\n",
    "score_function_result = test_data.copy()\n",
    "score_function_result['score'] = score_function_result.apply(lambda x: get_score(x['sentence'], x['entity 1'], x['entity 2']), axis=1)\n",
    "score_function_result.to_csv('score_function_result.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('score_function_result.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    sf_score = [float(lines[i].strip().split('\\t')[-1]) for i in range(1, len(lines))]\n",
    "    sf_score = np.array(sf_score)\n",
    "\n",
    "with open('user_label.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    user_score = [float(lines[i].strip().split('\\t')[-1]) / 5 for i in range(1, len(lines))]\n",
    "    user_score = np.array(user_score)\n",
    "    \n",
    "with open('user_label.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    data = []\n",
    "    for i in range(1, len(lines)):\n",
    "        ent1, ent2, sent, user_score_ = lines[i].strip().split('\\t')\n",
    "        data.append({'entity 1' : ent1, 'entity 2' : ent2, 'sentence' : sent, 'user label' : float(user_score_)/5})\n",
    "        \n",
    "sf_score = sf_score[user_score > 0]\n",
    "user_score = user_score[user_score > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(sf_score, user_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score range within [1,2,3,4,5]\n",
    "l2 = np.mean(np.abs(sf_score*5 - user_score*5))\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(user_score*5)\n",
    "np.mean(user_score[sf_score>0.7])\n",
    "np.mean(user_score[sf_score<=0.6])\n",
    "np.mean(sf_score*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.abs(sf_score - user_score)\n",
    "diff = []\n",
    "for i in range(len(dist)):\n",
    "    if dist[i] > 0.3:\n",
    "        diff.append(data[i].copy())\n",
    "        diff[-1]['score function label'] = sf_score[i]\n",
    "diff = pd.DataFrame(diff)\n",
    "len(diff)\n",
    "diff.to_csv('diff.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data (Below are not verified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Single sentence graph\n",
    "single_sent_graph = my_read_pickle(single_sent_graph_file)\n",
    "print('number of nodes:', single_sent_graph.number_of_nodes())\n",
    "print('number of edges:', single_sent_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data of level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "ent1 = 'Machine learning'\n",
    "ent2 = 'Algorithm'\n",
    "sample = generate_sample(single_sent_graph, ent1, ent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "triple:list = sample['triple']\n",
    "sources:List[str] = sample['source']\n",
    "entity:List[str] = sample['entity']\n",
    "avg_scores = [mean([tri['score'] for tri in path]) for path in triple]\n",
    "sorted_list = sorted(zip(avg_scores, triple), key=lambda x: x[0], reverse=True)\n",
    "triple = list(zip(*sorted_list))[1]\n",
    "contexts = [[{'e1' : entity[tri['e1']], \n",
    "            'e2' : entity[tri['e2']], \n",
    "            'sent' : sources[tri['sent']],\n",
    "            'score' : tri['score']} for tri in path] for path in triple]\n",
    "ctxs = []\n",
    "for ctx in contexts[:5]:\n",
    "    path = [ctx[0]['e1']]\n",
    "    sents = []\n",
    "    for i, tri in enumerate(ctx):\n",
    "        path.append(tri['e2'])\n",
    "        sents.append('sentence%d: %s' % (i+1, tri['sent']))\n",
    "    path = '; '.join(path)\n",
    "    sents = ' '.join(sents)\n",
    "    ctxs.append('%s %s' % ('path: ' + path, sents))\n",
    "for ctx in ctxs:\n",
    "    print(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data of level 1 [collect_sample_from_single_sent_graph] (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_explicit.json') as f_in:\n",
    "    a = json.load(f_in)\n",
    "    print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [len(set([tri['sent'] for tri in path])) < len(path) for item in a for path in item['triple']]\n",
    "sum(b)/len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [any([len(set([tri['sent'] for tri in path])) < len(path) for path in item['triple']]) for item in a]\n",
    "sum(b)/len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(item['triple']) for item in a])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_neo4j(a[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "random.shuffle(a)\n",
    "train_ratio = 0.96\n",
    "valid_ratio = 0.02\n",
    "training_data = a[:int(len(a)*train_ratio)]\n",
    "valid_data = a[int(len(a)*train_ratio):int(len(a)*(train_ratio+valid_ratio))]\n",
    "test_data = a[int(len(a)*(train_ratio+valid_ratio)):]\n",
    "with open('train.json', 'w') as f_out:\n",
    "    json.dump(training_data, f_out)\n",
    "with open('dev.json', 'w') as f_out:\n",
    "    json.dump(valid_data, f_out)\n",
    "with open('test.json', 'w') as f_out:\n",
    "    json.dump(test_data, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json') as f_in:\n",
    "    test_samples = json.load(f_in)\n",
    "head = [test_samples[i]['pair'] for i in range(5)]\n",
    "head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MyFiD/data/test.json') as f_in:\n",
    "    data = json.load(f_in)\n",
    "    eval_data = data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([{'ent1' : sample['pair'][0], 'ent2' : sample['pair'][1], 'sent' : sample['target']} for sample in eval_data]).to_csv('evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent1 = 'Shamsin'\n",
    "ent2 = 'Homs Governorate'\n",
    "sample = {}\n",
    "for sample in eval_data:\n",
    "    if ent1 in sample['pair'] and ent2 in sample['pair']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data for level 1 with random path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_occur_from_cooccur = my_read_pickle(entity_occur_from_cooccur_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_in, file_out in [('MyFiD/data/train.json', 'train_random.json'), ('MyFiD/data/dev.json', 'dev_random.json'), ('MyFiD/data/test.json', 'test_random.json')]:\n",
    "    with open(file_in) as f_in:\n",
    "        data = json.load(f_in)\n",
    "        for item in tqdm.tqdm(data):\n",
    "            for path in item['triple']:\n",
    "                for tri in path:\n",
    "                    item['source'][tri['sent']] = note2line(random.choice(list(entity_occur_from_cooccur[item['entity'][tri['e1']]] & entity_occur_from_cooccur[item['entity'][tri['e2']]]))).strip()\n",
    "        with open(file_out, 'w') as f_out:\n",
    "            json.dump(data, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len_count = []\n",
    "for data in training_data:\n",
    "    sents = data['source']\n",
    "    sent_len_count.extend([len(sent.split()) for sent in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sent_len_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x[x<100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent_len_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_neo4j(items[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data of level 2 from level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.json') as f_in:\n",
    "    sample = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_sample = generate_second_level_sample(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph4nlp.pytorch.data import GraphData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_labels = list(nlp.get_pipe(\"parser\").labels)\n",
    "dep_labels.extend(['i_'+dep for dep in dep_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GraphData()\n",
    "is_rel = []\n",
    "is_entity = []\n",
    "\n",
    "g.add_nodes(1)\n",
    "is_rel.append(0)\n",
    "is_entity.append(0)\n",
    "\n",
    "for src in second_level_sample['sources']:\n",
    "    pair = src['pair']\n",
    "    sent_tokens = src['sent']\n",
    "    \n",
    "    label_list = []\n",
    "    label_list.extend(sent_tokens)\n",
    "    token_num = len(sent_tokens)\n",
    "    start_node = g.get_node_num()\n",
    "    g.add_nodes(token_num)\n",
    "    is_rel.extend([0]*token_num)\n",
    "    is_entity.extend([0]*token_num)\n",
    "    is_entity[pair[0]+start_node] = 1\n",
    "    is_entity[pair[1]+start_node] = 1\n",
    "    \n",
    "    label_list.extend(['ROOT', 'ROOT', 'i_ROOT', 'i_ROOT'])\n",
    "    rel_start_node = start_node + token_num\n",
    "    g.add_nodes(4)\n",
    "    is_rel.extend([1]*4)\n",
    "    is_entity.extend([0]*4)\n",
    "    g.add_edges([0, 0], [rel_start_node, rel_start_node+1])\n",
    "    g.add_edges([rel_start_node, rel_start_node+1], [pair[0]+start_node, pair[1]+start_node])\n",
    "    g.add_edges([pair[0]+start_node, pair[1]+start_node], [rel_start_node+2, rel_start_node+3])\n",
    "    g.add_edges([rel_start_node+2, rel_start_node+3], [0, 0])\n",
    "    \n",
    "    rel_start_node += 4\n",
    "    triples = src['graph']\n",
    "    rel_num = len(triples)\n",
    "    is_rel.extend([1]*rel_num)\n",
    "    is_entity.extend([0]*rel_num)\n",
    "    g.add_nodes(rel_num)\n",
    "    for rel_idx, (tok_1, tok_2, rel) in enumerate(triples):\n",
    "        g.add_edges([tok_1+start_node, rel_idx+rel_start_node], [rel_idx+rel_start_node, tok_2+start_node])\n",
    "        label_list.append(rel)\n",
    "    for i, label in enumerate(label_list):\n",
    "        g.node_attributes[i+start_node]['label'] = label\n",
    "g.node_features['is_rel'] = torch.BoolTensor(is_rel)\n",
    "g.node_features['is_entity'] = torch.BoolTensor(is_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.get_edge_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_triangle_with_node(graph:nx.Graph, first_node:str, second_node:str='', third_node:str=''):\n",
    "    triangles = list(find_triangles(graph, first_node))\n",
    "    triangles.sort(key=lambda x: x[1])\n",
    "    triangle_with_sents = []\n",
    "    n_seen = set()\n",
    "    for n1, n2, n3 in triangles:\n",
    "        if second_node and n2 != second_node and n3 != second_node:\n",
    "            continue\n",
    "        if third_node and n2 != third_node and n3 != third_node:\n",
    "            continue\n",
    "        if n2 not in n_seen:\n",
    "            n_seen.add(n2)\n",
    "            triangle_with_sents.append((n1, note2line(graph.get_edge_data(n1, n2)['note']).strip(), n2, graph.get_edge_data(n1, n2)['score']))\n",
    "        if n3 not in n_seen:\n",
    "            n_seen.add(n3)\n",
    "            triangle_with_sents.append((n1, note2line(graph.get_edge_data(n1, n3)['note']).strip(), n3, graph.get_edge_data(n1, n3)['score']))\n",
    "        triangle_with_sents.append((n2, note2line(graph.get_edge_data(n3, n2)['note']).strip(), n3, graph.get_edge_data(n3, n2)['score']))\n",
    "    return triangle_with_sents\n",
    "\n",
    "\n",
    "def isf(w:str, D:int, counters:List[Counter]):\n",
    "    return math.log(D * 1.0 / sum([1 if w in sent else 0 for sent in counters]))\n",
    "\n",
    "\n",
    "def do_pagerank(sents:List[str]):\n",
    "    # Remove stop words\n",
    "    clean_sents = [[token for token in sent.split() if token not in sw and token not in self_define_stopwords] for sent in sents]\n",
    "\n",
    "    # Generate word counters\n",
    "    counters = [Counter(sent) for sent in clean_sents]\n",
    "\n",
    "    # Build similarity matrix\n",
    "    D = len(clean_sents)\n",
    "    sim_matrix = np.zeros((D, D))\n",
    "    part_list = [math.sqrt(sum([(sent[w] * isf(w, D, counters)) ** 2 for w in sent])) for sent in counters]\n",
    "    # return part_list\n",
    "    for i in range(D - 1):\n",
    "        for j in range(i + 1, D):\n",
    "            sent_1 = counters[i]\n",
    "            sent_2 = counters[j]\n",
    "            share_word_set = sent_1 & sent_2\n",
    "            numerator = sum([(sent_1[w] * sent_2[w] * (isf(w, D, counters) ** 2)) for w in share_word_set])\n",
    "            denominator = part_list[i] * part_list[j]\n",
    "            sim_matrix[i, j] = numerator / denominator\n",
    "    sim_matrix = sim_matrix + sim_matrix.T\n",
    "    g = nx.from_numpy_array(sim_matrix)\n",
    "    score = nx.pagerank(g)\n",
    "    temp = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "    idx = [item[0] for item in temp]\n",
    "    return [sents[i] for i in idx], [score[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triangles = find_triangle_with_node(single_sent_graph, 'Machine learning', 'Artificial neural network', 'Deep learning')\n",
    "test_triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = [triangle[1] for triangle in test_triangles]\n",
    "sents, score = do_pagerank(sent_list)\n",
    "list(zip(score, sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Generation Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = ['baseline1', 'baseline2', 'baseline3', 'baseline4', 'baseline5']\n",
    "data = {}\n",
    "for baseline in baselines:\n",
    "    with open('MyFiD/checkpoint/'+baseline+'_test/final_output.tsv') as f_in:\n",
    "        gens = []\n",
    "        tars = []\n",
    "        pairs = []\n",
    "        for line in f_in:\n",
    "            pair, gen, tar = line.strip().split('\\t')\n",
    "            gens.append(gen)\n",
    "            tars.append(tar)\n",
    "            pairs.append(pair)\n",
    "        data[baseline] = gens\n",
    "        data['target'] = tars\n",
    "        data['pair'] = pairs\n",
    "pd.DataFrame(data)[:100].to_csv('evaluation.csv', index=False, columns=['pair', 'target']+baselines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Super Sub-graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all sentences between two entities within one hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Single sentence graph\n",
    "single_sent_graph = my_read_pickle(single_sent_graph_file)\n",
    "edges = [edge for edge in tqdm.tqdm(single_sent_graph.edges) if single_sent_graph.get_edge_data(*edge)['score'] > 0.65]\n",
    "filtered_graph = single_sent_graph.edge_subgraph(edges)\n",
    "print('number of nodes:', filtered_graph.number_of_nodes())\n",
    "print('number of edges:', filtered_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = find_path_between_pair(single_sent_graph, 'Artificial intelligence', 'Natural language processing', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subgraph(paths:list, single_sent_graph:nx.Graph):\n",
    "    pairs = set()\n",
    "    triples = []\n",
    "    for path in paths:\n",
    "        if len(path) <= 2:\n",
    "            continue\n",
    "        for i in range(len(path)-1):\n",
    "            new_pair = frozenset((path[i], path[i+1]))\n",
    "            if new_pair not in pairs:\n",
    "                pairs.add(new_pair)\n",
    "                triples.append(list(new_pair) + [note2line(single_sent_graph.get_edge_data(path[i], path[i+1])['note']).strip()])\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = build_subgraph(paths, single_sent_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a graph for one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = my_read_pickle(entity_occur_from_cooccur_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all([a[i]['target'] ==b[i]['target'] for i in range(len(a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[i]['source'][a[i]['triple'][0][1]['sent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[i]['entity'][a[i]['triple'][0][0]['e2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[i]['entity'][b[i]['triple'][0][0]['e2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[i]['source'][b[i]['triple'][0][1]['sent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file = 'MyFiD/data/train.json'\n",
    "random_file = 'MyFiD/data/random_train.json'\n",
    "pair2sent = {}\n",
    "with open(original_file) as f_in:\n",
    "    original_samples = json.load(f_in)\n",
    "with open(random_file) as f_in:\n",
    "    random_samples = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 for sample in tqdm.tqdm(random_samples) if sample['target'] in sample['source']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[for t in range(len(original_samples[i][]))] \n",
    "count = 0\n",
    "overlap_pairs = set()\n",
    "for i in range(len(original_samples)):\n",
    "    random_sample = random_samples[i]\n",
    "    original_sample = original_samples[i]\n",
    "    for j in range(len(original_sample['triple'][0])):\n",
    "        if original_sample['source'][original_sample['triple'][0][j]['sent']] == random_sample['source'][random_sample['triple'][0][j]['sent']]:\n",
    "            count += 1\n",
    "            overlap_pairs.add(frozenset((original_sample['entity'][original_sample['triple'][0][j]['e1']], original_sample['entity'][original_sample['triple'][0][j]['e2']])))\n",
    "            break\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(overlap_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_occur = []\n",
    "for pair in overlap_pairs:\n",
    "    ent1, ent2 = pair\n",
    "    overlap_occur.append(len(d[ent1] & d[ent2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(overlap_occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(tqdm.tqdm(samples[6900:])):\n",
    "    entity = sample['entity']\n",
    "    stop = False\n",
    "    sent_list = []\n",
    "    for path in sample['triple']:\n",
    "        for tri in path:\n",
    "            ent1, ent2 = entity[tri['e1']], entity[tri['e2']]\n",
    "            pair = frozenset((ent1, ent2))\n",
    "            sents = pair2sent.get(pair)\n",
    "            if not sents:\n",
    "                sent_candidates = list(d[ent1] & d[ent2])\n",
    "                if len(sent_candidates) > 2:\n",
    "                    sents = sent_candidates[:2]\n",
    "                else:\n",
    "                    sents = sent_candidates\n",
    "                sents = [note2line(sent).strip() for sent in sents]\n",
    "                pair2sent[pair] = sents\n",
    "            try:\n",
    "                sent = sents[0] if sents[0] != sample['target'] else sents[1]\n",
    "            except:\n",
    "                print(i)\n",
    "                print(pair)\n",
    "                print(json.dumps(sample))\n",
    "                stop = True\n",
    "                break\n",
    "            # if sent not in sent_list:\n",
    "            #     sent_list.append(sent)\n",
    "            # tri['sent'] = sent_list.index(sent)\n",
    "        if stop:\n",
    "            break\n",
    "    if stop:\n",
    "        break\n",
    "    # sample['source'] = sent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Not necessary] Online operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    remove_list = ['See also', 'References', 'Further reading', 'Sources', 'External links']\n",
    "    dic = {sec : page.section(sec) for sec in page.sections}\n",
    "    dic['summary'] = page.summary\n",
    "    sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    while len(section_list) > 0:\n",
    "        section = section_list.pop()\n",
    "        if section in remove_list:\n",
    "            continue\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        # processed_text = clean_text(section_text)\n",
    "        processed_text = ' '.join(section_text.lower().split())\n",
    "        temp_sents = my_sentence_tokenize(processed_text, True)\n",
    "        sents += temp_sents\n",
    "    return list(sents)\n",
    "\n",
    "def collect_entity_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    return [text.lower() for text in page.links]\n",
    "\n",
    "def collect_keyword_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    soup = BeautifulSoup(page.html(), 'html.parser')\n",
    "    main_block = soup.find('div', class_='mw-parser-output')\n",
    "    keywords = set([l.text.lower() for l in main_block.findAll('a') if re.match(r'^(<a href=\"/wiki/)', str(l))])\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'python'\n",
    "\n",
    "p = wikipedia.page(keyword)\n",
    "if p is not None:\n",
    "    sents = collect_sents_from_wiki_page(p)\n",
    "    keywords = collect_keyword_from_wiki_page(p)\n",
    "    print('sentences collected')\n",
    "    my_write('%s.txt' % keyword, sents)\n",
    "    my_write('%s_kw.txt' % keyword, keywords)\n",
    "    df = filter_by_path(sents)\n",
    "    df.to_csv('%s_out.tsv' % keyword, sep='\\t', index=False)\n",
    "\n",
    "    dff = df[df.apply(lambda x: str(x['head']) in keywords and str(x['tail']) in keywords, axis=1)]\n",
    "    dff.to_csv('%s_out_f.tsv' % keyword, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = my_read_pickle(graph_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_sentence_graph = my_read_pickle('sentence_graph_significant_0.40.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_sentence_graph = my_read_pickle('sentence_graph_explicit_0.40.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_sentence_graph = my_read_pickle('sentence_graph_score_0.60.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scr_sentence_graph.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdn_sentence_graph = my_read_pickle('sentence_graph_random.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent1 = 'Python (programming language)'\n",
    "ent2 = 'Programming language'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sig_sentence_graph.get_edge_data(ent1, ent2)\n",
    "data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exp_sentence_graph.get_edge_data(ent1, ent2)\n",
    "data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MyFiD/data/test.json') as f_in:\n",
    "    test = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline1_output = open('MyFiD/checkpoint/baseline1_test/final_output.tsv').readlines()\n",
    "baseline3_output = open('MyFiD/checkpoint/baseline3_test/final_output.tsv').readlines()\n",
    "baseline5_output = open('MyFiD/checkpoint/baseline5_test/final_output.tsv').readlines()\n",
    "\n",
    "data = [{'pair' : 'ent1: %s ent2: %s' % tuple(item['pair']),\n",
    "         'rdn' : note2line(rdn_sentence_graph.get_edge_data(*item['pair'])['data'][0]['note']).strip(),\n",
    "         'rdn_score' : '',\n",
    "         'exp' : note2line(exp_sentence_graph.get_edge_data(*item['pair'])['data'][0]['note']).strip(), \n",
    "         'exp_score' : '',\n",
    "         'sig' : note2line(sig_sentence_graph.get_edge_data(*item['pair'])['data'][0]['note']).strip(), \n",
    "         'sig_score' : '',\n",
    "         'scr' : note2line(scr_sentence_graph.get_edge_data(*item['pair'])['data'][0]['note']).strip(),\n",
    "         'scr_score' : '',\n",
    "         'baseline1' : baseline1_output[i].strip().split('\\t')[1], \n",
    "         'baseline1_score' : '', \n",
    "         'baseline3' : baseline3_output[i].strip().split('\\t')[1], \n",
    "         'baseline3_score' : '', \n",
    "         'baseline5' : baseline5_output[i].strip().split('\\t')[1], \n",
    "         'baseline5_score' : '', \n",
    "         } for i, item in enumerate(tqdm.tqdm(test)) if len(graph.get_edge_data(*item['pair'])['data']) >= 10]\n",
    "pd.DataFrame(data[:100]).to_csv('evaluation_all.csv', index=False)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([(item['exp'] == item['scr']) and (item['sig'] == item['scr']) for item in data]) / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = ['baseline1', 'baseline3', 'baseline5']\n",
    "data = {}\n",
    "for baseline in baselines:\n",
    "    with open('MyFiD/checkpoint/'+baseline+'_test/final_output.tsv') as f_in:\n",
    "        gens = []\n",
    "        tars = []\n",
    "        pairs = []\n",
    "        for line in f_in:\n",
    "            pair, gen, tar = line.strip().split('\\t')\n",
    "            gens.append(gen)\n",
    "            tars.append(tar)\n",
    "            pairs.append(pair)\n",
    "        data[baseline] = gens\n",
    "        data['target'] = tars\n",
    "        data['pair'] = pairs\n",
    "pd.DataFrame(data)[:100].to_csv('evaluation.csv', index=False, columns=['pair', 'target']+baselines)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9496c91418be784f00ee6456e4343e8188c649322b68f201c83241a4029a42d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('FWD_py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
