{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences from Wikipedia\n",
    "+ This notebook is used for collecting sentences that tell relationship between two entities from wikipedia using some dependency path pattern\n",
    "+ **This notebook is fully valid under Owl3 machine (using the /scratch/data/wikipedia/full_text-2021-03-20 data)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki sub folder example: ../data/wikipedia/full_text-2021-03-20/BE\n",
      "save sub folder example: data/extract_wiki/wiki_sent_collect/BE\n",
      "wiki file example: ../data/wikipedia/full_text-2021-03-20/BE/wiki_00\n",
      "save sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.dat\n",
      "save cooccur file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_co.dat\n",
      "save selected sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_se.dat\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wikipedia\n",
    "import os\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "import wikipedia2vec\n",
    "from collections import Counter\n",
    "import bz2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from typing import List\n",
    "from nltk.corpus import stopwords\n",
    "self_define_stopwords = set(['-', ',', '.'])\n",
    "sw = set(stopwords.words('english'))\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "random.seed(0)\n",
    "import torch\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from tools.BasicUtils import my_read, my_write, my_read_pickle, my_write_pickle\n",
    "from tools.TextProcessing import (\n",
    "                my_sentence_tokenize,\n",
    "                my_sentence_tokenize, filter_specific_keywords, nlp, \n",
    "                exact_match, find_span\n",
    "                )\n",
    "\n",
    "from extract_wiki import (\n",
    "    save_path, entity_occur_from_cooccur_file, graph_file, single_sent_graph_file, \n",
    "    w2vec_dump_file, sub_path_pattern_count_file, \n",
    "    w2vec_keyword2idx_file, \n",
    "    test_path, path_test_file, \n",
    "    path_pattern_count_file, \n",
    "    save_sub_folders, wiki_sub_folders, \n",
    "    wiki_files, save_sent_files, save_cooccur_files, save_selected_files, \n",
    "    p, patterns, FeatureProcess, \n",
    "    note2line, cal_score_from_df, cal_freq_from_path, cal_freq_from_df, \n",
    "    gen_pattern, gen_kw_from_wiki_ent, get_entity_page, load_pattern_freq, find_triangles, find_path_between_pair, \n",
    "    generate_sample, generate_second_level_sample, sample_to_neo4j, get_sentence, informativeness_demo, find_dependency_info_from_tree, process_list\n",
    ")\n",
    "\n",
    "# Generate the save dir\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.mkdir(test_path)\n",
    "\n",
    "for save_dir in save_sub_folders:\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "# Get all files under wikipedia/full_text-2021-03-20\n",
    "\n",
    "print('wiki sub folder example:', wiki_sub_folders[0])\n",
    "print('save sub folder example:', save_sub_folders[0])\n",
    "print('wiki file example:', wiki_files[0])\n",
    "print('save sentence file example:', save_sent_files[0])\n",
    "print('save cooccur file example:', save_cooccur_files[0])\n",
    "print('save selected sentence file example:', save_selected_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] wikipedia2vec\n",
    "with bz2.open(w2vec_dump_file) as f_in:\n",
    "    w2vec = Wikipedia2Vec.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] wikipedia2vec\n",
    "\n",
    "# Find similar words or entities\n",
    "# ent1 = 'Python (programming language)'\n",
    "# w2vec.most_similar_by_vector(w2vec.get_entity_vector(ent1), 20)\n",
    "\n",
    "# Get similarity between two entities\n",
    "ent1 = 'Machine learning'\n",
    "ent2 = 'Information science'\n",
    "cosine_similarity(w2vec.get_entity_vector(ent1).reshape(1, -1), w2vec.get_entity_vector(ent2).reshape(1, -1))[0, 0]\n",
    "\n",
    "# Check the entity count and document count\n",
    "# ent1 = 'Hidden Markov model'\n",
    "# e = w2vec.get_entity(ent1)\n",
    "# print(e.count)\n",
    "# print(e.doc_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect sentences, entities, entities co-occurrances, titles from wikipedia dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roughly collect sentences, entity co-occurrances, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] Test get_sentence function\n",
    "get_sentence('ir_2.txt', 'sent.txt', 'cooccur.txt', 'title.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py collect_sent_and_cooccur (8 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct entity mapping in co-occurrance files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py correct_mapping_in_cooccur (6 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Late in the year, \"\"Oz\"\" published a feature called \"\"The Oz Guide to Sydney's Underworld\"\", which was based on information from two local journalists, and which included a \"\"top 20\"\" list of Sydney major criminals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test]\n",
    "lines = get_entity_page('Oz (magazine)')\n",
    "# sents = [note2line(note) for note in lines]\n",
    "# occurs = [note2line(note, '_co_.dat') for note in lines]\n",
    "# ori_occurs = [note2line(note, '_co.dat') for note in lines]\n",
    "# my_write('sent_check.txt', sents)\n",
    "# my_write('occur_check.txt', occurs)\n",
    "# my_write('ori_occur_check.txt', ori_occurs)\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate entity occurrance from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the entity occurrance dict from co-occurrance info [collect_ent_occur_from_cooccur]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Not necessary] Mapping keyword mention to wikipedia2vec entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_entity\n",
    "w2vec\n",
    "\n",
    "w2vec_keyword2idx = {}\n",
    "\n",
    "for entity in tqdm.tqdm(wikipedia_entity):\n",
    "    w2vec_entity = w2vec.get_entity(entity)\n",
    "    if w2vec_entity is None:\n",
    "        continue\n",
    "    kw = gen_kw_from_wiki_ent(entity)\n",
    "    if kw not in w2vec_keyword2idx:\n",
    "        w2vec_keyword2idx[kw] = [w2vec_entity.index]\n",
    "    else:\n",
    "        if w2vec_entity.index not in w2vec_keyword2idx[kw]:\n",
    "            w2vec_keyword2idx[kw].append(w2vec_entity.index)\n",
    "w2vec_kws = filter_specific_keywords(list(w2vec_keyword2idx.keys()))\n",
    "filter_keyword_from_w2vec = set(w2vec_kws)\n",
    "w2vec_keyword2idx = {k:v for k, v in w2vec_keyword2idx.items() if k in filter_keyword_from_w2vec}\n",
    "my_write_pickle(w2vec_keyword2idx_file, w2vec_keyword2idx)\n",
    "len(w2vec_keyword2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] w2vec_keyword2idx\n",
    "w2vec_keyword2idx = my_read_pickle(w2vec_keyword2idx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] w2vec_keyword2idx\n",
    "kw = 'feature engineering'\n",
    "kw_in_mention = kw in w2vec_keyword2idx\n",
    "print(kw_in_mention)\n",
    "if kw_in_mention:\n",
    "    for idx in w2vec_keyword2idx[kw]:\n",
    "        print(w2vec.dictionary.get_item_by_index(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect pattern frequency counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] [collect_pattern_freq] (12 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] cal_freq function\n",
    "c, log_max_cnt = load_pattern_freq(path_pattern_count_file)\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] cal_freq function\n",
    "cal_freq_from_path('i_nsubj prep pobj prep pobj prep pobj', c, log_max_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path_test_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# path = get_path(doc, kw1_steps, kw2_steps)\n",
    "# expand_dependency_info_from_tree(doc, branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test find dependency path\n",
    "data = test_df.iloc[2]\n",
    "doc = nlp(data['sent'])\n",
    "kw1_span = eval(data['kw1_span'])\n",
    "kw2_span = eval(data['kw2_span'])\n",
    "print(doc)\n",
    "kw1_steps, kw2_steps, branch = find_dependency_info_from_tree(doc, doc[kw1_span[0] : kw1_span[1]+1], doc[kw2_span[0] : kw2_span[1]+1])\n",
    "ans = get_path(doc, kw1_steps, kw2_steps)\n",
    "print(data['kw1'], data['kw1_span'])\n",
    "print(data['kw2'], data['kw2_span'])\n",
    "print(ans)\n",
    "print()\n",
    "ans = collect_sub_dependency_path(doc, branch)\n",
    "for item in ans:\n",
    "    print(doc[item[0]], '----', item[1], '----', doc[item[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test find sub dependency path\n",
    "data = test_df.iloc[1]\n",
    "doc = nlp(data['sent'])\n",
    "kw1_span = eval(data['kw1_span'])\n",
    "kw2_span = eval(data['kw2_span'])\n",
    "print(doc)\n",
    "kw1_steps, kw2_steps, branch = find_dependency_info_from_tree(doc, doc[kw1_span[0] : kw1_span[1]+1], doc[kw2_span[0] : kw2_span[1]+1])\n",
    "ans = collect_sub_dependency_path(doc, branch)\n",
    "for item in ans:\n",
    "    print(doc[item[0]], '----', item[1], '----', doc[item[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] collect dataset [collect_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Prepration] Sentence-edged Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the graph ['generate_graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate single sentence graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate single sentence graph ['generate_single_sent_graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Test] Check the score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of nodes: 1294502\n",
      "num of edges: 2503695\n"
     ]
    }
   ],
   "source": [
    "# [Load] Graph\n",
    "graph:nx.Graph = my_read_pickle(graph_file)\n",
    "\n",
    "print('num of nodes:', len(graph.nodes))\n",
    "print('num of edges:', len(graph.edges))\n",
    "\n",
    "single_sent_graph:nx.Graph = my_read_pickle(single_sent_graph_file)\n",
    "print('num of nodes:', len(single_sent_graph.nodes))\n",
    "print('num of edges:', len(single_sent_graph.edges))\n",
    "\n",
    "c, log_max_cnt = load_pattern_freq(path_pattern_count_file)\n",
    "c.most_common(10)\n",
    "d = my_read_pickle(entity_occur_from_cooccur_file)\n",
    "\n",
    "fp = FeatureProcess(sub_path_pattern_count_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] graph\n",
    "ent1 = 'Machine learning'\n",
    "# Check the neighbours of an entity\n",
    "list(graph.neighbors(ent1))\n",
    "\n",
    "# Check the edges of two entities\n",
    "# edges = graph.edges[ent1, 'Hinge loss']\n",
    "# edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect highest scored sentence for pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "edges = list(single_sent_graph.edges)\n",
    "random.Random(0).shuffle(edges)\n",
    "for edge in edges[:100]:\n",
    "    data = single_sent_graph.get_edge_data(*edge)\n",
    "    sents.append({'ent1' : edge[0], \n",
    "                #   'kw1_span' : data['span'][0], \n",
    "                  'ent2' : edge[1], \n",
    "                #   'kw2_span' : data['span'][1], \n",
    "                  'sent' : note2line(data['note']).strip(), \n",
    "                  'score' : data['score']})\n",
    "pd.DataFrame(sents).to_csv('highest_sents.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single sentence significance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = informativeness_demo(\"Many operating systems let a program return a result when its process terminates;\", 'Operating system', 'Process', fp)\n",
    "df.to_csv('temp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'kw1_span': (1, 2),\n",
       "  'kw2_span': (11, 11),\n",
       "  'pattern': 'i_nsubj ccomp advcl nsubj',\n",
       "  'dep_path': 'i_nsubj ccomp advcl nsubj',\n",
       "  'dep_coverage': 0.8732452507251984}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.feature_process(nlp(\"Many operating systems let a program return a result when its process terminates;\"), 'Operating system', 'Process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14793678156435072"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cal_score(pattern_freq:float, dep_coverage:float):\n",
    "    return 2 / ((1/pattern_freq)+(1/dep_coverage))\n",
    "cal_score(0.8732452507251984, 0.08081373200135519)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check score function on sentences from a pair of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent1 = 'Python (programming language)'\n",
    "ent2 = 'Java (programming language)'\n",
    "sents = [note2line(note).strip() for note in d[ent1] & d[ent2]]\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 385/385 [00:05<00:00, 67.84it/s]\n"
     ]
    }
   ],
   "source": [
    "b = cal_freq_from_df(pd.DataFrame(process_list(sents, [str((0, ent1, ent2))]*len(sents), fp.batched_feature_process)), c, log_max_cnt)\n",
    "b = cal_score_from_df(b)\n",
    "b = b.sort_values(by=['score'], ascending=False)\n",
    "b.to_csv('sentences.csv', index=False, columns=['kw1', 'kw1_span', 'kw2', 'kw2_span', 'sent', 'dep_coverage', 'pattern_freq', 'pattern', 'score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check score function on sentences from pairs containing one entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232/232 [00:23<00:00,  9.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = []\n",
    "ent = 'Machine learning'\n",
    "# for edge in graph.edges:\n",
    "for neighbor in tqdm.tqdm(list(graph.neighbors(ent))):\n",
    "    # if edge[0] not in d or edge[1] not in d:\n",
    "    #     continue\n",
    "    # s = d[edge[0]] & d[edge[1]]\n",
    "    if neighbor not in d:\n",
    "        continue\n",
    "    s = d[ent] & d[neighbor]\n",
    "    s = list(s)\n",
    "    sents = [note2line(note).strip() for note in s]\n",
    "    # pairs = [{'kw1' : gen_kw_from_wiki_ent(edge[0]), 'kw2' : gen_kw_from_wiki_ent(edge[1])}]\n",
    "    pairs = [{'kw1' : gen_kw_from_wiki_ent(ent, False), 'kw2' : gen_kw_from_wiki_ent(neighbor, False)}]\n",
    "    temp_list = []\n",
    "    for idx, sent in enumerate(sents):\n",
    "        res = fp.batched_feature_process(sent, pairs)\n",
    "        for i in res:\n",
    "            i['sent'] = sent\n",
    "            i['note'] = s[idx]\n",
    "        temp_list.extend(res)\n",
    "    if not temp_list:\n",
    "        continue\n",
    "    df = pd.DataFrame(temp_list)\n",
    "    df = cal_freq_from_df(df, c, log_max_cnt)\n",
    "    df = cal_score_from_df(df)\n",
    "    # if len(df) >= 5:\n",
    "    #     df = df[:5]\n",
    "    df = df.sort_values(by=['score'], ascending=False)\n",
    "    examples.append(df)\n",
    "        # if len(examples) >= 10:\n",
    "        #     break\n",
    "\n",
    "test_df = pd.concat(examples)\n",
    "test_df.to_csv(ent + '.csv', index=False, columns=['kw1', 'kw1_span', 'kw2', 'kw2_span', 'note', 'sent', 'dep_coverage', 'pattern_freq', 'pattern', 'score'])\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples = []\n",
    "for i in range(len(test_df)):\n",
    "    doc = nlp(test_df.loc[i, 'sent'])\n",
    "    kw1_span = eval(test_df.loc[i, 'kw1_span'])\n",
    "    kw2_span = eval(test_df.loc[i, 'kw2_span'])\n",
    "    test_df.loc[i, 'dep_coverage'] = find_dependency_info_from_tree(doc, doc[kw1_span[0]:kw1_span[1]+1], doc[kw2_span[0]:kw2_span[1]+1]).mean()\n",
    "for kw1, kw2 in set(zip(test_df['kw1'].tolist(), test_df['kw2'].tolist())):\n",
    "    temp_df:pd.DataFrame = test_df[(test_df['kw1'] == kw1) & (test_df['kw2'] == kw2)]\n",
    "    temp_df = cal_freq_from_df(temp_df, c, log_max_cnt)\n",
    "    temp_df = cal_score_from_df(temp_df)\n",
    "    temp_df = temp_df.sort_values(by=['score'], ascending=False)\n",
    "    new_examples.append(temp_df)\n",
    "new_test_df = pd.concat(new_examples)\n",
    "new_test_df.to_csv('new_temp.csv', index=False, columns=['kw1', 'kw1_span', 'kw2', 'kw2_span', 'note', 'sent', 'dep_coverage', 'pattern_freq', 'pattern', 'score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect sentences from a pair for score function demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [02:36<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "105\n",
      "54\n",
      "81\n",
      "58\n",
      "76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "h_pattern_h_cov_check = 0\n",
    "h_pattern_l_cov_check = 0\n",
    "l_pattern_h_cov_check = 0\n",
    "l_pattern_l_cov_check = 0\n",
    "sent_adequate = 0\n",
    "ent = 'Operating system'\n",
    "for n in tqdm.tqdm(list(single_sent_graph.neighbors(ent))):\n",
    "# for edge in tqdm.tqdm(list(single_sent_graph.edges)):\n",
    "#     ent = edge[0]\n",
    "#     n = edge[1]\n",
    "    if n not in d:\n",
    "        continue\n",
    "    if n == ent:\n",
    "        continue\n",
    "    s = d[ent] & d[n]\n",
    "    if len(s) >= 4:\n",
    "        s = list(s)\n",
    "        sents = [note2line(note).strip() for note in s]\n",
    "        pairs = [{'kw1' : gen_kw_from_wiki_ent(ent), 'kw2' : gen_kw_from_wiki_ent(n)}]\n",
    "        temp_list = []\n",
    "        for idx, sent in enumerate(sents):\n",
    "            res = fp.batched_feature_process(sent, pairs)\n",
    "            for i in res:\n",
    "                i['sent'] = sent\n",
    "                i['note'] = s[idx]\n",
    "            temp_list.extend(res)\n",
    "        if not temp_list:\n",
    "            continue\n",
    "        df = pd.DataFrame(temp_list)\n",
    "        if len(df) >= 4:\n",
    "            sent_adequate += 1\n",
    "        else:\n",
    "            continue\n",
    "        df = cal_freq_from_df(df, c, log_max_cnt)\n",
    "        df = cal_score_from_df(df)\n",
    "        \n",
    "        h_pattern_h_cov = -1\n",
    "        h_pattern_l_cov = -1\n",
    "        l_pattern_h_cov = -1\n",
    "        l_pattern_l_cov = -1\n",
    "        for idx in range(len(df)):\n",
    "            if h_pattern_h_cov < 0 and df.iloc[idx]['dep_coverage'] > 0.8 and  df.iloc[idx]['pattern_freq'] > 0.8:\n",
    "                h_pattern_h_cov = idx\n",
    "            elif h_pattern_l_cov < 0 and df.iloc[idx]['dep_coverage'] < 0.6 and  df.iloc[idx]['pattern_freq'] > 0.8:\n",
    "                h_pattern_l_cov = idx\n",
    "            elif l_pattern_h_cov < 0 and df.iloc[idx]['dep_coverage'] > 0.8 and  df.iloc[idx]['pattern_freq'] < 0.6:\n",
    "                l_pattern_h_cov = idx\n",
    "            elif l_pattern_l_cov < 0 and df.iloc[idx]['dep_coverage'] < 0.6 and  df.iloc[idx]['pattern_freq'] < 0.6:\n",
    "                l_pattern_l_cov = idx\n",
    "        if h_pattern_h_cov >= 0 and h_pattern_l_cov >= 0 and l_pattern_h_cov >= 0 and l_pattern_l_cov >= 0:\n",
    "            examples.append((df, h_pattern_h_cov, h_pattern_l_cov, l_pattern_h_cov, l_pattern_l_cov))\n",
    "        h_pattern_h_cov_check += (h_pattern_h_cov >= 0)\n",
    "        h_pattern_l_cov_check += (h_pattern_l_cov >= 0)\n",
    "        l_pattern_h_cov_check += (l_pattern_h_cov >= 0)\n",
    "        l_pattern_l_cov_check += (l_pattern_l_cov >= 0)\n",
    "\n",
    "print(len(examples))\n",
    "print(sent_adequate)\n",
    "print(h_pattern_h_cov_check)\n",
    "print(h_pattern_l_cov_check)\n",
    "print(l_pattern_h_cov_check)\n",
    "print(l_pattern_l_cov_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 4, 5, 3)\n",
      "operating system\n",
      "process\n"
     ]
    }
   ],
   "source": [
    "example = examples[0]\n",
    "df = example[0]\n",
    "print(example[1:])\n",
    "print(df.iloc[0]['kw1'])\n",
    "print(df.iloc[0]['kw2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('temp.csv', columns=['kw1', 'kw1_span', 'kw2', 'kw2_span', 'sent', 'pattern', 'pattern_freq', 'dep_coverage', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pattern = set(df[df['pattern_freq'] > 0.8]['sent'].tolist())\n",
    "l_pattern = set(df[df['pattern_freq'] < 0.5]['sent'].tolist())\n",
    "h_cov = set(df[df['dep_coverage'] > 0.8]['sent'].tolist())\n",
    "l_cov = set(df[df['dep_coverage'] < 0.6]['sent'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'As a rule of thumb, operating system based on a system call interface can be made more efficient than those requiring messages to be exchanged between distinct processes.',\n",
       " 'For security and reliability, most modern operating systems prevent direct inter-process communication between independent processes, providing strictly mediated and controlled inter-process communication functionality.',\n",
       " 'Operating systems need some ways to create processes.',\n",
       " 'The operating system must be able to set a CPU affinity for both processes and interrupts.',\n",
       " 'When an operating system is booted, typically several processes are created.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_pattern & h_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df[df['pattern_freq']<0.5]\n",
    "# temp_df[temp_df['pattern_freq']>0.07]\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sentence from file\n",
    "note2line('AW:87:3103')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [collect_score_function_eval_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test score function with human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, log_max_cnt = load_pattern_freq(path_pattern_count_file)\n",
    "\n",
    "pattern_freq_w = 0.55\n",
    "kw_recall_w = 0.25\n",
    "coverage_w = 0.2\n",
    "\n",
    "def get_score(sent:str, ent1:str, ent2:str):\n",
    "    kw1 = gen_kw_from_wiki_ent(ent1)\n",
    "    kw2 = gen_kw_from_wiki_ent(ent2)\n",
    "    data = fp.batched_feature_process(nlp(sent), [{'kw1' : kw1, 'kw2' : kw2}])\n",
    "    if not data:\n",
    "        return -1\n",
    "    data = data[0]\n",
    "    pattern_freq = cal_freq_from_path(gen_pattern(data['dep_path']), c, log_max_cnt)\n",
    "    return ((pattern_freq)**pattern_freq_w) * (((data['kw1_recall'] + data['kw2_recall']) / 2)**kw_recall_w) * (((data['dep_coverage'] + data['surface_coverage']) / 2)**coverage_w)\n",
    "\n",
    "test_data = pd.read_csv('test.tsv', sep='\\t')\n",
    "score_function_result = test_data.copy()\n",
    "score_function_result['score'] = score_function_result.apply(lambda x: get_score(x['sentence'], x['entity 1'], x['entity 2']), axis=1)\n",
    "score_function_result.to_csv('score_function_result.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('score_function_result.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    sf_score = [float(lines[i].strip().split('\\t')[-1]) for i in range(1, len(lines))]\n",
    "    sf_score = np.array(sf_score)\n",
    "\n",
    "with open('user_label.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    user_score = [float(lines[i].strip().split('\\t')[-1]) / 5 for i in range(1, len(lines))]\n",
    "    user_score = np.array(user_score)\n",
    "    \n",
    "with open('user_label.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    data = []\n",
    "    for i in range(1, len(lines)):\n",
    "        ent1, ent2, sent, user_score_ = lines[i].strip().split('\\t')\n",
    "        data.append({'entity 1' : ent1, 'entity 2' : ent2, 'sentence' : sent, 'user label' : float(user_score_)/5})\n",
    "        \n",
    "sf_score = sf_score[user_score > 0]\n",
    "user_score = user_score[user_score > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(sf_score, user_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score range within [1,2,3,4,5]\n",
    "l2 = np.mean(np.abs(sf_score*5 - user_score*5))\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(user_score*5)\n",
    "np.mean(user_score[sf_score>0.7])\n",
    "np.mean(user_score[sf_score<=0.6])\n",
    "np.mean(sf_score*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.abs(sf_score - user_score)\n",
    "diff = []\n",
    "for i in range(len(dist)):\n",
    "    if dist[i] > 0.3:\n",
    "        diff.append(data[i].copy())\n",
    "        diff[-1]['score function label'] = sf_score[i]\n",
    "diff = pd.DataFrame(diff)\n",
    "len(diff)\n",
    "diff.to_csv('diff.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 1294502\n",
      "number of edges: 2503695\n"
     ]
    }
   ],
   "source": [
    "# [Load] Single sentence graph\n",
    "single_sent_graph = my_read_pickle(single_sent_graph_file)\n",
    "print('number of nodes:', single_sent_graph.number_of_nodes())\n",
    "print('number of edges:', single_sent_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data of level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "ent1 = 'Machine learning'\n",
    "ent2 = 'Algorithm'\n",
    "sample = generate_sample(single_sent_graph, ent1, ent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pair': ('Machine learning', 'Algorithm'),\n",
       " 'entity': ['Data set',\n",
       "  'Yann LeCun',\n",
       "  'Algorithm',\n",
       "  'Logistic regression',\n",
       "  'Machine learning',\n",
       "  'Genetic algorithm',\n",
       "  'Artificial intelligence'],\n",
       " 'target': 'Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.',\n",
       " 'source': [\"Based on Fisher's linear discriminant model, this data set became a typical test case for many statistical classification techniques in machine learning such as support vector machines.\",\n",
       "  'Yann LeCun, inventor of the Convolutional Neural Network architecture, proposed the modern form of the back-propagation learning algorithm for neural networks in his PhD thesis in 1987.',\n",
       "  'Logistic regression and other log-linear models are also commonly used in machine learning.',\n",
       "  'As a scientific endeavor, machine learning grew out of the quest for artificial intelligence.',\n",
       "  'These algorithms are mainly used by artificial intelligence and intelligent information processing programs .',\n",
       "  'If the model is correct then the algorithm has exactly twice the asymptotic variance of logistic regression on the full data set.',\n",
       "  'The algorithm is simpler than a standard genetic algorithm, and in many cases leads to better results than a standard genetic algorithm.',\n",
       "  'In machine learning, genetic algorithms were used in the 1980s and 1990s.',\n",
       "  'An algorithm that is designed for one kind of model will generally fail on a data set that contains a radically different kind of model.',\n",
       "  'In a 2016 seminar, Yann LeCun described GANs as \"the coolest idea in machine learning in the last twenty years\".'],\n",
       " 'triple': [[{'e1': 4,\n",
       "    'e2': 1,\n",
       "    'pid': 0,\n",
       "    'score': 0.8830289569220175,\n",
       "    'sim': 0.6495980024337769,\n",
       "    'span': ('(15, 16)', '(5, 6)'),\n",
       "    'significant': 0.9191965487486453,\n",
       "    'explict': 0.8495997862752968,\n",
       "    'sent': 9},\n",
       "   {'e1': 1,\n",
       "    'e2': 2,\n",
       "    'pid': 0,\n",
       "    'score': 0.8540217541993205,\n",
       "    'sim': 0.5279337167739868,\n",
       "    'span': ('(21, 21)', '(0, 1)'),\n",
       "    'significant': 0.8354966995164663,\n",
       "    'explict': 0.8733869302087174,\n",
       "    'sent': 1}],\n",
       "  [{'e1': 4,\n",
       "    'e2': 0,\n",
       "    'pid': 1,\n",
       "    'score': 0.8546693884392919,\n",
       "    'sim': 0.5236525535583496,\n",
       "    'span': ('(9, 10)', '(22, 23)'),\n",
       "    'significant': 0.8322694402396944,\n",
       "    'explict': 0.8783084441352851,\n",
       "    'sent': 0},\n",
       "   {'e1': 0,\n",
       "    'e2': 2,\n",
       "    'pid': 1,\n",
       "    'score': 0.7704487847948678,\n",
       "    'sim': 0.5157889723777771,\n",
       "    'span': ('(1, 1)', '(15, 16)'),\n",
       "    'significant': 0.6287292004452418,\n",
       "    'explict': 0.9946490427264604,\n",
       "    'sent': 8}],\n",
       "  [{'e1': 4,\n",
       "    'e2': 5,\n",
       "    'pid': 2,\n",
       "    'score': 0.8937685011505752,\n",
       "    'sim': 0.7588196992874146,\n",
       "    'span': ('(1, 2)', '(4, 5)'),\n",
       "    'significant': 0.936056085640559,\n",
       "    'explict': 0.8551365641730013,\n",
       "    'sent': 7},\n",
       "   {'e1': 5,\n",
       "    'e2': 2,\n",
       "    'pid': 2,\n",
       "    'score': 0.7148994612457384,\n",
       "    'sim': 0.7263085246086121,\n",
       "    'span': ('(1, 1)', '(21, 22)'),\n",
       "    'significant': 0.6170663017375076,\n",
       "    'explict': 0.8495997862752968,\n",
       "    'sent': 6}],\n",
       "  [{'e1': 4,\n",
       "    'e2': 3,\n",
       "    'pid': 3,\n",
       "    'score': 0.7105607599484941,\n",
       "    'sim': 0.5849477648735046,\n",
       "    'span': ('(0, 1)', '(13, 14)'),\n",
       "    'significant': 0.6078013097632355,\n",
       "    'explict': 0.8551365641730013,\n",
       "    'sent': 2},\n",
       "   {'e1': 3,\n",
       "    'e2': 2,\n",
       "    'pid': 3,\n",
       "    'score': 0.7873754583858749,\n",
       "    'sim': 0.5682052969932556,\n",
       "    'span': ('(7, 7)', '(15, 16)'),\n",
       "    'significant': 0.8038380202912042,\n",
       "    'explict': 0.7715736687641906,\n",
       "    'sent': 5}],\n",
       "  [{'e1': 4,\n",
       "    'e2': 6,\n",
       "    'pid': 4,\n",
       "    'score': 0.9055928741487419,\n",
       "    'sim': 0.7181744575500488,\n",
       "    'span': ('(5, 6)', '(13, 14)'),\n",
       "    'significant': 0.9694871702552252,\n",
       "    'explict': 0.8495997862752968,\n",
       "    'sent': 3},\n",
       "   {'e1': 6,\n",
       "    'e2': 2,\n",
       "    'pid': 4,\n",
       "    'score': 0.6176430549210138,\n",
       "    'sim': 0.5581156611442566,\n",
       "    'span': ('(1, 1)', '(6, 7)'),\n",
       "    'significant': 0.5888677426113273,\n",
       "    'explict': 0.6493750884921322,\n",
       "    'sent': 4}]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "triple:list = sample['triple']\n",
    "sources:List[str] = sample['source']\n",
    "entity:List[str] = sample['entity']\n",
    "avg_scores = [mean([tri['score'] for tri in path]) for path in triple]\n",
    "sorted_list = sorted(zip(avg_scores, triple), key=lambda x: x[0], reverse=True)\n",
    "triple = list(zip(*sorted_list))[1]\n",
    "contexts = [[{'e1' : entity[tri['e1']], \n",
    "            'e2' : entity[tri['e2']], \n",
    "            'sent' : sources[tri['sent']],\n",
    "            'score' : tri['score']} for tri in path] for path in triple]\n",
    "ctxs = []\n",
    "for ctx in contexts[:5]:\n",
    "    path = [ctx[0]['e1']]\n",
    "    sents = []\n",
    "    for i, tri in enumerate(ctx):\n",
    "        path.append(tri['e2'])\n",
    "        sents.append('sentence%d: %s' % (i+1, tri['sent']))\n",
    "    path = '; '.join(path)\n",
    "    sents = ' '.join(sents)\n",
    "    ctxs.append('%s %s' % ('path: ' + path, sents))\n",
    "for ctx in ctxs:\n",
    "    print(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data of level 1 [collect_sample_from_single_sent_graph] (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_level_1.json') as f_in:\n",
    "    a = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15174993473320503"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [len(set([tri['sent'] for tri in path])) < len(path) for item in a for path in item['triple']]\n",
    "sum(b)/len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38365605326341107"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [any([len(set([tri['sent'] for tri in path])) < len(path) for path in item['triple']]) for item in a]\n",
    "sum(b)/len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(item['triple']) for item in a])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (n) DETACH DELETE (n);\n",
      "CREATE (:ENT:TARGET {ent:\"Ealdred (archbishop of York)\"});\n",
      "CREATE (:ENT:TARGET {ent:\"Cathedral chapter\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Archdeacon\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Abbot\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Archbishop of Cologne\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Archbishop of York\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Canon (priest)\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Bishop\"});\n",
      "MATCH (ent1:ENT {ent:\"Bishop\"}), (ent2:ENT {ent:\"Cathedral chapter\"}) CREATE (ent1)-[:Sent {sent:\"The cathedral chapter of Worms Cathedral elected him as Bishop of Worms on 16 April 1673.\", pair:\"Bishop <-> Cathedral chapter\", score:0.965}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Ealdred (archbishop of York)\"}), (ent2:ENT {ent:\"Canon (priest)\"}) CREATE (ent1)-[:Sent {sent:\"In 1066 Ealdred, the Archbishop of York, held the Lordship, this transferring by 1086 to the canons of Beverley, with Thomas of Bayeux, the later Archbishop of York, as Tenant-in-chief to King William I.\", pair:\"Ealdred (archbishop of York) <-> Canon (priest)\", score:0.669}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Ealdred (archbishop of York)\"}), (ent2:ENT {ent:\"Archbishop of York\"}) CREATE (ent1)-[:Sent {sent:\"Ealdred was Abbot of Tavistock, Bishop of Worcester, and Archbishop of York in History of Anglo-Saxon England.\", pair:\"Ealdred (archbishop of York) <-> Archbishop of York\", score:0.912}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Archbishop of York\"}), (ent2:ENT {ent:\"Bishop\"}) CREATE (ent1)-[:Sent {sent:\"In both the Church of England and the Church of Ireland, two bishops have the title of primate: the archbishops of Archbishop of Canterbury and Archbishop of York in England and of Archbishop of Armagh and Archbishop of Dublin in Ireland.\", pair:\"Archbishop of York <-> Bishop\", score:0.791}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Archbishop of Cologne\"}), (ent2:ENT {ent:\"Cathedral chapter\"}) CREATE (ent1)-[:Sent {sent:\"On 19 November 1562, the cathedral chapter elected Friedrich as the new Archbishop of Cologne.\", pair:\"Archbishop of Cologne <-> Cathedral chapter\", score:0.962}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Ealdred (archbishop of York)\"}), (ent2:ENT {ent:\"Abbot\"}) CREATE (ent1)-[:Sent {sent:\"Among its famous abbots was Ealdred, who crowned Harold II and William I, and died Archbishop of York.\", pair:\"Ealdred (archbishop of York) <-> Abbot\", score:0.765}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Abbot\"}), (ent2:ENT {ent:\"Bishop\"}) CREATE (ent1)-[:Sent {sent:\"Thus at the first Council of Constantinople, AD 448, 23 archimandrites or abbots sign, with 30 bishops.\", pair:\"Abbot <-> Bishop\", score:0.828}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Archbishop of York\"}), (ent2:ENT {ent:\"Archdeacon\"}) CREATE (ent1)-[:Sent {sent:\"By way of compensation for this loss, Thurstan, Archbishop of York, conferred upon the Archdeacon all the privileges and prerogatives of a bishop, with the exception that he could not ordain, consecrate, or confirm.\", pair:\"Archbishop of York <-> Archdeacon\", score:0.793}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Archdeacon\"}), (ent2:ENT {ent:\"Cathedral chapter\"}) CREATE (ent1)-[:Sent {sent:\"At Skara the cathedral chapter consisted latterly of a dean, an archdeacon, a subdean, and twenty-one canons.\", pair:\"Archdeacon <-> Cathedral chapter\", score:0.790}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Ealdred (archbishop of York)\"}), (ent2:ENT {ent:\"Archbishop of Cologne\"}) CREATE (ent1)-[:Sent {sent:\"In this mission Ealdred was somewhat successful and obtained insight into the working of the German church during a stay of a year with HermannII, the Archbishop of Cologne.\", pair:\"Ealdred (archbishop of York) <-> Archbishop of Cologne\", score:0.765}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Canon (priest)\"}), (ent2:ENT {ent:\"Cathedral chapter\"}) CREATE (ent1)-[:Sent {sent:\"At Skara the cathedral chapter consisted latterly of a dean, an archdeacon, a subdean, and twenty-one canons.\", pair:\"Canon (priest) <-> Cathedral chapter\", score:0.967}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Ealdred (archbishop of York)\"}), (ent2:ENT {ent:\"Cathedral chapter\"}) CREATE (ent1)-[:OUT {sent:\"Ealdred was a monk in the cathedral chapter at Winchester Cathedral before becoming abbot of Tavistock Abbey about 1027, an office he held until about 1043.\", pair:\"Ealdred (archbishop of York) <-> Cathedral chapter\"}]->(ent2);\n"
     ]
    }
   ],
   "source": [
    "sample_to_neo4j(a[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "random.shuffle(a)\n",
    "train_ratio = 0.96\n",
    "valid_ratio = 0.02\n",
    "training_data = a[:int(len(a)*train_ratio)]\n",
    "valid_data = a[int(len(a)*train_ratio):int(len(a)*(train_ratio+valid_ratio))]\n",
    "test_data = a[int(len(a)*(train_ratio+valid_ratio)):]\n",
    "with open('train.json', 'w') as f_out:\n",
    "    json.dump(training_data, f_out)\n",
    "with open('dev.json', 'w') as f_out:\n",
    "    json.dump(valid_data, f_out)\n",
    "with open('test.json', 'w') as f_out:\n",
    "    json.dump(test_data, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nx.Graph()\n",
    "a.add_edge(1,2,a=1)\n",
    "a.add_edge(2,3,a=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_edge_data(1,3) == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data for level 1 with random path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_occur_from_cooccur = my_read_pickle(entity_occur_from_cooccur_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_in, file_out in [('MyFiD/data/train.json', 'train_random.json'), ('MyFiD/data/dev.json', 'dev_random.json'), ('MyFiD/data/test.json', 'test_random.json')]:\n",
    "    with open(file_in) as f_in:\n",
    "        data = json.load(f_in)\n",
    "        for item in tqdm.tqdm(data):\n",
    "            for path in item['triple']:\n",
    "                for tri in path:\n",
    "                    item['source'][tri['sent']] = note2line(random.choice(list(entity_occur_from_cooccur[item['entity'][tri['e1']]] & entity_occur_from_cooccur[item['entity'][tri['e2']]]))).strip()\n",
    "        with open(file_out, 'w') as f_out:\n",
    "            json.dump(data, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len_count = []\n",
    "for data in training_data:\n",
    "    sents = data['source']\n",
    "    sent_len_count.extend([len(sent.split()) for sent in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sent_len_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x[x<100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent_len_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_neo4j(items[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data of level 2 from level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.json') as f_in:\n",
    "    sample = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_sample = generate_second_level_sample(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph4nlp.pytorch.data import GraphData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_labels = list(nlp.get_pipe(\"parser\").labels)\n",
    "dep_labels.extend(['i_'+dep for dep in dep_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GraphData()\n",
    "is_rel = []\n",
    "is_entity = []\n",
    "\n",
    "g.add_nodes(1)\n",
    "is_rel.append(0)\n",
    "is_entity.append(0)\n",
    "\n",
    "for src in second_level_sample['sources']:\n",
    "    pair = src['pair']\n",
    "    sent_tokens = src['sent']\n",
    "    \n",
    "    label_list = []\n",
    "    label_list.extend(sent_tokens)\n",
    "    token_num = len(sent_tokens)\n",
    "    start_node = g.get_node_num()\n",
    "    g.add_nodes(token_num)\n",
    "    is_rel.extend([0]*token_num)\n",
    "    is_entity.extend([0]*token_num)\n",
    "    is_entity[pair[0]+start_node] = 1\n",
    "    is_entity[pair[1]+start_node] = 1\n",
    "    \n",
    "    label_list.extend(['ROOT', 'ROOT', 'i_ROOT', 'i_ROOT'])\n",
    "    rel_start_node = start_node + token_num\n",
    "    g.add_nodes(4)\n",
    "    is_rel.extend([1]*4)\n",
    "    is_entity.extend([0]*4)\n",
    "    g.add_edges([0, 0], [rel_start_node, rel_start_node+1])\n",
    "    g.add_edges([rel_start_node, rel_start_node+1], [pair[0]+start_node, pair[1]+start_node])\n",
    "    g.add_edges([pair[0]+start_node, pair[1]+start_node], [rel_start_node+2, rel_start_node+3])\n",
    "    g.add_edges([rel_start_node+2, rel_start_node+3], [0, 0])\n",
    "    \n",
    "    rel_start_node += 4\n",
    "    triples = src['graph']\n",
    "    rel_num = len(triples)\n",
    "    is_rel.extend([1]*rel_num)\n",
    "    is_entity.extend([0]*rel_num)\n",
    "    g.add_nodes(rel_num)\n",
    "    for rel_idx, (tok_1, tok_2, rel) in enumerate(triples):\n",
    "        g.add_edges([tok_1+start_node, rel_idx+rel_start_node], [rel_idx+rel_start_node, tok_2+start_node])\n",
    "        label_list.append(rel)\n",
    "    for i, label in enumerate(label_list):\n",
    "        g.node_attributes[i+start_node]['label'] = label\n",
    "g.node_features['is_rel'] = torch.BoolTensor(is_rel)\n",
    "g.node_features['is_entity'] = torch.BoolTensor(is_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.get_edge_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_triangle_with_node(graph:nx.Graph, first_node:str, second_node:str='', third_node:str=''):\n",
    "    triangles = list(find_triangles(graph, first_node))\n",
    "    triangles.sort(key=lambda x: x[1])\n",
    "    triangle_with_sents = []\n",
    "    n_seen = set()\n",
    "    for n1, n2, n3 in triangles:\n",
    "        if second_node and n2 != second_node and n3 != second_node:\n",
    "            continue\n",
    "        if third_node and n2 != third_node and n3 != third_node:\n",
    "            continue\n",
    "        if n2 not in n_seen:\n",
    "            n_seen.add(n2)\n",
    "            triangle_with_sents.append((n1, note2line(graph.get_edge_data(n1, n2)['note']).strip(), n2, graph.get_edge_data(n1, n2)['score']))\n",
    "        if n3 not in n_seen:\n",
    "            n_seen.add(n3)\n",
    "            triangle_with_sents.append((n1, note2line(graph.get_edge_data(n1, n3)['note']).strip(), n3, graph.get_edge_data(n1, n3)['score']))\n",
    "        triangle_with_sents.append((n2, note2line(graph.get_edge_data(n3, n2)['note']).strip(), n3, graph.get_edge_data(n3, n2)['score']))\n",
    "    return triangle_with_sents\n",
    "\n",
    "\n",
    "def isf(w:str, D:int, counters:List[Counter]):\n",
    "    return math.log(D * 1.0 / sum([1 if w in sent else 0 for sent in counters]))\n",
    "\n",
    "\n",
    "def do_pagerank(sents:List[str]):\n",
    "    # Remove stop words\n",
    "    clean_sents = [[token for token in sent.split() if token not in sw and token not in self_define_stopwords] for sent in sents]\n",
    "\n",
    "    # Generate word counters\n",
    "    counters = [Counter(sent) for sent in clean_sents]\n",
    "\n",
    "    # Build similarity matrix\n",
    "    D = len(clean_sents)\n",
    "    sim_matrix = np.zeros((D, D))\n",
    "    part_list = [math.sqrt(sum([(sent[w] * isf(w, D, counters)) ** 2 for w in sent])) for sent in counters]\n",
    "    # return part_list\n",
    "    for i in range(D - 1):\n",
    "        for j in range(i + 1, D):\n",
    "            sent_1 = counters[i]\n",
    "            sent_2 = counters[j]\n",
    "            share_word_set = sent_1 & sent_2\n",
    "            numerator = sum([(sent_1[w] * sent_2[w] * (isf(w, D, counters) ** 2)) for w in share_word_set])\n",
    "            denominator = part_list[i] * part_list[j]\n",
    "            sim_matrix[i, j] = numerator / denominator\n",
    "    sim_matrix = sim_matrix + sim_matrix.T\n",
    "    g = nx.from_numpy_array(sim_matrix)\n",
    "    score = nx.pagerank(g)\n",
    "    temp = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "    idx = [item[0] for item in temp]\n",
    "    return [sents[i] for i in idx], [score[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triangles = find_triangle_with_node(single_sent_graph, 'Machine learning', 'Artificial neural network', 'Deep learning')\n",
    "test_triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = [triangle[1] for triangle in test_triangles]\n",
    "sents, score = do_pagerank(sent_list)\n",
    "list(zip(score, sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Super Sub-graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all sentences between two entities within one hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Single sentence graph\n",
    "single_sent_graph = my_read_pickle(single_sent_graph_file)\n",
    "edges = [edge for edge in tqdm.tqdm(single_sent_graph.edges) if single_sent_graph.get_edge_data(*edge)['score'] > 0.65]\n",
    "filtered_graph = single_sent_graph.edge_subgraph(edges)\n",
    "print('number of nodes:', filtered_graph.number_of_nodes())\n",
    "print('number of edges:', filtered_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = find_path_between_pair(single_sent_graph, 'Artificial intelligence', 'Natural language processing', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subgraph(paths:list, single_sent_graph:nx.Graph):\n",
    "    pairs = set()\n",
    "    triples = []\n",
    "    for path in paths:\n",
    "        if len(path) <= 2:\n",
    "            continue\n",
    "        for i in range(len(path)-1):\n",
    "            new_pair = frozenset((path[i], path[i+1]))\n",
    "            if new_pair not in pairs:\n",
    "                pairs.add(new_pair)\n",
    "                triples.append(list(new_pair) + [note2line(single_sent_graph.get_edge_data(path[i], path[i+1])['note']).strip()])\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = build_subgraph(paths, single_sent_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a graph for one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentence\n",
    "doc = nlp('sephardi were exempt from the ban , but it appears that few applied for a letter of free passage .')\n",
    "\n",
    "# Check noun phrases in the sentences\n",
    "print(list(doc.noun_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('ada is a structured , statically typed , imperative , and object-oriented high-level programming language , extended from pascal and other language .')\n",
    "pairs = [{'kw1' : 'ada', 'kw2' : 'programming language'}]\n",
    "feature_process(doc, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Not necessary] Online operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    remove_list = ['See also', 'References', 'Further reading', 'Sources', 'External links']\n",
    "    dic = {sec : page.section(sec) for sec in page.sections}\n",
    "    dic['summary'] = page.summary\n",
    "    sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    while len(section_list) > 0:\n",
    "        section = section_list.pop()\n",
    "        if section in remove_list:\n",
    "            continue\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        # processed_text = clean_text(section_text)\n",
    "        processed_text = ' '.join(section_text.lower().split())\n",
    "        temp_sents = my_sentence_tokenize(processed_text, True)\n",
    "        sents += temp_sents\n",
    "    return list(sents)\n",
    "\n",
    "def collect_entity_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    return [text.lower() for text in page.links]\n",
    "\n",
    "def collect_keyword_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    soup = BeautifulSoup(page.html(), 'html.parser')\n",
    "    main_block = soup.find('div', class_='mw-parser-output')\n",
    "    keywords = set([l.text.lower() for l in main_block.findAll('a') if re.match(r'^(<a href=\"/wiki/)', str(l))])\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'python'\n",
    "\n",
    "p = wikipedia.page(keyword)\n",
    "if p is not None:\n",
    "    sents = collect_sents_from_wiki_page(p)\n",
    "    keywords = collect_keyword_from_wiki_page(p)\n",
    "    print('sentences collected')\n",
    "    my_write('%s.txt' % keyword, sents)\n",
    "    my_write('%s_kw.txt' % keyword, keywords)\n",
    "    df = filter_by_path(sents)\n",
    "    df.to_csv('%s_out.tsv' % keyword, sep='\\t', index=False)\n",
    "\n",
    "    dff = df[df.apply(lambda x: str(x['head']) in keywords and str(x['tail']) in keywords, axis=1)]\n",
    "    dff.to_csv('%s_out_f.tsv' % keyword, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-crafted analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_df = wiki_path_test_df[wiki_path_test_df['sim'] >= 0.0]\n",
    "\n",
    "def match_path_pattern(path:str):\n",
    "    for pp in patterns:\n",
    "        if exact_match(pp, path):\n",
    "            return pp\n",
    "    return ''\n",
    "\n",
    "wiki_test_df['pattern'] = wiki_test_df.apply(lambda x: match_path_pattern(x['path']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_path_result_sim_based(df:pd.DataFrame, paths:list):\n",
    "    summary_df = pd.DataFrame(columns=['path', 'cnt', 'ratio', 'avg_sim'])\n",
    "    for pp in paths:\n",
    "        sub_df = df[df['pattern'] == pp]\n",
    "        summary_df = summary_df.append({\n",
    "            'path' : pp,\n",
    "            'cnt' : len(sub_df),\n",
    "            'ratio' : len(sub_df) / len(df),\n",
    "            'avg_sim' : sum(sub_df['sim']) / len(sub_df) if len(sub_df) else 0\n",
    "        }, ignore_index=True)\n",
    "    summary_df = summary_df.append({\n",
    "        'path' : 'general',\n",
    "        'cnt' : len(df),\n",
    "        'ratio' : 1,\n",
    "        'avg_sim' : sum(df['sim']) / len(df) if len(df) else 0\n",
    "    }, ignore_index=True)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path_result_sim_based(wiki_test_df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_example_sent_for_pattern(df:pd.DataFrame, path:str, num:int=30, posfix:str='.dat'):\n",
    "    sub_df = df[df['pattern'] == path]\n",
    "    num = min(len(sub_df), num)\n",
    "    sub_df = sub_df[:num]\n",
    "    sub_df['sent'] = sub_df.apply(lambda x: note2line(x['sent'], posfix=posfix).strip(), axis=1)\n",
    "    return sub_df\n",
    "\n",
    "for patt in patterns:\n",
    "    temp_df = collect_example_sent_for_pattern(wiki_test_df, patt)\n",
    "    temp_df.to_csv('%s.tsv' % (patt[:10] if len(patt) >= 10 else patt), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_set = my_read_pickle('data/extract_wiki/triangles.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(triangle_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tri in enumerate(triangle_set):\n",
    "    print(tri)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i, tri in enumerate(triangle_set):\n",
    "    samples.append(find_triangle_with_node(single_sent_graph, *tri))\n",
    "    if i > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9496c91418be784f00ee6456e4343e8188c649322b68f201c83241a4029a42d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('FWD_py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
