{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences from Wikipedia\n",
    "+ This notebook is used for collecting sentences that tell relationship between two entities from wikipedia using some dependency path pattern\n",
    "+ **This notebook is fully valid under Owl3 machine (using the /scratch/data/wikipedia/full_text-2021-03-20 data)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki sub folder example: ../data/wikipedia/full_text-2021-03-20/BE\n",
      "save sub folder example: data/extract_wiki/wiki_sent_collect/BE\n",
      "wiki file example: ../data/wikipedia/full_text-2021-03-20/BE/wiki_00\n",
      "save sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.dat\n",
      "save cooccur file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_co.dat\n",
      "save selected sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_se.dat\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wikipedia\n",
    "import os\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "import wikipedia2vec\n",
    "from collections import Counter\n",
    "import bz2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from typing import List\n",
    "from nltk.corpus import stopwords\n",
    "self_define_stopwords = set(['-', ',', '.'])\n",
    "sw = set(stopwords.words('english'))\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "random.seed(0)\n",
    "import torch\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from tools.BasicUtils import my_read, my_write, my_read_pickle, my_write_pickle\n",
    "from tools.TextProcessing import (\n",
    "                my_sentence_tokenize,\n",
    "                my_sentence_tokenize, filter_specific_keywords, nlp, \n",
    "                exact_match, find_root_in_span\n",
    "                )\n",
    "\n",
    "from extract_wiki import (\n",
    "    wikipedia_entity_file, \n",
    "    save_path, entity_occur_from_cooccur_file, graph_file, single_sent_graph_file, \n",
    "    w2vec_dump_file, \n",
    "    w2vec_keyword2idx_file, \n",
    "    test_path, path_test_file, \n",
    "    path_pattern_count_file, \n",
    "    save_sub_folders, wiki_sub_folders, \n",
    "    wiki_files, save_sent_files, save_cooccur_files, save_selected_files, save_title_files, save_cooccur__files, \n",
    "    p, patterns, \n",
    "    note2line, line2note, filter_unrelated_from_df, cal_score_from_df, cal_freq_from_path, cal_freq_from_df, \n",
    "    feature_process, gen_pattern, gen_kw_from_wiki_ent, get_entity_page, load_pattern_freq, find_triangles, find_path_between_pair, \n",
    "    generate_sample, generate_second_level_sample, sample_to_neo4j, get_sentence, informativeness_demo, find_dependency_info_from_tree\n",
    ")\n",
    "\n",
    "# Generate the save dir\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.mkdir(test_path)\n",
    "\n",
    "for save_dir in save_sub_folders:\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "# Get all files under wikipedia/full_text-2021-03-20\n",
    "\n",
    "print('wiki sub folder example:', wiki_sub_folders[0])\n",
    "print('save sub folder example:', save_sub_folders[0])\n",
    "print('wiki file example:', wiki_files[0])\n",
    "print('save sentence file example:', save_sent_files[0])\n",
    "print('save cooccur file example:', save_cooccur_files[0])\n",
    "print('save selected sentence file example:', save_selected_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] wikipedia2vec\n",
    "with bz2.open(w2vec_dump_file) as f_in:\n",
    "    w2vec = Wikipedia2Vec.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] wikipedia2vec\n",
    "\n",
    "# Find similar words or entities\n",
    "# ent1 = 'Python (programming language)'\n",
    "# w2vec.most_similar_by_vector(w2vec.get_entity_vector(ent1), 20)\n",
    "\n",
    "# Get similarity between two entities\n",
    "ent1 = 'Machine learning'\n",
    "ent2 = 'Information science'\n",
    "cosine_similarity(w2vec.get_entity_vector(ent1).reshape(1, -1), w2vec.get_entity_vector(ent2).reshape(1, -1))[0, 0]\n",
    "\n",
    "# Check the entity count and document count\n",
    "# ent1 = 'Hidden Markov model'\n",
    "# e = w2vec.get_entity(ent1)\n",
    "# print(e.count)\n",
    "# print(e.doc_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect sentences, entities, entities co-occurrances, titles from wikipedia dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roughly collect sentences, entity co-occurrances, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] Test get_sentence function\n",
    "get_sentence('ir_2.txt', 'sent.txt', 'cooccur.txt', 'title.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py collect_sent_and_cooccur (8 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect wikipedia entities\n",
    "wikipedia_entity = set()\n",
    "for f in tqdm.tqdm(save_title_files):\n",
    "    with open(f) as f_in:\n",
    "        wikipedia_entity.update(f_in.read().split('\\n'))\n",
    "print(len(wikipedia_entity))\n",
    "my_write(wikipedia_entity_file, list(wikipedia_entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wikipedia entities\n",
    "with open(wikipedia_entity_file) as f_in:\n",
    "    wikipedia_entity = set(f_in.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct entity mapping in co-occurrance files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec\n",
    "wikipedia_entity\n",
    "\n",
    "# Generate lower-cased entity to original entity mapping\n",
    "print('Generate lower-cased entity to original entity mapping')\n",
    "wikipedia_entity_low2orig_map = {}\n",
    "for ent in wikipedia_entity:\n",
    "    ent_low = ent.lower()\n",
    "    if ent_low not in wikipedia_entity_low2orig_map:\n",
    "        wikipedia_entity_low2orig_map[ent_low] = []\n",
    "    wikipedia_entity_low2orig_map[ent_low].append(ent)\n",
    "\n",
    "# Correct mapping\n",
    "print('Correct mapping')\n",
    "for i in tqdm.tqdm(range(len(save_cooccur_files))):\n",
    "    with open(save_cooccur_files[i]) as f_in:\n",
    "        new_file_lines = []\n",
    "        for line_idx, line in enumerate(f_in):\n",
    "            line = line.strip()\n",
    "            entities = line.split('\\t')\n",
    "            new_entities = []\n",
    "            for ent in entities:\n",
    "                if ent in wikipedia_entity:\n",
    "                    new_entities.append(ent)\n",
    "                else:\n",
    "                    ent_low = ent.lower()\n",
    "                    if ent_low in wikipedia_entity_low2orig_map:\n",
    "                        candidates = wikipedia_entity_low2orig_map[ent_low]\n",
    "                        if len(candidates) == 1:\n",
    "                            new_entities.append(candidates[0])\n",
    "                        else:\n",
    "                            note = line2note(save_cooccur_files[i], line_idx, '_co.dat')\n",
    "                            page_title = note2line(note, '_ti.dat').strip()\n",
    "                            try:\n",
    "                                page_ent_vec = w2vec.get_entity_vector(page_title)\n",
    "                            except:\n",
    "                                continue\n",
    "                            most_similar_idx, most_similar_val = -1, -1\n",
    "                            for candidate_idx, candidate_ent in enumerate(candidates):\n",
    "                                try:\n",
    "                                    candidate_vec = w2vec.get_entity_vector(candidate_ent)\n",
    "                                except:\n",
    "                                    continue\n",
    "                                similar_val = cosine_similarity(page_ent_vec.reshape(1, -1), candidate_vec.reshape(1, -1))[0,0]\n",
    "                                if similar_val > most_similar_val:\n",
    "                                    most_similar_val = similar_val\n",
    "                                    most_similar_idx = candidate_idx\n",
    "                            if most_similar_idx >= 0:\n",
    "                                new_entities.append(candidates[most_similar_idx])\n",
    "            new_file_lines.append('\\t'.join(new_entities))\n",
    "        my_write(save_cooccur__files[i], new_file_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test]\n",
    "lines = get_entity_page('Information retrieval')\n",
    "# sents = [note2line(note) for note in lines]\n",
    "# occurs = [note2line(note, '_co_.dat') for note in lines]\n",
    "# ori_occurs = [note2line(note, '_co.dat') for note in lines]\n",
    "# my_write('sent_check.txt', sents)\n",
    "# my_write('occur_check.txt', occurs)\n",
    "# my_write('ori_occur_check.txt', ori_occurs)\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate entity occurrance from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the entity occurrance dict from co-occurrance info [collect_ent_occur_from_cooccur]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Not necessary] Mapping keyword mention to wikipedia2vec entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_entity\n",
    "w2vec\n",
    "\n",
    "w2vec_keyword2idx = {}\n",
    "\n",
    "for entity in tqdm.tqdm(wikipedia_entity):\n",
    "    w2vec_entity = w2vec.get_entity(entity)\n",
    "    if w2vec_entity is None:\n",
    "        continue\n",
    "    kw = gen_kw_from_wiki_ent(entity)\n",
    "    if kw not in w2vec_keyword2idx:\n",
    "        w2vec_keyword2idx[kw] = [w2vec_entity.index]\n",
    "    else:\n",
    "        if w2vec_entity.index not in w2vec_keyword2idx[kw]:\n",
    "            w2vec_keyword2idx[kw].append(w2vec_entity.index)\n",
    "w2vec_kws = filter_specific_keywords(list(w2vec_keyword2idx.keys()))\n",
    "filter_keyword_from_w2vec = set(w2vec_kws)\n",
    "w2vec_keyword2idx = {k:v for k, v in w2vec_keyword2idx.items() if k in filter_keyword_from_w2vec}\n",
    "my_write_pickle(w2vec_keyword2idx_file, w2vec_keyword2idx)\n",
    "len(w2vec_keyword2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] w2vec_keyword2idx\n",
    "w2vec_keyword2idx = my_read_pickle(w2vec_keyword2idx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] w2vec_keyword2idx\n",
    "kw = 'feature engineering'\n",
    "kw_in_mention = kw in w2vec_keyword2idx\n",
    "print(kw_in_mention)\n",
    "if kw_in_mention:\n",
    "    for idx in w2vec_keyword2idx[kw]:\n",
    "        print(w2vec.dictionary.get_item_by_index(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect pattern frequency counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] [collect_pattern_freq] (12 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i_nsubj attr prep pobj', 130),\n",
       " ('i_nsubj prep pobj', 113),\n",
       " ('i_nsubj attr', 72),\n",
       " ('i_nsubj attr prep pobj prep pobj', 66),\n",
       " ('i_nsubjpass prep pobj', 64),\n",
       " ('i_nsubj dobj', 62),\n",
       " ('i_nsubj prep pobj prep pobj', 60),\n",
       " ('i_nsubjpass prep pobj prep pobj', 44),\n",
       " ('i_nsubj dobj prep pobj', 39),\n",
       " ('i_nsubjpass agent pobj', 20)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Load] cal_freq function\n",
    "c, log_max_cnt = load_pattern_freq(path_pattern_count_file)\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6152091645510752"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Test] cal_freq function\n",
    "cal_freq_from_path('i_nsubj prep pobj prep pobj prep pobj', c, log_max_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(path_test_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>kw1</th>\n",
       "      <th>kw1_span</th>\n",
       "      <th>kw1_ent</th>\n",
       "      <th>kw2</th>\n",
       "      <th>kw2_span</th>\n",
       "      <th>kw2_ent</th>\n",
       "      <th>sent</th>\n",
       "      <th>dep_path</th>\n",
       "      <th>pattern</th>\n",
       "      <th>dep_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.701339</td>\n",
       "      <td>archbishop of cologne</td>\n",
       "      <td>(3, 5)</td>\n",
       "      <td>Archbishop of Cologne</td>\n",
       "      <td>anno ii</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>Anno II</td>\n",
       "      <td>Anno II was Archbishop of Cologne from 1056 un...</td>\n",
       "      <td>i_attr nsubj</td>\n",
       "      <td>i_nsubj attr</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.631102</td>\n",
       "      <td>little darling</td>\n",
       "      <td>(3, 4)</td>\n",
       "      <td>Little Darling (I Need You)</td>\n",
       "      <td>marvin gaye</td>\n",
       "      <td>(11, 12)</td>\n",
       "      <td>Marvin Gaye</td>\n",
       "      <td>The track \"Little Darling \" is a remake of the...</td>\n",
       "      <td>i_appos i_nsubj attr prep pobj</td>\n",
       "      <td>i_nsubj attr prep pobj</td>\n",
       "      <td>0.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.614200</td>\n",
       "      <td>lombardy</td>\n",
       "      <td>(13, 13)</td>\n",
       "      <td>Lombardy</td>\n",
       "      <td>bagolino</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>Bagolino</td>\n",
       "      <td>Bagolino is a \"comune\" in the province of Bres...</td>\n",
       "      <td>i_pobj i_prep i_attr nsubj</td>\n",
       "      <td>i_nsubj attr prep pobj</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.806008</td>\n",
       "      <td>valle sabbia</td>\n",
       "      <td>(30, 31)</td>\n",
       "      <td>Valle Sabbia</td>\n",
       "      <td>bagolino</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>Bagolino</td>\n",
       "      <td>Bagolino is a \"comune\" in the province of Bres...</td>\n",
       "      <td>i_pobj i_prep i_pobj i_prep nsubj</td>\n",
       "      <td>i_nsubj prep pobj prep pobj</td>\n",
       "      <td>0.30303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.636523</td>\n",
       "      <td>comune</td>\n",
       "      <td>(4, 4)</td>\n",
       "      <td>Comune</td>\n",
       "      <td>bagolino</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>Bagolino</td>\n",
       "      <td>Bagolino is a \"comune\" in the province of Bres...</td>\n",
       "      <td>i_attr nsubj</td>\n",
       "      <td>i_nsubj attr</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sim                    kw1  kw1_span                      kw1_ent  \\\n",
       "0  0.701339  archbishop of cologne    (3, 5)        Archbishop of Cologne   \n",
       "1  0.631102         little darling    (3, 4)  Little Darling (I Need You)   \n",
       "2  0.614200               lombardy  (13, 13)                     Lombardy   \n",
       "3  0.806008           valle sabbia  (30, 31)                 Valle Sabbia   \n",
       "4  0.636523                 comune    (4, 4)                       Comune   \n",
       "\n",
       "           kw2  kw2_span      kw2_ent  \\\n",
       "0      anno ii    (0, 1)      Anno II   \n",
       "1  marvin gaye  (11, 12)  Marvin Gaye   \n",
       "2     bagolino    (0, 0)     Bagolino   \n",
       "3     bagolino    (0, 0)     Bagolino   \n",
       "4     bagolino    (0, 0)     Bagolino   \n",
       "\n",
       "                                                sent  \\\n",
       "0  Anno II was Archbishop of Cologne from 1056 un...   \n",
       "1  The track \"Little Darling \" is a remake of the...   \n",
       "2  Bagolino is a \"comune\" in the province of Bres...   \n",
       "3  Bagolino is a \"comune\" in the province of Bres...   \n",
       "4  Bagolino is a \"comune\" in the province of Bres...   \n",
       "\n",
       "                            dep_path                      pattern  \\\n",
       "0                       i_attr nsubj                 i_nsubj attr   \n",
       "1     i_appos i_nsubj attr prep pobj       i_nsubj attr prep pobj   \n",
       "2         i_pobj i_prep i_attr nsubj       i_nsubj attr prep pobj   \n",
       "3  i_pobj i_prep i_pobj i_prep nsubj  i_nsubj prep pobj prep pobj   \n",
       "4                       i_attr nsubj                 i_nsubj attr   \n",
       "\n",
       "   dep_coverage  \n",
       "0       1.00000  \n",
       "1       0.87500  \n",
       "2       1.00000  \n",
       "3       0.30303  \n",
       "4       1.00000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# path = get_path(doc, kw1_steps, kw2_steps)\n",
    "# expand_dependency_info_from_tree(doc, branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagolino is a \"comune\" in the province of Brescia, in Lombardy, Italy, in the valley of the river Caffaro, on the right side of Valle Sabbia.\n",
      "lombardy (13, 13)\n",
      "bagolino (0, 0)\n",
      "i_pobj i_prep i_attr nsubj\n",
      "\n",
      "is ---- punct ---- .\n",
      "is ---- prep ---- on\n",
      "is ---- prep pobj ---- side\n",
      "is ---- prep pobj prep ---- of\n",
      "is ---- prep pobj prep pobj ---- Sabbia\n",
      "is ---- prep pobj prep pobj compound ---- Valle\n",
      "is ---- prep pobj amod ---- right\n",
      "is ---- prep pobj det ---- the\n",
      "comune ---- punct ---- ,\n",
      "comune ---- prep ---- in\n",
      "comune ---- prep pobj ---- valley\n",
      "comune ---- prep pobj prep ---- of\n",
      "comune ---- prep pobj prep pobj ---- Caffaro\n",
      "comune ---- prep pobj prep pobj compound ---- river\n",
      "comune ---- prep pobj prep pobj det ---- the\n",
      "comune ---- prep pobj det ---- the\n",
      "comune ---- punct ---- ,\n",
      "comune ---- punct ---- ,\n",
      "comune ---- prep ---- in\n",
      "comune ---- prep pobj ---- province\n",
      "comune ---- prep pobj prep ---- of\n",
      "comune ---- prep pobj prep pobj ---- Brescia\n",
      "comune ---- prep pobj det ---- the\n",
      "comune ---- punct ---- \"\n",
      "comune ---- punct ---- \"\n",
      "comune ---- det ---- a\n",
      "Lombardy ---- appos ---- Italy\n",
      "Lombardy ---- punct ---- ,\n"
     ]
    }
   ],
   "source": [
    "# Test find dependency path\n",
    "data = test_df.iloc[2]\n",
    "doc = nlp(data['sent'])\n",
    "kw1_span = eval(data['kw1_span'])\n",
    "kw2_span = eval(data['kw2_span'])\n",
    "print(doc)\n",
    "kw1_steps, kw2_steps, branch = find_dependency_info_from_tree(doc, doc[kw1_span[0] : kw1_span[1]+1], doc[kw2_span[0] : kw2_span[1]+1])\n",
    "ans = get_path(doc, kw1_steps, kw2_steps)\n",
    "print(data['kw1'], data['kw1_span'])\n",
    "print(data['kw2'], data['kw2_span'])\n",
    "print(ans)\n",
    "print()\n",
    "ans = collect_sub_dependency_path(doc, branch)\n",
    "for item in ans:\n",
    "    print(doc[item[0]], '----', item[1], '----', doc[item[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The office of archchancellor of the Imperial Kingdom of Italy was at this period regarded as an appanage of the Archbishopric of Cologne, and this was probably the reason why Anno had a considerable share in settling a papal dispute brewing since 1061: relying on an assessment by his nephew Bishop Burchard of Halberstadt, he declared Pope Alexander II to be the rightful pope at a synod held at Mantua in May 1064, and took other steps to secure his recognition against Empress Agnes' candidate Antipope Honorius II.\n",
      "declared ---- punct ---- .\n",
      "declared ---- cc ---- and\n",
      "declared ---- punct ---- ,\n",
      "declared ---- nsubj ---- he\n",
      "declared ---- punct ---- ,\n",
      "declared ---- ccomp ---- was\n",
      "declared ---- ccomp conj ---- was\n",
      "declared ---- ccomp conj advcl ---- relying\n",
      "declared ---- ccomp conj advcl prep ---- on\n",
      "declared ---- ccomp conj advcl prep pobj ---- assessment\n",
      "declared ---- ccomp conj advcl prep pobj prep ---- by\n",
      "declared ---- ccomp conj advcl prep pobj prep pobj ---- nephew\n",
      "declared ---- ccomp conj advcl prep pobj prep pobj appos ---- Burchard\n",
      "declared ---- ccomp conj advcl prep pobj prep pobj appos prep ---- of\n",
      "declared ---- ccomp conj advcl prep pobj prep pobj appos prep pobj ---- Halberstadt\n",
      "declared ---- ccomp conj advcl prep pobj prep pobj appos compound ---- Bishop\n",
      "declared ---- ccomp conj advcl prep pobj prep pobj poss ---- his\n",
      "declared ---- ccomp conj advcl prep pobj det ---- an\n",
      "declared ---- ccomp conj punct ---- :\n",
      "declared ---- ccomp conj attr ---- reason\n",
      "declared ---- ccomp conj attr relcl ---- had\n",
      "declared ---- ccomp conj attr relcl dobj ---- share\n",
      "declared ---- ccomp conj attr relcl dobj prep ---- in\n",
      "declared ---- ccomp conj attr relcl dobj prep pcomp ---- settling\n",
      "declared ---- ccomp conj attr relcl dobj prep pcomp prep ---- since\n",
      "declared ---- ccomp conj attr relcl dobj prep pcomp prep pobj ---- 1061\n",
      "declared ---- ccomp conj attr relcl dobj prep pcomp dobj ---- brewing\n",
      "declared ---- ccomp conj attr relcl dobj prep pcomp dobj compound ---- dispute\n",
      "declared ---- ccomp conj attr relcl dobj prep pcomp dobj amod ---- papal\n",
      "declared ---- ccomp conj attr relcl dobj prep pcomp dobj det ---- a\n",
      "declared ---- ccomp conj attr relcl dobj amod ---- considerable\n",
      "declared ---- ccomp conj attr relcl dobj det ---- a\n",
      "declared ---- ccomp conj attr relcl nsubj ---- Anno\n",
      "declared ---- ccomp conj attr relcl advmod ---- why\n",
      "declared ---- ccomp conj attr det ---- the\n",
      "declared ---- ccomp conj advmod ---- probably\n",
      "declared ---- ccomp conj nsubj ---- this\n",
      "declared ---- ccomp cc ---- and\n",
      "declared ---- ccomp punct ---- ,\n",
      "declared ---- ccomp prep ---- at\n",
      "declared ---- ccomp prep pobj ---- period\n",
      "declared ---- ccomp prep pobj acl ---- regarded\n",
      "declared ---- ccomp prep pobj acl prep ---- as\n",
      "declared ---- ccomp prep pobj acl prep pobj ---- appanage\n",
      "declared ---- ccomp prep pobj acl prep pobj prep ---- of\n",
      "declared ---- ccomp prep pobj acl prep pobj prep pobj ---- Archbishopric\n",
      "declared ---- ccomp prep pobj acl prep pobj prep pobj prep ---- of\n",
      "declared ---- ccomp prep pobj acl prep pobj prep pobj prep pobj ---- Cologne\n",
      "declared ---- ccomp prep pobj acl prep pobj prep pobj det ---- the\n",
      "declared ---- ccomp prep pobj acl prep pobj det ---- an\n",
      "declared ---- ccomp prep pobj det ---- this\n",
      "declared ---- ccomp nsubj ---- office\n",
      "declared ---- ccomp nsubj prep ---- of\n",
      "declared ---- ccomp nsubj prep pobj ---- archchancellor\n",
      "declared ---- ccomp nsubj prep pobj prep ---- of\n",
      "declared ---- ccomp nsubj prep pobj prep pobj ---- Kingdom\n",
      "declared ---- ccomp nsubj prep pobj prep pobj prep ---- of\n",
      "declared ---- ccomp nsubj prep pobj prep pobj prep pobj ---- Italy\n",
      "declared ---- ccomp nsubj prep pobj prep pobj compound ---- Imperial\n",
      "declared ---- ccomp nsubj prep pobj prep pobj det ---- the\n",
      "declared ---- ccomp nsubj det ---- The\n",
      "be ---- prep ---- at\n",
      "be ---- prep pobj ---- synod\n",
      "be ---- prep pobj acl ---- held\n",
      "be ---- prep pobj acl prep ---- in\n",
      "be ---- prep pobj acl prep pobj ---- May\n",
      "be ---- prep pobj acl prep pobj nummod ---- 1064\n",
      "be ---- prep pobj acl prep ---- at\n",
      "be ---- prep pobj acl prep pobj ---- Mantua\n",
      "be ---- prep pobj det ---- a\n",
      "be ---- attr ---- pope\n",
      "be ---- attr amod ---- rightful\n",
      "be ---- attr det ---- the\n",
      "be ---- aux ---- to\n",
      "took ---- dobj ---- steps\n",
      "took ---- dobj amod ---- other\n",
      "secure ---- aux ---- to\n",
      "recognition ---- poss ---- his\n",
      "candidate ---- poss ---- Agnes\n",
      "candidate ---- poss case ---- '\n",
      "candidate ---- poss compound ---- Empress\n"
     ]
    }
   ],
   "source": [
    "# Test find sub dependency path\n",
    "data = test_df.iloc[1]\n",
    "doc = nlp(data['sent'])\n",
    "kw1_span = eval(data['kw1_span'])\n",
    "kw2_span = eval(data['kw2_span'])\n",
    "print(doc)\n",
    "kw1_steps, kw2_steps, branch = find_dependency_info_from_tree(doc, doc[kw1_span[0] : kw1_span[1]+1], doc[kw2_span[0] : kw2_span[1]+1])\n",
    "ans = collect_sub_dependency_path(doc, branch)\n",
    "for item in ans:\n",
    "    print(doc[item[0]], '----', item[1], '----', doc[item[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "death"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] collect dataset [collect_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Prepration] Sentence-edged Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the graph ['generate_graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate single sentence graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate single sentence graph ['generate_single_sent_graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments & Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the basic properties of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Graph\n",
    "graph:nx.Graph = my_read_pickle(graph_file)\n",
    "\n",
    "print('num of nodes:', len(graph.nodes))\n",
    "print('num of edges:', len(graph.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] graph\n",
    "ent1 = 'Machine learning'\n",
    "# Check the neighbours of an entity\n",
    "list(graph.neighbors(ent1))\n",
    "\n",
    "# Check the edges of two entities\n",
    "# edges = graph.edges[ent1, 'Hinge loss']\n",
    "# edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, log_max_cnt = load_pattern_freq(path_pattern_count_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(356)/np.log(476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = my_read_pickle(entity_occur_from_cooccur_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = d['Computer science'] & d['Information science']\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "h_pattern_h_cov_check = 0\n",
    "h_pattern_l_cov_check = 0\n",
    "l_pattern_h_cov_check = 0\n",
    "l_pattern_l_cov_check = 0\n",
    "ent = 'Machine learning'\n",
    "for n in tqdm.tqdm(list(graph.neighbors(ent))):\n",
    "    if n not in d:\n",
    "        continue\n",
    "    if n == ent:\n",
    "        continue\n",
    "    s = d[ent] & d[n]\n",
    "    if len(s) >= 4:\n",
    "        s = list(s)\n",
    "        sents = [note2line(note).strip() for note in s]\n",
    "        pairs = [{'kw1' : gen_kw_from_wiki_ent(ent), 'kw2' : gen_kw_from_wiki_ent(n)}]\n",
    "        temp_list = []\n",
    "        for idx, sent in enumerate(sents):\n",
    "            res = feature_process(nlp(sent), pairs)\n",
    "            for i in res:\n",
    "                i['sent'] = sent\n",
    "                i['note'] = s[idx]\n",
    "            temp_list.extend(res)\n",
    "        if not temp_list:\n",
    "            continue\n",
    "        df = pd.DataFrame(temp_list)\n",
    "        df = cal_freq_from_df(df, c, log_max_cnt)\n",
    "        df = cal_score_from_df(df)\n",
    "        \n",
    "        h_pattern_h_cov = -1\n",
    "        h_pattern_l_cov = -1\n",
    "        l_pattern_h_cov = -1\n",
    "        l_pattern_l_cov = -1\n",
    "        for idx in range(len(df)):\n",
    "            if h_pattern_h_cov < 0 and df.iloc[idx]['dep_coverage'] > 0.7 and  df.iloc[idx]['pattern_freq'] > 0.7:\n",
    "                h_pattern_h_cov = idx\n",
    "            elif h_pattern_l_cov < 0 and df.iloc[idx]['dep_coverage'] < 0.6 and  df.iloc[idx]['pattern_freq'] > 0.7:\n",
    "                h_pattern_l_cov = idx\n",
    "            elif l_pattern_h_cov < 0 and df.iloc[idx]['dep_coverage'] > 0.7 and  df.iloc[idx]['pattern_freq'] < 0.4:\n",
    "                l_pattern_h_cov = idx\n",
    "            elif l_pattern_l_cov < 0 and df.iloc[idx]['dep_coverage'] < 0.6 and  df.iloc[idx]['pattern_freq'] < 0.4:\n",
    "                l_pattern_l_cov = idx\n",
    "        if h_pattern_h_cov >= 0 and h_pattern_l_cov >= 0 and l_pattern_h_cov >= 0 and l_pattern_l_cov >= 0:\n",
    "            examples.append((df, h_pattern_h_cov, h_pattern_l_cov, l_pattern_h_cov, l_pattern_l_cov))\n",
    "        h_pattern_h_cov_check += (h_pattern_h_cov >= 0)\n",
    "        h_pattern_l_cov_check += (h_pattern_l_cov >= 0)\n",
    "        l_pattern_h_cov_check += (l_pattern_h_cov >= 0)\n",
    "        l_pattern_l_cov_check += (l_pattern_l_cov >= 0)\n",
    "\n",
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(h_pattern_h_cov_check)\n",
    "print(h_pattern_l_cov_check)\n",
    "print(l_pattern_h_cov_check)\n",
    "print(l_pattern_l_cov_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = examples[0]\n",
    "df = example[0]\n",
    "example[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['kw1'], df.iloc[0]['kw2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informativeness_demo(\"When annexed to the Kingdom of Italy in 1859 Lombardy achieved its present-day territorial shape by adding the OltrepÃ² Pavese to the province of Pavia.\", 'lombardy', 'pavia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df[df['pattern_freq']<0.5]\n",
    "# temp_df[temp_df['pattern_freq']>0.07]\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sentence from file\n",
    "note2line('AW:87:3103')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2neig_cnt = {node : len(list(graph.neighbors(node))) for node in graph.nodes.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neig_cnt = [v for v in node2neig_cnt.values() if v < 20]\n",
    "plt.title('num of nodes vs num of neighbors each node')\n",
    "plt.hist(neig_cnt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2triangle_num = nx.triangles(graph)\n",
    "print('num of triangles:', sum(node2triangle_num.values()) / 3)\n",
    "plt.title('num of nodes vs num of triangles each node')\n",
    "plt.hist([v for v in node2triangle_num.values() if v >= 1 and v < 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Single sentence graph\n",
    "graph = my_read_pickle(graph_file)\n",
    "print('number of nodes:', graph.number_of_nodes())\n",
    "print('number of edges:', graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data of level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "ent1 = 'Data mining'\n",
    "ent2 = 'Decision tree'\n",
    "sample = generate_sample(graph, ent1, ent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "triple:list = sample['triple']\n",
    "sources:List[str] = sample['source']\n",
    "entity:List[str] = sample['entity']\n",
    "avg_scores = [mean([tri['score'] for tri in path]) for path in triple]\n",
    "sorted_list = sorted(zip(avg_scores, triple), key=lambda x: x[0], reverse=True)\n",
    "triple = list(zip(*sorted_list))[1]\n",
    "contexts = [[{'e1' : entity[tri['e1']], \n",
    "            'e2' : entity[tri['e2']], \n",
    "            'sent' : sources[tri['sent']],\n",
    "            'score' : tri['score']} for tri in path] for path in triple]\n",
    "ctxs = []\n",
    "for ctx in contexts[:5]:\n",
    "    path = [ctx[0]['e1']]\n",
    "    sents = []\n",
    "    for i, tri in enumerate(ctx):\n",
    "        path.append(tri['e2'])\n",
    "        sents.append('sentence%d: %s' % (i+1, tri['sent']))\n",
    "    path = '; '.join(path)\n",
    "    sents = ' '.join(sents)\n",
    "    ctxs.append('%s %s' % ('path: ' + path, sents))\n",
    "for ctx in ctxs:\n",
    "    print(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data of level 1 [collect_one_hop_sample_from_single_sent_graph] (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_level_1.json') as f_in:\n",
    "    a = json.load(f_in)\n",
    "items = [item for item in a if len(item['source']) > 1]\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(items)\n",
    "train_ratio = 0.85\n",
    "valid_ratio = 0.05\n",
    "training_data = items[:int(len(items)*train_ratio)]\n",
    "valid_data = items[int(len(items)*train_ratio):int(len(items)*(train_ratio+valid_ratio))]\n",
    "test_data = items[int(len(items)*(train_ratio+valid_ratio)):]\n",
    "with open('train.json', 'w') as f_out:\n",
    "    json.dump(training_data, f_out)\n",
    "with open('dev.json', 'w') as f_out:\n",
    "    json.dump(valid_data, f_out)\n",
    "with open('test.json', 'w') as f_out:\n",
    "    json.dump(test_data, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data for level 1 with random path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_occur_from_cooccur = my_read_pickle(entity_occur_from_cooccur_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_in, file_out in [('MyFiD/data/train.json', 'train_random.json'), ('MyFiD/data/dev.json', 'dev_random.json'), ('MyFiD/data/test.json', 'test_random.json')]:\n",
    "    with open(file_in) as f_in:\n",
    "        data = json.load(f_in)\n",
    "        for item in tqdm.tqdm(data):\n",
    "            for path in item['triple']:\n",
    "                for tri in path:\n",
    "                    item['source'][tri['sent']] = note2line(random.choice(list(entity_occur_from_cooccur[item['entity'][tri['e1']]] & entity_occur_from_cooccur[item['entity'][tri['e2']]]))).strip()\n",
    "        with open(file_out, 'w') as f_out:\n",
    "            json.dump(data, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len_count = []\n",
    "for data in training_data:\n",
    "    sents = data['source']\n",
    "    sent_len_count.extend([len(sent.split()) for sent in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sent_len_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(x[x<100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sent_len_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_neo4j(items[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data of level 2 from level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.json') as f_in:\n",
    "    sample = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_sample = generate_second_level_sample(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph4nlp.pytorch.data import GraphData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_labels = list(nlp.get_pipe(\"parser\").labels)\n",
    "dep_labels.extend(['i_'+dep for dep in dep_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GraphData()\n",
    "is_rel = []\n",
    "is_entity = []\n",
    "\n",
    "g.add_nodes(1)\n",
    "is_rel.append(0)\n",
    "is_entity.append(0)\n",
    "\n",
    "for src in second_level_sample['sources']:\n",
    "    pair = src['pair']\n",
    "    sent_tokens = src['sent']\n",
    "    \n",
    "    label_list = []\n",
    "    label_list.extend(sent_tokens)\n",
    "    token_num = len(sent_tokens)\n",
    "    start_node = g.get_node_num()\n",
    "    g.add_nodes(token_num)\n",
    "    is_rel.extend([0]*token_num)\n",
    "    is_entity.extend([0]*token_num)\n",
    "    is_entity[pair[0]+start_node] = 1\n",
    "    is_entity[pair[1]+start_node] = 1\n",
    "    \n",
    "    label_list.extend(['ROOT', 'ROOT', 'i_ROOT', 'i_ROOT'])\n",
    "    rel_start_node = start_node + token_num\n",
    "    g.add_nodes(4)\n",
    "    is_rel.extend([1]*4)\n",
    "    is_entity.extend([0]*4)\n",
    "    g.add_edges([0, 0], [rel_start_node, rel_start_node+1])\n",
    "    g.add_edges([rel_start_node, rel_start_node+1], [pair[0]+start_node, pair[1]+start_node])\n",
    "    g.add_edges([pair[0]+start_node, pair[1]+start_node], [rel_start_node+2, rel_start_node+3])\n",
    "    g.add_edges([rel_start_node+2, rel_start_node+3], [0, 0])\n",
    "    \n",
    "    rel_start_node += 4\n",
    "    triples = src['graph']\n",
    "    rel_num = len(triples)\n",
    "    is_rel.extend([1]*rel_num)\n",
    "    is_entity.extend([0]*rel_num)\n",
    "    g.add_nodes(rel_num)\n",
    "    for rel_idx, (tok_1, tok_2, rel) in enumerate(triples):\n",
    "        g.add_edges([tok_1+start_node, rel_idx+rel_start_node], [rel_idx+rel_start_node, tok_2+start_node])\n",
    "        label_list.append(rel)\n",
    "    for i, label in enumerate(label_list):\n",
    "        g.node_attributes[i+start_node]['label'] = label\n",
    "g.node_features['is_rel'] = torch.BoolTensor(is_rel)\n",
    "g.node_features['is_entity'] = torch.BoolTensor(is_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.get_edge_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_triangle_with_node(graph:nx.Graph, first_node:str, second_node:str='', third_node:str=''):\n",
    "    triangles = list(find_triangles(graph, first_node))\n",
    "    triangles.sort(key=lambda x: x[1])\n",
    "    triangle_with_sents = []\n",
    "    n_seen = set()\n",
    "    for n1, n2, n3 in triangles:\n",
    "        if second_node and n2 != second_node and n3 != second_node:\n",
    "            continue\n",
    "        if third_node and n2 != third_node and n3 != third_node:\n",
    "            continue\n",
    "        if n2 not in n_seen:\n",
    "            n_seen.add(n2)\n",
    "            triangle_with_sents.append((n1, note2line(graph.get_edge_data(n1, n2)['note']).strip(), n2, graph.get_edge_data(n1, n2)['score']))\n",
    "        if n3 not in n_seen:\n",
    "            n_seen.add(n3)\n",
    "            triangle_with_sents.append((n1, note2line(graph.get_edge_data(n1, n3)['note']).strip(), n3, graph.get_edge_data(n1, n3)['score']))\n",
    "        triangle_with_sents.append((n2, note2line(graph.get_edge_data(n3, n2)['note']).strip(), n3, graph.get_edge_data(n3, n2)['score']))\n",
    "    return triangle_with_sents\n",
    "\n",
    "\n",
    "def isf(w:str, D:int, counters:List[Counter]):\n",
    "    return math.log(D * 1.0 / sum([1 if w in sent else 0 for sent in counters]))\n",
    "\n",
    "\n",
    "def do_pagerank(sents:List[str]):\n",
    "    # Remove stop words\n",
    "    clean_sents = [[token for token in sent.split() if token not in sw and token not in self_define_stopwords] for sent in sents]\n",
    "\n",
    "    # Generate word counters\n",
    "    counters = [Counter(sent) for sent in clean_sents]\n",
    "\n",
    "    # Build similarity matrix\n",
    "    D = len(clean_sents)\n",
    "    sim_matrix = np.zeros((D, D))\n",
    "    part_list = [math.sqrt(sum([(sent[w] * isf(w, D, counters)) ** 2 for w in sent])) for sent in counters]\n",
    "    # return part_list\n",
    "    for i in range(D - 1):\n",
    "        for j in range(i + 1, D):\n",
    "            sent_1 = counters[i]\n",
    "            sent_2 = counters[j]\n",
    "            share_word_set = sent_1 & sent_2\n",
    "            numerator = sum([(sent_1[w] * sent_2[w] * (isf(w, D, counters) ** 2)) for w in share_word_set])\n",
    "            denominator = part_list[i] * part_list[j]\n",
    "            sim_matrix[i, j] = numerator / denominator\n",
    "    sim_matrix = sim_matrix + sim_matrix.T\n",
    "    g = nx.from_numpy_array(sim_matrix)\n",
    "    score = nx.pagerank(g)\n",
    "    temp = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "    idx = [item[0] for item in temp]\n",
    "    return [sents[i] for i in idx], [score[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triangles = find_triangle_with_node(single_sent_graph, 'Machine learning', 'Artificial neural network', 'Deep learning')\n",
    "test_triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = [triangle[1] for triangle in test_triangles]\n",
    "sents, score = do_pagerank(sent_list)\n",
    "list(zip(score, sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Test] Check the score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Graph\n",
    "# graph:nx.Graph = my_read_pickle(graph_file)\n",
    "\n",
    "# print('num of nodes:', len(graph.nodes))\n",
    "# print('num of edges:', len(graph.edges))\n",
    "\n",
    "c, log_max_cnt = load_pattern_freq(path_pattern_count_file)\n",
    "d = my_read_pickle(entity_occur_from_cooccur_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "ent = 'Machine learning'\n",
    "# for edge in graph.edges:\n",
    "for neighbor in graph.neighbors(ent):\n",
    "    # if edge[0] not in d or edge[1] not in d:\n",
    "    #     continue\n",
    "    # s = d[edge[0]] & d[edge[1]]\n",
    "    if neighbor not in d:\n",
    "        continue\n",
    "    s = d[ent] & d[neighbor]\n",
    "    s = list(s)\n",
    "    sents = [note2line(note).strip() for note in s]\n",
    "    # pairs = [{'kw1' : gen_kw_from_wiki_ent(edge[0]), 'kw2' : gen_kw_from_wiki_ent(edge[1])}]\n",
    "    pairs = [{'kw1' : gen_kw_from_wiki_ent(ent), 'kw2' : gen_kw_from_wiki_ent(neighbor)}]\n",
    "    temp_list = []\n",
    "    for idx, sent in enumerate(sents):\n",
    "        res = feature_process(nlp(sent), pairs)\n",
    "        for i in res:\n",
    "            i['sent'] = sent\n",
    "            i['note'] = s[idx]\n",
    "        temp_list.extend(res)\n",
    "    if not temp_list:\n",
    "        continue\n",
    "    df = pd.DataFrame(temp_list)\n",
    "    df = cal_freq_from_df(df, c, log_max_cnt)\n",
    "    df = cal_score_from_df(df)\n",
    "    if len(df) >= 5:\n",
    "        df = df[:5]\n",
    "        df = df.sort_values(by=['score'], ascending=False)\n",
    "        examples.append(df)\n",
    "        if len(examples) >= 10:\n",
    "            break\n",
    "\n",
    "test_df = pd.concat(examples)\n",
    "test_df.to_csv('temp.csv', index=False, columns=['kw1', 'kw1_span', 'kw2', 'kw2_span', 'note', 'sent', 'dep_coverage', 'pattern_freq', 'pattern', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df:pd.DataFrame = pd.read_csv('temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples = []\n",
    "for i in range(len(test_df)):\n",
    "    doc = nlp(test_df.loc[i, 'sent'])\n",
    "    kw1_span = eval(test_df.loc[i, 'kw1_span'])\n",
    "    kw2_span = eval(test_df.loc[i, 'kw2_span'])\n",
    "    test_df.loc[i, 'dep_coverage'] = find_dependency_info_from_tree(doc, doc[kw1_span[0]:kw1_span[1]+1], doc[kw2_span[0]:kw2_span[1]+1]).mean()\n",
    "for kw1, kw2 in set(zip(test_df['kw1'].tolist(), test_df['kw2'].tolist())):\n",
    "    temp_df:pd.DataFrame = test_df[(test_df['kw1'] == kw1) & (test_df['kw2'] == kw2)]\n",
    "    temp_df = cal_freq_from_df(temp_df, c, log_max_cnt)\n",
    "    temp_df = cal_score_from_df(temp_df)\n",
    "    temp_df = temp_df.sort_values(by=['score'], ascending=False)\n",
    "    new_examples.append(temp_df)\n",
    "new_test_df = pd.concat(new_examples)\n",
    "new_test_df.to_csv('new_temp.csv', index=False, columns=['kw1', 'kw1_span', 'kw2', 'kw2_span', 'note', 'sent', 'dep_coverage', 'pattern_freq', 'pattern', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.loc[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(test_df['kw1_span'].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [collect_score_function_eval_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test score function with human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, log_max_cnt = load_pattern_freq(path_pattern_count_file)\n",
    "\n",
    "pattern_freq_w = 0.55\n",
    "kw_recall_w = 0.25\n",
    "coverage_w = 0.2\n",
    "\n",
    "def get_score(sent:str, ent1:str, ent2:str):\n",
    "    kw1 = gen_kw_from_wiki_ent(ent1)\n",
    "    kw2 = gen_kw_from_wiki_ent(ent2)\n",
    "    data = feature_process(nlp(sent), [{'kw1' : kw1, 'kw2' : kw2}])\n",
    "    if not data:\n",
    "        return -1\n",
    "    data = data[0]\n",
    "    pattern_freq = cal_freq_from_path(gen_pattern(data['dep_path']), c, log_max_cnt)\n",
    "    return ((pattern_freq)**pattern_freq_w) * (((data['kw1_recall'] + data['kw2_recall']) / 2)**kw_recall_w) * (((data['dep_coverage'] + data['surface_coverage']) / 2)**coverage_w)\n",
    "\n",
    "test_data = pd.read_csv('test.tsv', sep='\\t')\n",
    "score_function_result = test_data.copy()\n",
    "score_function_result['score'] = score_function_result.apply(lambda x: get_score(x['sentence'], x['entity 1'], x['entity 2']), axis=1)\n",
    "score_function_result.to_csv('score_function_result.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('score_function_result.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    sf_score = [float(lines[i].strip().split('\\t')[-1]) for i in range(1, len(lines))]\n",
    "    sf_score = np.array(sf_score)\n",
    "\n",
    "with open('user_label.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    user_score = [float(lines[i].strip().split('\\t')[-1]) / 5 for i in range(1, len(lines))]\n",
    "    user_score = np.array(user_score)\n",
    "    \n",
    "with open('user_label.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    data = []\n",
    "    for i in range(1, len(lines)):\n",
    "        ent1, ent2, sent, user_score_ = lines[i].strip().split('\\t')\n",
    "        data.append({'entity 1' : ent1, 'entity 2' : ent2, 'sentence' : sent, 'user label' : float(user_score_)/5})\n",
    "        \n",
    "sf_score = sf_score[user_score > 0]\n",
    "user_score = user_score[user_score > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(sf_score, user_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score range within [1,2,3,4,5]\n",
    "l2 = np.mean(np.abs(sf_score*5 - user_score*5))\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(user_score*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(user_score[sf_score>0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(user_score[sf_score<=0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sf_score*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.abs(sf_score - user_score)\n",
    "diff = []\n",
    "for i in range(len(dist)):\n",
    "    if dist[i] > 0.3:\n",
    "        diff.append(data[i].copy())\n",
    "        diff[-1]['score function label'] = sf_score[i]\n",
    "diff = pd.DataFrame(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.to_csv('diff.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Super Sub-graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all sentences between two entities within one hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Single sentence graph\n",
    "single_sent_graph = my_read_pickle(single_sent_graph_file)\n",
    "edges = [edge for edge in tqdm.tqdm(single_sent_graph.edges) if single_sent_graph.get_edge_data(*edge)['score'] > 0.65]\n",
    "filtered_graph = single_sent_graph.edge_subgraph(edges)\n",
    "print('number of nodes:', filtered_graph.number_of_nodes())\n",
    "print('number of edges:', filtered_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = find_path_between_pair(single_sent_graph, 'Artificial intelligence', 'Natural language processing', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subgraph(paths:list, single_sent_graph:nx.Graph):\n",
    "    pairs = set()\n",
    "    triples = []\n",
    "    for path in paths:\n",
    "        if len(path) <= 2:\n",
    "            continue\n",
    "        for i in range(len(path)-1):\n",
    "            new_pair = frozenset((path[i], path[i+1]))\n",
    "            if new_pair not in pairs:\n",
    "                pairs.add(new_pair)\n",
    "                triples.append(list(new_pair) + [note2line(single_sent_graph.get_edge_data(path[i], path[i+1])['note']).strip()])\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = build_subgraph(paths, single_sent_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a graph for one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentence\n",
    "doc = nlp('sephardi were exempt from the ban , but it appears that few applied for a letter of free passage .')\n",
    "\n",
    "# Check noun phrases in the sentences\n",
    "print(list(doc.noun_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('ada is a structured , statically typed , imperative , and object-oriented high-level programming language , extended from pascal and other language .')\n",
    "pairs = [{'kw1' : 'ada', 'kw2' : 'programming language'}]\n",
    "feature_process(doc, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Not necessary] Online operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    remove_list = ['See also', 'References', 'Further reading', 'Sources', 'External links']\n",
    "    dic = {sec : page.section(sec) for sec in page.sections}\n",
    "    dic['summary'] = page.summary\n",
    "    sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    while len(section_list) > 0:\n",
    "        section = section_list.pop()\n",
    "        if section in remove_list:\n",
    "            continue\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        # processed_text = clean_text(section_text)\n",
    "        processed_text = ' '.join(section_text.lower().split())\n",
    "        temp_sents = my_sentence_tokenize(processed_text, True)\n",
    "        sents += temp_sents\n",
    "    return list(sents)\n",
    "\n",
    "def collect_entity_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    return [text.lower() for text in page.links]\n",
    "\n",
    "def collect_keyword_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    soup = BeautifulSoup(page.html(), 'html.parser')\n",
    "    main_block = soup.find('div', class_='mw-parser-output')\n",
    "    keywords = set([l.text.lower() for l in main_block.findAll('a') if re.match(r'^(<a href=\"/wiki/)', str(l))])\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'python'\n",
    "\n",
    "p = wikipedia.page(keyword)\n",
    "if p is not None:\n",
    "    sents = collect_sents_from_wiki_page(p)\n",
    "    keywords = collect_keyword_from_wiki_page(p)\n",
    "    print('sentences collected')\n",
    "    my_write('%s.txt' % keyword, sents)\n",
    "    my_write('%s_kw.txt' % keyword, keywords)\n",
    "    df = filter_by_path(sents)\n",
    "    df.to_csv('%s_out.tsv' % keyword, sep='\\t', index=False)\n",
    "\n",
    "    dff = df[df.apply(lambda x: str(x['head']) in keywords and str(x['tail']) in keywords, axis=1)]\n",
    "    dff.to_csv('%s_out_f.tsv' % keyword, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-crafted analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_df = wiki_path_test_df[wiki_path_test_df['sim'] >= 0.0]\n",
    "\n",
    "def match_path_pattern(path:str):\n",
    "    for pp in patterns:\n",
    "        if exact_match(pp, path):\n",
    "            return pp\n",
    "    return ''\n",
    "\n",
    "wiki_test_df['pattern'] = wiki_test_df.apply(lambda x: match_path_pattern(x['path']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_path_result_sim_based(df:pd.DataFrame, paths:list):\n",
    "    summary_df = pd.DataFrame(columns=['path', 'cnt', 'ratio', 'avg_sim'])\n",
    "    for pp in paths:\n",
    "        sub_df = df[df['pattern'] == pp]\n",
    "        summary_df = summary_df.append({\n",
    "            'path' : pp,\n",
    "            'cnt' : len(sub_df),\n",
    "            'ratio' : len(sub_df) / len(df),\n",
    "            'avg_sim' : sum(sub_df['sim']) / len(sub_df) if len(sub_df) else 0\n",
    "        }, ignore_index=True)\n",
    "    summary_df = summary_df.append({\n",
    "        'path' : 'general',\n",
    "        'cnt' : len(df),\n",
    "        'ratio' : 1,\n",
    "        'avg_sim' : sum(df['sim']) / len(df) if len(df) else 0\n",
    "    }, ignore_index=True)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path_result_sim_based(wiki_test_df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_example_sent_for_pattern(df:pd.DataFrame, path:str, num:int=30, posfix:str='.dat'):\n",
    "    sub_df = df[df['pattern'] == path]\n",
    "    num = min(len(sub_df), num)\n",
    "    sub_df = sub_df[:num]\n",
    "    sub_df['sent'] = sub_df.apply(lambda x: note2line(x['sent'], posfix=posfix).strip(), axis=1)\n",
    "    return sub_df\n",
    "\n",
    "for patt in patterns:\n",
    "    temp_df = collect_example_sent_for_pattern(wiki_test_df, patt)\n",
    "    temp_df.to_csv('%s.tsv' % (patt[:10] if len(patt) >= 10 else patt), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_set = my_read_pickle('data/extract_wiki/triangles.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(triangle_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tri in enumerate(triangle_set):\n",
    "    print(tri)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i, tri in enumerate(triangle_set):\n",
    "    samples.append(find_triangle_with_node(single_sent_graph, *tri))\n",
    "    if i > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9496c91418be784f00ee6456e4343e8188c649322b68f201c83241a4029a42d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('FWD_py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
