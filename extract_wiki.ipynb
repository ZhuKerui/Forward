{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Sentences from Wikipedia\n",
    "+ This notebook is used for collecting sentences that tell relationship between two entities from wikipedia using some dependency path pattern\n",
    "+ **This notebook is fully valid under Owl3 machine (using the /scratch/data/wikipedia/full_text-2021-03-20 data)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki sub folder example: ../data/wikipedia/full_text-2021-03-20/BE\n",
      "save sub folder example: data/extract_wiki/wiki_sent_collect/BE\n",
      "wiki file example: ../data/wikipedia/full_text-2021-03-20/BE/wiki_00\n",
      "save sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00.dat\n",
      "save cooccur file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_co.dat\n",
      "save selected sentence file example: data/extract_wiki/wiki_sent_collect/BE/wiki_00_se.dat\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import wikipedia\n",
    "import os\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "from collections import Counter\n",
    "import bz2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from typing import List\n",
    "from nltk.corpus import stopwords\n",
    "self_define_stopwords = set(['-', ',', '.'])\n",
    "sw = set(stopwords.words('english'))\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "random.seed(0)\n",
    "import torch\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from tools.BasicUtils import my_read, my_write, my_read_pickle, my_write_pickle\n",
    "from tools.TextProcessing import (\n",
    "                my_sentence_tokenize,\n",
    "                my_sentence_tokenize, filter_specific_keywords, nlp, \n",
    "                exact_match\n",
    "                )\n",
    "\n",
    "from extract_wiki import (\n",
    "    wikipedia_entity_file, record_columns, \n",
    "    save_path, entity_occur_from_cooccur_file, entity_occur_from_selected_file, graph_file, single_sent_graph_file, \n",
    "    w2vec_dump_file, \n",
    "    w2vec_keyword2idx_file, \n",
    "    test_path, path_test_file, \n",
    "    path_pattern_count_file, \n",
    "    save_sub_folders, wiki_sub_folders, \n",
    "    wiki_files, save_sent_files, save_cooccur_files, save_selected_files, save_title_files, save_cooccur__files, \n",
    "    p, patterns, \n",
    "    note2line, line2note, process_file, filter_unrelated_from_df, cal_score_from_df, cal_freq_from_path, cal_freq_from_df, \n",
    "    feature_columns, feature_process, gen_pattern, gen_kw_from_wiki_ent, get_entity_page, load_pattern_freq, find_triangles, find_path_between_pair, \n",
    "    generate_sample, generate_second_level_sample, sample_to_neo4j\n",
    ")\n",
    "\n",
    "# Generate the save dir\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.mkdir(test_path)\n",
    "\n",
    "for save_dir in save_sub_folders:\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "# Get all files under wikipedia/full_text-2021-03-20\n",
    "\n",
    "print('wiki sub folder example:', wiki_sub_folders[0])\n",
    "print('save sub folder example:', save_sub_folders[0])\n",
    "print('wiki file example:', wiki_files[0])\n",
    "print('save sentence file example:', save_sent_files[0])\n",
    "print('save cooccur file example:', save_cooccur_files[0])\n",
    "print('save selected sentence file example:', save_selected_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] wikipedia2vec\n",
    "with bz2.open(w2vec_dump_file) as f_in:\n",
    "    w2vec = Wikipedia2Vec.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] wikipedia2vec\n",
    "\n",
    "# Find similar words or entities\n",
    "# ent1 = 'Python (programming language)'\n",
    "# w2vec.most_similar_by_vector(w2vec.get_entity_vector(ent1), 20)\n",
    "\n",
    "# Get similarity between two entities\n",
    "# ent1 = 'Data mining'\n",
    "# ent2 = 'Database system'\n",
    "# cosine_similarity(w2vec.get_entity_vector(ent1).reshape(1, -1), w2vec.get_entity_vector(ent2).reshape(1, -1))[0, 0]\n",
    "\n",
    "# Check the entity count and document count\n",
    "ent1 = 'Hidden Markov model'\n",
    "e = w2vec.get_entity(ent1)\n",
    "print(e.count)\n",
    "print(e.doc_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect sentences, entities, entities co-occurrances, titles from wikipedia dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roughly collect sentences, entity co-occurrances, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python extract_wiki.py collect_sent_and_cooccur (8 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect wikipedia entities\n",
    "wikipedia_entity = set()\n",
    "for f in tqdm.tqdm(save_title_files):\n",
    "    with open(f) as f_in:\n",
    "        wikipedia_entity.update(f_in.read().split('\\n'))\n",
    "print(len(wikipedia_entity))\n",
    "my_write(wikipedia_entity_file, list(wikipedia_entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wikipedia entities\n",
    "with open(wikipedia_entity_file) as f_in:\n",
    "    wikipedia_entity = set(f_in.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct entity mapping in co-occurrance files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec\n",
    "wikipedia_entity\n",
    "\n",
    "# Generate lower-cased entity to original entity mapping\n",
    "print('Generate lower-cased entity to original entity mapping')\n",
    "wikipedia_entity_low2orig_map = {}\n",
    "for ent in wikipedia_entity:\n",
    "    ent_low = ent.lower()\n",
    "    if ent_low not in wikipedia_entity_low2orig_map:\n",
    "        wikipedia_entity_low2orig_map[ent_low] = []\n",
    "    wikipedia_entity_low2orig_map[ent_low].append(ent)\n",
    "\n",
    "# Correct mapping\n",
    "print('Correct mapping')\n",
    "for i in tqdm.tqdm(range(len(save_cooccur_files))):\n",
    "    with open(save_cooccur_files[i]) as f_in:\n",
    "        new_file_lines = []\n",
    "        for line_idx, line in enumerate(f_in):\n",
    "            line = line.strip()\n",
    "            entities = line.split('\\t')\n",
    "            new_entities = []\n",
    "            for ent in entities:\n",
    "                if ent in wikipedia_entity:\n",
    "                    new_entities.append(ent)\n",
    "                else:\n",
    "                    ent_low = ent.lower()\n",
    "                    if ent_low in wikipedia_entity_low2orig_map:\n",
    "                        candidates = wikipedia_entity_low2orig_map[ent_low]\n",
    "                        if len(candidates) == 1:\n",
    "                            new_entities.append(candidates[0])\n",
    "                        else:\n",
    "                            note = line2note(save_cooccur_files[i], line_idx, '_co.dat')\n",
    "                            page_title = note2line(note, '_ti.dat').strip()\n",
    "                            try:\n",
    "                                page_ent_vec = w2vec.get_entity_vector(page_title)\n",
    "                            except:\n",
    "                                continue\n",
    "                            most_similar_idx, most_similar_val = -1, -1\n",
    "                            for candidate_idx, candidate_ent in enumerate(candidates):\n",
    "                                try:\n",
    "                                    candidate_vec = w2vec.get_entity_vector(candidate_ent)\n",
    "                                except:\n",
    "                                    continue\n",
    "                                similar_val = cosine_similarity(page_ent_vec.reshape(1, -1), candidate_vec.reshape(1, -1))[0,0]\n",
    "                                if similar_val > most_similar_val:\n",
    "                                    most_similar_val = similar_val\n",
    "                                    most_similar_idx = candidate_idx\n",
    "                            if most_similar_idx >= 0:\n",
    "                                new_entities.append(candidates[most_similar_idx])\n",
    "            new_file_lines.append('\\t'.join(new_entities))\n",
    "        my_write(save_cooccur__files[i], new_file_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test]\n",
    "lines = get_entity_page('Machine learning')\n",
    "sents = [note2line(note) for note in lines]\n",
    "occurs = [note2line(note, '_co_.dat') for note in lines]\n",
    "ori_occurs = [note2line(note, '_co.dat') for note in lines]\n",
    "my_write('sent_check.txt', sents)\n",
    "my_write('occur_check.txt', occurs)\n",
    "my_write('ori_occur_check.txt', ori_occurs)\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Not necessary] Mapping keyword mention to wikipedia2vec entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_entity\n",
    "w2vec\n",
    "\n",
    "w2vec_keyword2idx = {}\n",
    "\n",
    "for entity in tqdm.tqdm(wikipedia_entity):\n",
    "    w2vec_entity = w2vec.get_entity(entity)\n",
    "    if w2vec_entity is None:\n",
    "        continue\n",
    "    kw = gen_kw_from_wiki_ent(entity)\n",
    "    if kw not in w2vec_keyword2idx:\n",
    "        w2vec_keyword2idx[kw] = [w2vec_entity.index]\n",
    "    else:\n",
    "        if w2vec_entity.index not in w2vec_keyword2idx[kw]:\n",
    "            w2vec_keyword2idx[kw].append(w2vec_entity.index)\n",
    "w2vec_kws = filter_specific_keywords(list(w2vec_keyword2idx.keys()))\n",
    "filter_keyword_from_w2vec = set(w2vec_kws)\n",
    "w2vec_keyword2idx = {k:v for k, v in w2vec_keyword2idx.items() if k in filter_keyword_from_w2vec}\n",
    "my_write_pickle(w2vec_keyword2idx_file, w2vec_keyword2idx)\n",
    "len(w2vec_keyword2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] w2vec_keyword2idx\n",
    "w2vec_keyword2idx = my_read_pickle(w2vec_keyword2idx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] w2vec_keyword2idx\n",
    "kw = 'feature engineering'\n",
    "kw_in_mention = kw in w2vec_keyword2idx\n",
    "print(kw_in_mention)\n",
    "if kw_in_mention:\n",
    "    for idx in w2vec_keyword2idx[kw]:\n",
    "        print(w2vec.dictionary.get_item_by_index(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Test] Collect entity occurrance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the entity occurrance dict from co-occurrance info [collect_ent_occur_from_cooccur]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Preparation] Collect dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect pattern frequency counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] Collect sample data using general wikipedia2vec keywords and wiki sent files\n",
    "wiki_path_test_df = pd.concat([pd.DataFrame(process_file(save_sent_files[file_idx], save_cooccur__files[file_idx], w2vec)) for file_idx in range(8)], ignore_index=True)\n",
    "wiki_path_test_df.to_csv(path_test_file, sep='\\t', columns=feature_columns, index=False)\n",
    "print(len(wiki_path_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Load path test data (pd.DataFrame)\n",
    "wiki_path_test_df = pd.read_csv(open(path_test_file), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] Pattern frequency generation\n",
    "\n",
    "sub_df = wiki_path_test_df[wiki_path_test_df['sim'] > 0.5]\n",
    "\n",
    "sub_df = sub_df.assign(pick = sub_df.apply(lambda x: 1 if 'nsubj' in x['dep_path'] else 0, axis=1))\n",
    "sub_df = sub_df[sub_df['pick'] > 0]\n",
    "\n",
    "c = Counter(sub_df['pattern'].to_list())\n",
    "\n",
    "my_write_pickle(path_pattern_count_file, c)\n",
    "\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] cal_freq function\n",
    "c, log_max_cnt = load_pattern_freq(path_pattern_count_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] cal_freq function\n",
    "cal_freq_from_path('i_nsubj prep pobj prep pobj', c, log_max_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate data collection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] Collect sample data using general wikipedia2vec keywords and wiki sent files\n",
    "sub_df = filter_unrelated_from_df(wiki_path_test_df, 0.4)\n",
    "sub_df = cal_freq_from_df(sub_df, c, log_max_cnt)\n",
    "sub_df = cal_score_from_df(sub_df, 0.5, 0.25, 0.25)\n",
    "sub_df = sub_df[sub_df['score'] > 0.5]\n",
    "sub_df = sub_df.assign(sent = sub_df.apply(lambda x: note2line(x['sent']).strip(), axis=1))\n",
    "sub_df.to_csv('full_phrase_check.tsv', columns = ['sent', 'kw1', 'kw1_recall', 'kw1_full_span', 'kw2', 'kw2_recall', 'kw2_full_span'], sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Create] collect dataset [collect_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate entity occurrance from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the entity occurance ['collect_ent_occur_from_selected']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check entity occurrance in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] entity occur dict which has occurance record for all entities in selected sentences\n",
    "entity_occur_from_selected = my_read_pickle(entity_occur_from_selected_file)\n",
    "\n",
    "# Demo check co-occur of two entities in selected sentences\n",
    "def get_selected_record(entity_dict:dict, ent1:str, ent2:str):\n",
    "    kw1_occur = entity_dict.get(ent1)\n",
    "    kw2_occur = entity_dict.get(ent2)\n",
    "    if not kw1_occur or not kw2_occur:\n",
    "        return None\n",
    "    co_occur = kw1_occur & kw2_occur\n",
    "    data = []\n",
    "    for occur in co_occur:\n",
    "        record = note2line(occur, '_se.dat').strip().split('\\t')\n",
    "        sent = note2line(record[7]).strip()\n",
    "        data_dict = {record_columns[i] : record[i] for i in range(len(record))}\n",
    "        data_dict['sent'] = sent\n",
    "        data.append(data_dict)\n",
    "    df = pd.DataFrame(data = data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] entity occur dict\n",
    "\n",
    "ent1 = 'Artificial intelligence'\n",
    "# Check the existance of an entity\n",
    "print(ent1 in entity_occur_from_selected)\n",
    "\n",
    "# Check the sentences where an entity appear\n",
    "# for note in entity_occur_from_selected[ent1]:\n",
    "#     print(note2line(note2line(note, '_se.dat').split('\\t')[7]).strip())\n",
    "\n",
    "# Check the records of two entities\n",
    "ent2 = 'Natural language processing'\n",
    "df = get_selected_record(entity_occur_from_selected, ent1, ent2)\n",
    "if df is not None:\n",
    "    print(len(df))\n",
    "    df.to_csv('sents.tsv', columns=['score'] + feature_columns + ['pattern', 'pattern_freq'], sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Prepration] Sentence-edged Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the graph ['generate_graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate single sentence graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate single sentence graph ['generate_single_sent_graph']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the basic properties of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Graph\n",
    "graph = my_read_pickle(graph_file)\n",
    "\n",
    "print('num of nodes:', len(graph.nodes))\n",
    "print('num of edges:', len(graph.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Test] graph\n",
    "ent1 = 'Data mining'\n",
    "# Check the neighbours of an entity\n",
    "# list(graph.neighbors(ent1))\n",
    "\n",
    "# Check the edges of two entities\n",
    "graph.edges[ent1, 'Data fusion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sentence from file\n",
    "note2line('AC:32:2390')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2neig_cnt = {node : len(list(graph.neighbors(node))) for node in graph.nodes.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neig_cnt = [v for v in node2neig_cnt.values() if v < 20]\n",
    "plt.title('num of nodes vs num of neighbors each node')\n",
    "plt.hist(neig_cnt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2triangle_num = nx.triangles(graph)\n",
    "print('num of triangles:', sum(node2triangle_num.values()) / 3)\n",
    "plt.title('num of nodes vs num of triangles each node')\n",
    "plt.hist([v for v in node2triangle_num.values() if v >= 1 and v < 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training data from single sentence graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 1294537\n",
      "number of edges: 3004240\n"
     ]
    }
   ],
   "source": [
    "# [Load] Single sentence graph\n",
    "graph = my_read_pickle(graph_file)\n",
    "print('number of nodes:', graph.number_of_nodes())\n",
    "print('number of edges:', graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data of level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "ent1 = 'Machine learning'\n",
    "ent2 = 'Artificial intelligence'\n",
    "sample = generate_sample(graph, ent1, ent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pair': ('Machine learning', 'Artificial intelligence'),\n",
       " 'entity': ['Deep learning',\n",
       "  'Computer science',\n",
       "  'Software',\n",
       "  'Google',\n",
       "  'Intelligent agent',\n",
       "  'Fuzzy logic',\n",
       "  'Software engineering',\n",
       "  'Computer vision',\n",
       "  'Sepp Hochreiter',\n",
       "  'Neural network software',\n",
       "  'Computer program',\n",
       "  'Library (computing)',\n",
       "  'Artificial neural network',\n",
       "  'Neural Designer',\n",
       "  'Inductive logic programming',\n",
       "  'Genetic algorithm',\n",
       "  'Artificial intelligence',\n",
       "  'Assistive technology',\n",
       "  'DeepMind',\n",
       "  'Natural language',\n",
       "  'Algorithm',\n",
       "  'Natural language processing',\n",
       "  'Statistical relational learning',\n",
       "  'IBM',\n",
       "  'Machine learning',\n",
       "  'Markov random field',\n",
       "  'Logic programming'],\n",
       " 'target': 'as a scientific endeavor , machine learning grew out of the quest for artificial intelligence .',\n",
       " 'source': ['in march 2018 , google announced tensorflow.js version 1.0 for machine learning in javascript .',\n",
       "  'in february 2017 , ibm announced the first machine learning hub in silicon valley to share expertise and teach company about machine learning and data science',\n",
       "  'natural language processing ( nlp ) is a subfield of linguistics , computer science , and artificial intelligence concerned with the interaction between computer and human language , in particular how to program computer to process and analyze large amount of natural language data .',\n",
       "  'in artificial intelligence , especially computer vision and artificial neural network , a hard sigmoid is non-smooth function used in place of a sigmoid function .',\n",
       "  'sepp hochreiter has made numerous contribution in the field of machine learning , deep learning and bioinformatics .',\n",
       "  'the genetic algorithm is another machine learning technique used for stylometry .',\n",
       "  'in 2016 , deepmind turned its artificial intelligence to protein folding , one of the toughest problem in science .',\n",
       "  'in artificial intelligence , an intelligent agent ( ia ) refers to an autonomous entity which acts , directing its activity towards achieving goal ( i.e .',\n",
       "  'inductive logic programming has nowadays bloomed as a subfield of artificial intelligence and machine learning which uses logic programming as a uniform representation for example , background knowledge and hypothesis .',\n",
       "  'enterprise ai software applies artificial intelligence method , such as machine learning and neural network , to solve complex analytical problem in commerce , industry , and government .',\n",
       "  'the system software utilizes artificial intelligence technology .',\n",
       "  \"since the so-called `` statistical revolution '' in the late 1980s and mid-1990s , much natural language processing research has relied heavily on machine learning .\",\n",
       "  'capability of these intelligent agent include knowledge sharing , machine learning , data mining , and automated inference .',\n",
       "  'deep learning is a subset of machine learning which focuses heavily on the use of artificial neural network ( ann ) that learn to solve complex task .',\n",
       "  'markov random field find application in a variety of field , ranging from computer graphic to computer vision , machine learning or computational biology .',\n",
       "  'trax ’ s computer vision technology uses artificial intelligence , fine-grained image recognition , and machine learning engine to convert store image into shelf insight .',\n",
       "  'fuzzy logic has made significant contribution in the field of machine learning and data mining .',\n",
       "  \"`` artificial intelligence '' ( `` ai '' ) is the sub intelligence exhibited by machine or software , and the branch of computer science that develops machine and software with animal-like intelligence .\",\n",
       "  'in the 2010s , representation learning and deep neural network-style machine learning method became widespread in natural language processing , due in part to a flurry of result showing that such technique can achieve state-of-the-art result in many natural language task , for example in language modeling , parsing , and many others .',\n",
       "  'deepmind sponsor two chair of machine learning :',\n",
       "  'in the domain of artificial intelligence , a markov random field is used to model various low- to mid-level task in image processing and computer vision .',\n",
       "  'logic programming associate ( lpa ) is a company specializing in logic programming and artificial intelligence software .',\n",
       "  'sepp hochreiter is also a founding director of institute of advanced research in artificial intelligence ( iarai )',\n",
       "  'the shuffled frog leaping algorithm is an optimization algorithm used in artificial intelligence .',\n",
       "  'google brain is a deep learning artificial intelligence research team at google .',\n",
       "  'artificial neural network ( ann ) is a subfield of the research area machine learning .',\n",
       "  'in computer science , and more particularly in artificial intelligence these mean field type genetic algorithm are used as random search heuristic that mimic the process of evolution to generate useful solution to complex optimization problem .',\n",
       "  'more fundamentally , many algorithm used in machine learning are not easily explainable .',\n",
       "  'deep learning has transformed many important subfields of artificial intelligence , including computer vision , speech recognition , natural language processing and others .',\n",
       "  'fuzzy logic has been applied to many field , from control theory to artificial intelligence .',\n",
       "  'many computer vision technique also incorporate form of machine learning , and have been applied on various video game .',\n",
       "  'in 2017 , ibm and mit established a new joint research venture in artificial intelligence .',\n",
       "  'machine learning is a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence .'],\n",
       " 'triple': [[{'e1': 0,\n",
       "    'e2': 16,\n",
       "    'sent': 28,\n",
       "    'score': 0.675836849870984,\n",
       "    'span': ('(0, 1)', '(8, 9)'),\n",
       "    'pid': 0},\n",
       "   {'e1': 24,\n",
       "    'e2': 0,\n",
       "    'sent': 13,\n",
       "    'score': 0.7023659666630779,\n",
       "    'span': ('(6, 7)', '(0, 1)'),\n",
       "    'pid': 0}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 1,\n",
       "    'sent': 32,\n",
       "    'score': 0.708780933515503,\n",
       "    'span': ('(9, 10)', '(0, 1)'),\n",
       "    'pid': 1},\n",
       "   {'e1': 1,\n",
       "    'e2': 16,\n",
       "    'sent': 17,\n",
       "    'score': 0.6551316948823923,\n",
       "    'span': ('(25, 26)', '(2, 3)'),\n",
       "    'pid': 1}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 2,\n",
       "    'sent': 9,\n",
       "    'score': 0.6202381583814122,\n",
       "    'span': ('(2, 2)', '(10, 11)'),\n",
       "    'pid': 2},\n",
       "   {'e1': 2,\n",
       "    'e2': 16,\n",
       "    'sent': 10,\n",
       "    'score': 0.7915515706805699,\n",
       "    'span': ('(2, 2)', '(4, 5)'),\n",
       "    'pid': 2}],\n",
       "  [{'e1': 3,\n",
       "    'e2': 16,\n",
       "    'sent': 24,\n",
       "    'score': 0.7190521700446796,\n",
       "    'span': ('(0, 0)', '(6, 7)'),\n",
       "    'pid': 3},\n",
       "   {'e1': 24,\n",
       "    'e2': 3,\n",
       "    'sent': 0,\n",
       "    'score': 0.7313666724769176,\n",
       "    'span': ('(4, 4)', '(10, 11)'),\n",
       "    'pid': 3}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 4,\n",
       "    'sent': 12,\n",
       "    'score': 0.6180190321170231,\n",
       "    'span': ('(3, 4)', '(9, 10)'),\n",
       "    'pid': 4},\n",
       "   {'e1': 4,\n",
       "    'e2': 16,\n",
       "    'sent': 7,\n",
       "    'score': 0.6865890479690392,\n",
       "    'span': ('(5, 6)', '(1, 2)'),\n",
       "    'pid': 4}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 5,\n",
       "    'sent': 16,\n",
       "    'score': 0.6450806315947157,\n",
       "    'span': ('(10, 11)', '(0, 1)'),\n",
       "    'pid': 5},\n",
       "   {'e1': 5,\n",
       "    'e2': 16,\n",
       "    'sent': 29,\n",
       "    'score': 0.7641430867167776,\n",
       "    'span': ('(0, 1)', '(13, 14)'),\n",
       "    'pid': 5}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 7,\n",
       "    'sent': 30,\n",
       "    'score': 0.7133761867745221,\n",
       "    'span': ('(8, 9)', '(1, 2)'),\n",
       "    'pid': 6},\n",
       "   {'e1': 7,\n",
       "    'e2': 16,\n",
       "    'sent': 15,\n",
       "    'score': 0.637753214952814,\n",
       "    'span': ('(7, 8)', '(3, 4)'),\n",
       "    'pid': 6}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 8,\n",
       "    'sent': 4,\n",
       "    'score': 0.6263626774539954,\n",
       "    'span': ('(10, 11)', '(0, 1)'),\n",
       "    'pid': 7},\n",
       "   {'e1': 8,\n",
       "    'e2': 16,\n",
       "    'sent': 22,\n",
       "    'score': 0.7664140325400282,\n",
       "    'span': ('(0, 1)', '(13, 14)'),\n",
       "    'pid': 7}],\n",
       "  [{'e1': 12,\n",
       "    'e2': 16,\n",
       "    'sent': 3,\n",
       "    'score': 0.6803749333171201,\n",
       "    'span': ('(1, 2)', '(5, 6)'),\n",
       "    'pid': 8},\n",
       "   {'e1': 24,\n",
       "    'e2': 12,\n",
       "    'sent': 25,\n",
       "    'score': 0.8364572947394484,\n",
       "    'span': ('(13, 14)', '(0, 2)'),\n",
       "    'pid': 8}],\n",
       "  [{'e1': 15,\n",
       "    'e2': 16,\n",
       "    'sent': 26,\n",
       "    'score': 0.6012598998740435,\n",
       "    'span': ('(14, 15)', '(8, 9)'),\n",
       "    'pid': 9},\n",
       "   {'e1': 24,\n",
       "    'e2': 15,\n",
       "    'sent': 5,\n",
       "    'score': 0.7738752399027572,\n",
       "    'span': ('(1, 2)', '(5, 6)'),\n",
       "    'pid': 9}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 18,\n",
       "    'sent': 19,\n",
       "    'score': 0.8558260156859331,\n",
       "    'span': ('(5, 6)', '(0, 0)'),\n",
       "    'pid': 10},\n",
       "   {'e1': 18,\n",
       "    'e2': 16,\n",
       "    'sent': 6,\n",
       "    'score': 0.6498587942373536,\n",
       "    'span': ('(6, 7)', '(3, 3)'),\n",
       "    'pid': 10}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 19,\n",
       "    'sent': 18,\n",
       "    'score': 0.6093265378164741,\n",
       "    'span': ('(12, 13)', '(18, 19)'),\n",
       "    'pid': 11},\n",
       "   {'e1': 19,\n",
       "    'e2': 16,\n",
       "    'sent': 2,\n",
       "    'score': 0.657836822121304,\n",
       "    'span': ('(16, 17)', '(0, 1)'),\n",
       "    'pid': 11}],\n",
       "  [{'e1': 20,\n",
       "    'e2': 16,\n",
       "    'sent': 23,\n",
       "    'score': 0.6687090292177155,\n",
       "    'span': ('(4, 4)', '(11, 12)'),\n",
       "    'pid': 12},\n",
       "   {'e1': 24,\n",
       "    'e2': 20,\n",
       "    'sent': 27,\n",
       "    'score': 0.7456375843657445,\n",
       "    'span': ('(4, 4)', '(7, 8)'),\n",
       "    'pid': 12}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 21,\n",
       "    'sent': 11,\n",
       "    'score': 0.7203112040945833,\n",
       "    'span': ('(26, 27)', '(18, 20)'),\n",
       "    'pid': 13},\n",
       "   {'e1': 21,\n",
       "    'e2': 16,\n",
       "    'sent': 2,\n",
       "    'score': 0.681985795084369,\n",
       "    'span': ('(16, 17)', '(0, 2)'),\n",
       "    'pid': 13}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 23,\n",
       "    'sent': 1,\n",
       "    'score': 0.63789807031702,\n",
       "    'span': ('(8, 9)', '(4, 4)'),\n",
       "    'pid': 14},\n",
       "   {'e1': 23,\n",
       "    'e2': 16,\n",
       "    'sent': 31,\n",
       "    'score': 0.7196610286711704,\n",
       "    'span': ('(3, 3)', '(13, 14)'),\n",
       "    'pid': 14}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 25,\n",
       "    'sent': 14,\n",
       "    'score': 0.6223320381459048,\n",
       "    'span': ('(19, 20)', '(0, 2)'),\n",
       "    'pid': 15},\n",
       "   {'e1': 25,\n",
       "    'e2': 16,\n",
       "    'sent': 20,\n",
       "    'score': 0.6782554859438923,\n",
       "    'span': ('(8, 10)', '(4, 5)'),\n",
       "    'pid': 15}],\n",
       "  [{'e1': 24,\n",
       "    'e2': 26,\n",
       "    'sent': 8,\n",
       "    'score': 0.6706825908623096,\n",
       "    'span': ('(13, 14)', '(1, 2)'),\n",
       "    'pid': 16},\n",
       "   {'e1': 26,\n",
       "    'e2': 16,\n",
       "    'sent': 21,\n",
       "    'score': 0.650631190780266,\n",
       "    'span': ('(0, 1)', '(14, 15)'),\n",
       "    'pid': 16}]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_neo4j(sample:dict):\n",
    "    cmd = ['MATCH (n) DETACH DELETE (n);']\n",
    "    for ent in sample['pair']:\n",
    "        cmd.append('CREATE (:ENT:TARGET {ent:\"%s\"});' % ent)\n",
    "    for ent in sample['entity']:\n",
    "        if ent not in sample['pair']:\n",
    "            cmd.append('CREATE (:ENT:INTERMEDIA {ent:\"%s\"});' % ent)\n",
    "    for path in sample['triple']:\n",
    "        for tri in path:\n",
    "            ent1 = sample['entity'][tri['e1']]\n",
    "            ent2 = sample['entity'][tri['e2']]\n",
    "            sent = sample['source'][tri['sent']]\n",
    "            score = tri['score']\n",
    "            cmd.append('MATCH (ent1:ENT {ent:\"%s\"}), (ent2:ENT {ent:\"%s\"}) CREATE (ent1)-[:Sent {sent:\"%s\", pair:\"%s <-> %s\", score:%.3f}]->(ent2);' % (ent1, ent2, sent, ent1, ent2, score))\n",
    "    cmd.append('MATCH (ent1:ENT {ent:\"%s\"}), (ent2:ENT {ent:\"%s\"}) CREATE (ent1)-[:OUT {sent:\"%s\", pair:\"%s <-> %s\"}]->(ent2);' % (*sample['pair'], sample['target'], *sample['pair']))\n",
    "    print('\\n'.join(cmd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH (n) DETACH DELETE (n);\n",
      "CREATE (:ENT:TARGET {ent:\"Machine learning\"});\n",
      "CREATE (:ENT:TARGET {ent:\"Artificial intelligence\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Deep learning\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Computer science\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Software\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Google\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Intelligent agent\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Fuzzy logic\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Software engineering\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Computer vision\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Sepp Hochreiter\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Neural network software\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Computer program\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Library (computing)\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Artificial neural network\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Neural Designer\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Inductive logic programming\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Genetic algorithm\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Assistive technology\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"DeepMind\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Natural language\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Algorithm\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Natural language processing\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Statistical relational learning\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"IBM\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Markov random field\"});\n",
      "CREATE (:ENT:INTERMEDIA {ent:\"Logic programming\"});\n",
      "MATCH (ent1:ENT {ent:\"Deep learning\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"deep learning has transformed many important subfields of artificial intelligence , including computer vision , speech recognition , natural language processing and others .\", pair:\"Deep learning <-> Artificial intelligence\", score:0.676}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Deep learning\"}) CREATE (ent1)-[:Sent {sent:\"deep learning is a subset of machine learning which focuses heavily on the use of artificial neural network ( ann ) that learn to solve complex task .\", pair:\"Machine learning <-> Deep learning\", score:0.702}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Computer science\"}) CREATE (ent1)-[:Sent {sent:\"machine learning is a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence .\", pair:\"Machine learning <-> Computer science\", score:0.709}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Computer science\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"`` artificial intelligence '' ( `` ai '' ) is the sub intelligence exhibited by machine or software , and the branch of computer science that develops machine and software with animal-like intelligence .\", pair:\"Computer science <-> Artificial intelligence\", score:0.655}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Software\"}) CREATE (ent1)-[:Sent {sent:\"enterprise ai software applies artificial intelligence method , such as machine learning and neural network , to solve complex analytical problem in commerce , industry , and government .\", pair:\"Machine learning <-> Software\", score:0.620}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Software\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"the system software utilizes artificial intelligence technology .\", pair:\"Software <-> Artificial intelligence\", score:0.792}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Google\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"google brain is a deep learning artificial intelligence research team at google .\", pair:\"Google <-> Artificial intelligence\", score:0.719}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Google\"}) CREATE (ent1)-[:Sent {sent:\"in march 2018 , google announced tensorflow.js version 1.0 for machine learning in javascript .\", pair:\"Machine learning <-> Google\", score:0.731}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Intelligent agent\"}) CREATE (ent1)-[:Sent {sent:\"capability of these intelligent agent include knowledge sharing , machine learning , data mining , and automated inference .\", pair:\"Machine learning <-> Intelligent agent\", score:0.618}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Intelligent agent\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"in artificial intelligence , an intelligent agent ( ia ) refers to an autonomous entity which acts , directing its activity towards achieving goal ( i.e .\", pair:\"Intelligent agent <-> Artificial intelligence\", score:0.687}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Fuzzy logic\"}) CREATE (ent1)-[:Sent {sent:\"fuzzy logic has made significant contribution in the field of machine learning and data mining .\", pair:\"Machine learning <-> Fuzzy logic\", score:0.645}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Fuzzy logic\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"fuzzy logic has been applied to many field , from control theory to artificial intelligence .\", pair:\"Fuzzy logic <-> Artificial intelligence\", score:0.764}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Computer vision\"}) CREATE (ent1)-[:Sent {sent:\"many computer vision technique also incorporate form of machine learning , and have been applied on various video game .\", pair:\"Machine learning <-> Computer vision\", score:0.713}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Computer vision\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"trax ’ s computer vision technology uses artificial intelligence , fine-grained image recognition , and machine learning engine to convert store image into shelf insight .\", pair:\"Computer vision <-> Artificial intelligence\", score:0.638}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Sepp Hochreiter\"}) CREATE (ent1)-[:Sent {sent:\"sepp hochreiter has made numerous contribution in the field of machine learning , deep learning and bioinformatics .\", pair:\"Machine learning <-> Sepp Hochreiter\", score:0.626}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Sepp Hochreiter\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"sepp hochreiter is also a founding director of institute of advanced research in artificial intelligence ( iarai )\", pair:\"Sepp Hochreiter <-> Artificial intelligence\", score:0.766}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Artificial neural network\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"in artificial intelligence , especially computer vision and artificial neural network , a hard sigmoid is non-smooth function used in place of a sigmoid function .\", pair:\"Artificial neural network <-> Artificial intelligence\", score:0.680}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Artificial neural network\"}) CREATE (ent1)-[:Sent {sent:\"artificial neural network ( ann ) is a subfield of the research area machine learning .\", pair:\"Machine learning <-> Artificial neural network\", score:0.836}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Genetic algorithm\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"in computer science , and more particularly in artificial intelligence these mean field type genetic algorithm are used as random search heuristic that mimic the process of evolution to generate useful solution to complex optimization problem .\", pair:\"Genetic algorithm <-> Artificial intelligence\", score:0.601}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Genetic algorithm\"}) CREATE (ent1)-[:Sent {sent:\"the genetic algorithm is another machine learning technique used for stylometry .\", pair:\"Machine learning <-> Genetic algorithm\", score:0.774}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"DeepMind\"}) CREATE (ent1)-[:Sent {sent:\"deepmind sponsor two chair of machine learning :\", pair:\"Machine learning <-> DeepMind\", score:0.856}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"DeepMind\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"in 2016 , deepmind turned its artificial intelligence to protein folding , one of the toughest problem in science .\", pair:\"DeepMind <-> Artificial intelligence\", score:0.650}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Natural language\"}) CREATE (ent1)-[:Sent {sent:\"in the 2010s , representation learning and deep neural network-style machine learning method became widespread in natural language processing , due in part to a flurry of result showing that such technique can achieve state-of-the-art result in many natural language task , for example in language modeling , parsing , and many others .\", pair:\"Machine learning <-> Natural language\", score:0.609}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Natural language\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"natural language processing ( nlp ) is a subfield of linguistics , computer science , and artificial intelligence concerned with the interaction between computer and human language , in particular how to program computer to process and analyze large amount of natural language data .\", pair:\"Natural language <-> Artificial intelligence\", score:0.658}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Algorithm\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"the shuffled frog leaping algorithm is an optimization algorithm used in artificial intelligence .\", pair:\"Algorithm <-> Artificial intelligence\", score:0.669}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Algorithm\"}) CREATE (ent1)-[:Sent {sent:\"more fundamentally , many algorithm used in machine learning are not easily explainable .\", pair:\"Machine learning <-> Algorithm\", score:0.746}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Natural language processing\"}) CREATE (ent1)-[:Sent {sent:\"since the so-called `` statistical revolution '' in the late 1980s and mid-1990s , much natural language processing research has relied heavily on machine learning .\", pair:\"Machine learning <-> Natural language processing\", score:0.720}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Natural language processing\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"natural language processing ( nlp ) is a subfield of linguistics , computer science , and artificial intelligence concerned with the interaction between computer and human language , in particular how to program computer to process and analyze large amount of natural language data .\", pair:\"Natural language processing <-> Artificial intelligence\", score:0.682}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"IBM\"}) CREATE (ent1)-[:Sent {sent:\"in february 2017 , ibm announced the first machine learning hub in silicon valley to share expertise and teach company about machine learning and data science\", pair:\"Machine learning <-> IBM\", score:0.638}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"IBM\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"in 2017 , ibm and mit established a new joint research venture in artificial intelligence .\", pair:\"IBM <-> Artificial intelligence\", score:0.720}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Markov random field\"}) CREATE (ent1)-[:Sent {sent:\"markov random field find application in a variety of field , ranging from computer graphic to computer vision , machine learning or computational biology .\", pair:\"Machine learning <-> Markov random field\", score:0.622}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Markov random field\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"in the domain of artificial intelligence , a markov random field is used to model various low- to mid-level task in image processing and computer vision .\", pair:\"Markov random field <-> Artificial intelligence\", score:0.678}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Logic programming\"}) CREATE (ent1)-[:Sent {sent:\"inductive logic programming has nowadays bloomed as a subfield of artificial intelligence and machine learning which uses logic programming as a uniform representation for example , background knowledge and hypothesis .\", pair:\"Machine learning <-> Logic programming\", score:0.671}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Logic programming\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:Sent {sent:\"logic programming associate ( lpa ) is a company specializing in logic programming and artificial intelligence software .\", pair:\"Logic programming <-> Artificial intelligence\", score:0.651}]->(ent2);\n",
      "MATCH (ent1:ENT {ent:\"Machine learning\"}), (ent2:ENT {ent:\"Artificial intelligence\"}) CREATE (ent1)-[:OUT {sent:\"as a scientific endeavor , machine learning grew out of the quest for artificial intelligence .\", pair:\"Machine learning <-> Artificial intelligence\"}]->(ent2);\n"
     ]
    }
   ],
   "source": [
    "sample_to_neo4j(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data of level 1 [collect_one_hop_sample_from_single_sent_graph] (5 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107684"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dataset_level_1.json') as f_in:\n",
    "    a = json.load(f_in)\n",
    "items = [item for item in a if len(item['source']) > 1]\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(items)\n",
    "train_ratio = 0.85\n",
    "valid_ratio = 0.05\n",
    "training_data = items[:int(len(items)*train_ratio)]\n",
    "valid_data = items[int(len(items)*train_ratio):int(len(items)*(train_ratio+valid_ratio))]\n",
    "test_data = items[int(len(items)*(train_ratio+valid_ratio)):]\n",
    "with open('train.json', 'w') as f_out:\n",
    "    json.dump(training_data, f_out)\n",
    "with open('dev.json', 'w') as f_out:\n",
    "    json.dump(valid_data, f_out)\n",
    "with open('test.json', 'w') as f_out:\n",
    "    json.dump(test_data, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['many christian church holding to a doctrine of the real presence of christ in the eucharist ( for example , catholic , eastern orthodox , lutheran , moravian , anglican , methodist , oriental orthodox , reformed , and irvingian ) reserve to ordained clergy the function of consecrating the eucharist , but not necessarily that of distributing the element to communicant .',\n",
       " 'in the final part of the discourse ( ) jesus prays for his follower and the coming christian church .',\n",
       " 'in christian theology the name of god has always had much deeper meaning and significance than being just a label or designator .',\n",
       " 'in the middle age , satan played a minimal role in christian theology and was used as a comic relief figure in mystery play .',\n",
       " 'in christian theology , the real presence of christ in the eucharist is the doctrine that jesus is present in the eucharist , not merely symbolically or metaphorically .',\n",
       " 'many christian view the new jerusalem as a current reality , that the new jerusalem is the consummation of the body of christ , the christian church and that christians already take part in membership of both the heavenly jerusalem and the earthly church in a kind of dual citizenship .',\n",
       " 'his theology originated in the view that god speaks to us through the christian church today and not just through the bible .',\n",
       " 'we have to admit that satan can also work through the structure of the christian church . ”',\n",
       " 'in christian theology , jesus is sometimes referred to as a redeemer .',\n",
       " 'john of patmos describes the new jerusalem in the book of revelation in the christian bible , and so the new jerusalem holds an important place in christian eschatology and christian mysticism , and has also influenced christian philosophy and christian theology .']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_len_count = []\n",
    "for data in training_data:\n",
    "    sents = data['source']\n",
    "    sent_len_count.extend([len(sent.split()) for sent in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(sent_len_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(x[x<100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "529554"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_len_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_to_neo4j(items[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data of level 2 from level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.json') as f_in:\n",
    "    sample = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_sample = generate_second_level_sample(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph4nlp.pytorch.data import GraphData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_labels = list(nlp.get_pipe(\"parser\").labels)\n",
    "dep_labels.extend(['i_'+dep for dep in dep_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GraphData()\n",
    "is_rel = []\n",
    "is_entity = []\n",
    "\n",
    "g.add_nodes(1)\n",
    "is_rel.append(0)\n",
    "is_entity.append(0)\n",
    "\n",
    "for src in second_level_sample['sources']:\n",
    "    pair = src['pair']\n",
    "    sent_tokens = src['sent']\n",
    "    \n",
    "    label_list = []\n",
    "    label_list.extend(sent_tokens)\n",
    "    token_num = len(sent_tokens)\n",
    "    start_node = g.get_node_num()\n",
    "    g.add_nodes(token_num)\n",
    "    is_rel.extend([0]*token_num)\n",
    "    is_entity.extend([0]*token_num)\n",
    "    is_entity[pair[0]+start_node] = 1\n",
    "    is_entity[pair[1]+start_node] = 1\n",
    "    \n",
    "    label_list.extend(['ROOT', 'ROOT', 'i_ROOT', 'i_ROOT'])\n",
    "    rel_start_node = start_node + token_num\n",
    "    g.add_nodes(4)\n",
    "    is_rel.extend([1]*4)\n",
    "    is_entity.extend([0]*4)\n",
    "    g.add_edges([0, 0], [rel_start_node, rel_start_node+1])\n",
    "    g.add_edges([rel_start_node, rel_start_node+1], [pair[0]+start_node, pair[1]+start_node])\n",
    "    g.add_edges([pair[0]+start_node, pair[1]+start_node], [rel_start_node+2, rel_start_node+3])\n",
    "    g.add_edges([rel_start_node+2, rel_start_node+3], [0, 0])\n",
    "    \n",
    "    rel_start_node += 4\n",
    "    triples = src['graph']\n",
    "    rel_num = len(triples)\n",
    "    is_rel.extend([1]*rel_num)\n",
    "    is_entity.extend([0]*rel_num)\n",
    "    g.add_nodes(rel_num)\n",
    "    for rel_idx, (tok_1, tok_2, rel) in enumerate(triples):\n",
    "        g.add_edges([tok_1+start_node, rel_idx+rel_start_node], [rel_idx+rel_start_node, tok_2+start_node])\n",
    "        label_list.append(rel)\n",
    "    for i, label in enumerate(label_list):\n",
    "        g.node_attributes[i+start_node]['label'] = label\n",
    "g.node_features['is_rel'] = torch.BoolTensor(is_rel)\n",
    "g.node_features['is_entity'] = torch.BoolTensor(is_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.get_edge_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_triangle_with_node(graph:nx.Graph, first_node:str, second_node:str='', third_node:str=''):\n",
    "    triangles = list(find_triangles(graph, first_node))\n",
    "    triangles.sort(key=lambda x: x[1])\n",
    "    triangle_with_sents = []\n",
    "    n_seen = set()\n",
    "    for n1, n2, n3 in triangles:\n",
    "        if second_node and n2 != second_node and n3 != second_node:\n",
    "            continue\n",
    "        if third_node and n2 != third_node and n3 != third_node:\n",
    "            continue\n",
    "        if n2 not in n_seen:\n",
    "            n_seen.add(n2)\n",
    "            triangle_with_sents.append((n1, note2line(graph.get_edge_data(n1, n2)['note']).strip(), n2, graph.get_edge_data(n1, n2)['score']))\n",
    "        if n3 not in n_seen:\n",
    "            n_seen.add(n3)\n",
    "            triangle_with_sents.append((n1, note2line(graph.get_edge_data(n1, n3)['note']).strip(), n3, graph.get_edge_data(n1, n3)['score']))\n",
    "        triangle_with_sents.append((n2, note2line(graph.get_edge_data(n3, n2)['note']).strip(), n3, graph.get_edge_data(n3, n2)['score']))\n",
    "    return triangle_with_sents\n",
    "\n",
    "\n",
    "def isf(w:str, D:int, counters:List[Counter]):\n",
    "    return math.log(D * 1.0 / sum([1 if w in sent else 0 for sent in counters]))\n",
    "\n",
    "\n",
    "def do_pagerank(sents:List[str]):\n",
    "    # Remove stop words\n",
    "    clean_sents = [[token for token in sent.split() if token not in sw and token not in self_define_stopwords] for sent in sents]\n",
    "\n",
    "    # Generate word counters\n",
    "    counters = [Counter(sent) for sent in clean_sents]\n",
    "\n",
    "    # Build similarity matrix\n",
    "    D = len(clean_sents)\n",
    "    sim_matrix = np.zeros((D, D))\n",
    "    part_list = [math.sqrt(sum([(sent[w] * isf(w, D, counters)) ** 2 for w in sent])) for sent in counters]\n",
    "    # return part_list\n",
    "    for i in range(D - 1):\n",
    "        for j in range(i + 1, D):\n",
    "            sent_1 = counters[i]\n",
    "            sent_2 = counters[j]\n",
    "            share_word_set = sent_1 & sent_2\n",
    "            numerator = sum([(sent_1[w] * sent_2[w] * (isf(w, D, counters) ** 2)) for w in share_word_set])\n",
    "            denominator = part_list[i] * part_list[j]\n",
    "            sim_matrix[i, j] = numerator / denominator\n",
    "    sim_matrix = sim_matrix + sim_matrix.T\n",
    "    g = nx.from_numpy_array(sim_matrix)\n",
    "    score = nx.pagerank(g)\n",
    "    temp = sorted(score.items(), key=lambda x: x[1], reverse=True)\n",
    "    idx = [item[0] for item in temp]\n",
    "    return [sents[i] for i in idx], [score[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triangles = find_triangle_with_node(single_sent_graph, 'Machine learning', 'Artificial neural network', 'Deep learning')\n",
    "test_triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list = [triangle[1] for triangle in test_triangles]\n",
    "sents, score = do_pagerank(sent_list)\n",
    "list(zip(score, sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Test] Check the score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [collect_score_function_eval_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test score function with human evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, log_max_cnt = load_pattern_freq(path_pattern_count_file)\n",
    "\n",
    "pattern_freq_w = 0.55\n",
    "kw_recall_w = 0.25\n",
    "coverage_w = 0.2\n",
    "\n",
    "def get_score(sent:str, ent1:str, ent2:str):\n",
    "    kw1 = gen_kw_from_wiki_ent(ent1)\n",
    "    kw2 = gen_kw_from_wiki_ent(ent2)\n",
    "    data = feature_process(nlp(sent), [{'kw1' : kw1, 'kw2' : kw2}])\n",
    "    if not data:\n",
    "        return -1\n",
    "    data = data[0]\n",
    "    pattern_freq = cal_freq_from_path(gen_pattern(data['dep_path']), c, log_max_cnt)\n",
    "    return ((pattern_freq)**pattern_freq_w) * (((data['kw1_recall'] + data['kw2_recall']) / 2)**kw_recall_w) * (((data['dep_coverage'] + data['surface_coverage']) / 2)**coverage_w)\n",
    "\n",
    "test_data = pd.read_csv('test.tsv', sep='\\t')\n",
    "score_function_result = test_data.copy()\n",
    "score_function_result['score'] = score_function_result.apply(lambda x: get_score(x['sentence'], x['entity 1'], x['entity 2']), axis=1)\n",
    "score_function_result.to_csv('score_function_result.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('score_function_result.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    sf_score = [float(lines[i].strip().split('\\t')[-1]) for i in range(1, len(lines))]\n",
    "    sf_score = np.array(sf_score)\n",
    "\n",
    "with open('user_label.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    user_score = [float(lines[i].strip().split('\\t')[-1]) / 5 for i in range(1, len(lines))]\n",
    "    user_score = np.array(user_score)\n",
    "    \n",
    "with open('user_label.tsv') as f_in:\n",
    "    lines = f_in.readlines()\n",
    "    data = []\n",
    "    for i in range(1, len(lines)):\n",
    "        ent1, ent2, sent, user_score_ = lines[i].strip().split('\\t')\n",
    "        data.append({'entity 1' : ent1, 'entity 2' : ent2, 'sentence' : sent, 'user label' : float(user_score_)/5})\n",
    "        \n",
    "sf_score = sf_score[user_score > 0]\n",
    "user_score = user_score[user_score > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(sf_score, user_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score range within [1,2,3,4,5]\n",
    "l2 = np.mean(np.abs(sf_score*5 - user_score*5))\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(user_score*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(user_score[sf_score>0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(user_score[sf_score<=0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(sf_score*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = np.abs(sf_score - user_score)\n",
    "diff = []\n",
    "for i in range(len(dist)):\n",
    "    if dist[i] > 0.3:\n",
    "        diff.append(data[i].copy())\n",
    "        diff[-1]['score function label'] = sf_score[i]\n",
    "diff = pd.DataFrame(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.to_csv('diff.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Super Sub-graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect all sentences between two entities within one hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Load] Single sentence graph\n",
    "single_sent_graph = my_read_pickle(single_sent_graph_file)\n",
    "edges = [edge for edge in tqdm.tqdm(single_sent_graph.edges) if single_sent_graph.get_edge_data(*edge)['score'] > 0.65]\n",
    "filtered_graph = single_sent_graph.edge_subgraph(edges)\n",
    "print('number of nodes:', filtered_graph.number_of_nodes())\n",
    "print('number of edges:', filtered_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = find_path_between_pair(single_sent_graph, 'Artificial intelligence', 'Natural language processing', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subgraph(paths:list, single_sent_graph:nx.Graph):\n",
    "    pairs = set()\n",
    "    triples = []\n",
    "    for path in paths:\n",
    "        if len(path) <= 2:\n",
    "            continue\n",
    "        for i in range(len(path)-1):\n",
    "            new_pair = frozenset((path[i], path[i+1]))\n",
    "            if new_pair not in pairs:\n",
    "                pairs.add(new_pair)\n",
    "                triples.append(list(new_pair) + [note2line(single_sent_graph.get_edge_data(path[i], path[i+1])['note']).strip()])\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = build_subgraph(paths, single_sent_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a graph for one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentence\n",
    "doc = nlp('sephardi were exempt from the ban , but it appears that few applied for a letter of free passage .')\n",
    "\n",
    "# Check noun phrases in the sentences\n",
    "print(list(doc.noun_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('ada is a structured , statically typed , imperative , and object-oriented high-level programming language , extended from pascal and other language .')\n",
    "pairs = [{'kw1' : 'ada', 'kw2' : 'programming language'}]\n",
    "feature_process(doc, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Not necessary] Online operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sents_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    remove_list = ['See also', 'References', 'Further reading', 'Sources', 'External links']\n",
    "    dic = {sec : page.section(sec) for sec in page.sections}\n",
    "    dic['summary'] = page.summary\n",
    "    sents = []\n",
    "    section_list = list(dic.keys())\n",
    "    while len(section_list) > 0:\n",
    "        section = section_list.pop()\n",
    "        if section in remove_list:\n",
    "            continue\n",
    "        section_text = dic[section]\n",
    "        if not section_text:\n",
    "            continue\n",
    "        # processed_text = clean_text(section_text)\n",
    "        processed_text = ' '.join(section_text.lower().split())\n",
    "        temp_sents = my_sentence_tokenize(processed_text, True)\n",
    "        sents += temp_sents\n",
    "    return list(sents)\n",
    "\n",
    "def collect_entity_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    return [text.lower() for text in page.links]\n",
    "\n",
    "def collect_keyword_from_wiki_page(page:wikipedia.WikipediaPage):\n",
    "    soup = BeautifulSoup(page.html(), 'html.parser')\n",
    "    main_block = soup.find('div', class_='mw-parser-output')\n",
    "    keywords = set([l.text.lower() for l in main_block.findAll('a') if re.match(r'^(<a href=\"/wiki/)', str(l))])\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'python'\n",
    "\n",
    "p = wikipedia.page(keyword)\n",
    "if p is not None:\n",
    "    sents = collect_sents_from_wiki_page(p)\n",
    "    keywords = collect_keyword_from_wiki_page(p)\n",
    "    print('sentences collected')\n",
    "    my_write('%s.txt' % keyword, sents)\n",
    "    my_write('%s_kw.txt' % keyword, keywords)\n",
    "    df = filter_by_path(sents)\n",
    "    df.to_csv('%s_out.tsv' % keyword, sep='\\t', index=False)\n",
    "\n",
    "    dff = df[df.apply(lambda x: str(x['head']) in keywords and str(x['tail']) in keywords, axis=1)]\n",
    "    dff.to_csv('%s_out_f.tsv' % keyword, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand-crafted analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_test_df = wiki_path_test_df[wiki_path_test_df['sim'] >= 0.0]\n",
    "\n",
    "def match_path_pattern(path:str):\n",
    "    for pp in patterns:\n",
    "        if exact_match(pp, path):\n",
    "            return pp\n",
    "    return ''\n",
    "\n",
    "wiki_test_df['pattern'] = wiki_test_df.apply(lambda x: match_path_pattern(x['path']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_path_result_sim_based(df:pd.DataFrame, paths:list):\n",
    "    summary_df = pd.DataFrame(columns=['path', 'cnt', 'ratio', 'avg_sim'])\n",
    "    for pp in paths:\n",
    "        sub_df = df[df['pattern'] == pp]\n",
    "        summary_df = summary_df.append({\n",
    "            'path' : pp,\n",
    "            'cnt' : len(sub_df),\n",
    "            'ratio' : len(sub_df) / len(df),\n",
    "            'avg_sim' : sum(sub_df['sim']) / len(sub_df) if len(sub_df) else 0\n",
    "        }, ignore_index=True)\n",
    "    summary_df = summary_df.append({\n",
    "        'path' : 'general',\n",
    "        'cnt' : len(df),\n",
    "        'ratio' : 1,\n",
    "        'avg_sim' : sum(df['sim']) / len(df) if len(df) else 0\n",
    "    }, ignore_index=True)\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path_result_sim_based(wiki_test_df, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_example_sent_for_pattern(df:pd.DataFrame, path:str, num:int=30, posfix:str='.dat'):\n",
    "    sub_df = df[df['pattern'] == path]\n",
    "    num = min(len(sub_df), num)\n",
    "    sub_df = sub_df[:num]\n",
    "    sub_df['sent'] = sub_df.apply(lambda x: note2line(x['sent'], posfix=posfix).strip(), axis=1)\n",
    "    return sub_df\n",
    "\n",
    "for patt in patterns:\n",
    "    temp_df = collect_example_sent_for_pattern(wiki_test_df, patt)\n",
    "    temp_df.to_csv('%s.tsv' % (patt[:10] if len(patt) >= 10 else patt), index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle_set = my_read_pickle('data/extract_wiki/triangles.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(triangle_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, tri in enumerate(triangle_set):\n",
    "    print(tri)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i, tri in enumerate(triangle_set):\n",
    "    samples.append(find_triangle_with_node(single_sent_graph, *tri))\n",
    "    if i > 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9496c91418be784f00ee6456e4343e8188c649322b68f201c83241a4029a42d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('FWD_py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
