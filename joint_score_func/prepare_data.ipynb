{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.TextProcessing import build_word_tree, process_keywords, nlp, clean_text\n",
    "from tools.BasicUtils import my_write, my_csv_read, my_read, my_json_read\n",
    "from tools.OpenIEUtils import processed_file_reader, openie_my_map"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect keywords from terms-cs-cfl-epoch200.txt\n",
    "stable_kw = []\n",
    "unstable_kw = []\n",
    "r = my_csv_read('../data/raw_data/terms-cs-cfl-epoch200.txt', delimiter='\\t')\n",
    "candidate_kw_list = [item[0] for item in r if float(item[1]) > 0.1]\n",
    "stable_kw, unstable_kw = process_keywords(candidate_kw_list)\n",
    "# Save keywords\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "my_write('data/keyword.txt', stable_kw)\n",
    "# Generate word tree (25 seconds)\n",
    "build_word_tree('data/keyword.txt', 'data/wordtree.json', 'data/entity.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Go to py folder and run followings in the backend \n",
    "# \"python gen_co_occur.py ../joint_score_func/data/wordtree.json ../data/corpus/small_sent.txt ../joint_score_func/data/co_occur.txt\"\n",
    "# \"python gen_occur.py ../joint_score_func/data/keyword.txt ../joint_score_func/data/co_occur.txt ../joint_score_func/data/occur.json\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Generate dataset using Wikidata knowledge graph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Knowledge Graph filtering\n",
    "\n",
    "# Load known cs keywords\n",
    "kw_set = set(my_read('data/keyword.txt'))\n",
    "# Get potential cs entity id\n",
    "eid_set = set([eid for eid, ent in my_csv_read('../data/raw_data/wikidata/entity_names.txt', delimiter='\\t') if ent.lower() in kw_set])\n",
    "# Get the subgraph that both entities are potential cs keywords\n",
    "kg_cs_triples = [(eid1, eid2, rid) for eid1, eid2, rid in my_csv_read('../data/raw_data/wikidata/triples.txt', delimiter=' ') if eid1 in eid_set and eid2 in eid_set]\n",
    "# Get cs entities and relations from subgraph\n",
    "cs_eid_set = set()\n",
    "cs_rid_set = set()\n",
    "for eid1, eid2, rid in kg_cs_triples:\n",
    "    cs_eid_set.update((eid1, eid2))\n",
    "    cs_rid_set.add(rid)\n",
    "# Map id to text\n",
    "eid2ent_dict = {eid:ent.lower() for eid, ent in my_csv_read('../data/raw_data/wikidata/entity_names.txt', delimiter='\\t') if eid in cs_eid_set}\n",
    "rid2rel_dict = {rid:rel.lower() for rid, rel in my_csv_read('../data/raw_data/wikidata/relation_names.txt', delimiter='\\t') if rid in cs_rid_set}\n",
    "# Save files\n",
    "json.dump(eid2ent_dict, open('data/eid2ent.json', 'w'))\n",
    "json.dump(rid2rel_dict, open('data/rid2rel.json', 'w'))\n",
    "csv.writer(open('data/kg_cs_triples.csv', 'w')).writerows(kg_cs_triples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Filter out pairs that have little co-occurance\n",
    "occur_dict = my_json_read('data/occur.json')\n",
    "occur_dict = {k:set(v) for k, v in occur_dict.items()}\n",
    "acceptable_triple_data = []\n",
    "for eid1, eid2, rid in kg_cs_triples:\n",
    "    ent1, ent2 = eid2ent_dict[eid1], eid2ent_dict[eid2]\n",
    "    if len(occur_dict[ent1] & occur_dict[ent2]) > 10 and rid in rid2rel_dict:\n",
    "        acceptable_triple_data.append((ent1, ent2, rid2rel_dict[rid]))\n",
    "csv.writer(open('data/datasets.csv', 'w')).writerows(acceptable_triple_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Generate dataset using OpenIE training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "openie_triples = json.load(open('../data/corpus/openie_triples.json'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filtered_triples = copy.deepcopy(openie_triples)\n",
    "for item in filtered_triples:\n",
    "    item['triples'] = [tri for tri in item['triples'] if tri[0] >= 0.9]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filtered_triples[3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "openie_triples[3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Ollie extraction on arxiv corpus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "low_score_list = processed_file_reader('../openie/ollie_test/small_processed_low.txt')\n",
    "high_score_list = processed_file_reader('../openie/ollie_test/small_processed_high.txt')\n",
    "all_score_list = processed_file_reader('../openie/ollie_test/small_processed_all.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "low_score_triple = [item for item in low_score_list if len(item) > 1]\n",
    "high_score_triple = [item for item in high_score_list if len(item) > 1]\n",
    "no_extraction = [item for item in all_score_list if len(item) == 1]\n",
    "print(len(low_score_triple))\n",
    "print(len(high_score_triple))\n",
    "print(len(no_extraction))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate pos dataset\n",
    "pos_dataset = []\n",
    "for item in high_score_triple:\n",
    "    sent = clean_text(item[0])\n",
    "    for triple in item[1:]:\n",
    "        ent1, rel, ent2 = triple.split(';')\n",
    "        ent1, rel, ent2 = clean_text(ent1), clean_text(rel), clean_text(ent2)\n",
    "        pos_dataset.append({'label' : 1, 'ent1' : ent1, 'rel' : rel, 'ent2' : ent2, 'sent' : sent})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate neg dataset from low score triples\n",
    "neg_dataset_1 = []\n",
    "for item in low_score_triple:\n",
    "    sent = clean_text(item[0])\n",
    "    for triple in item[1:]:\n",
    "        ent1, rel, ent2 = triple.split(';')\n",
    "        ent1, rel, ent2 = clean_text(ent1), clean_text(rel), clean_text(ent2)\n",
    "        neg_dataset_1.append({'label' : 0, 'ent1' : ent1, 'rel' : rel, 'ent2' : ent2, 'sent' : sent})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate neg dataset from no extraction sentences with noun chunks\n",
    "neg_dataset_2 = []\n",
    "for item in no_extraction:\n",
    "    sent = clean_text(item[0])\n",
    "    noun_chunks = list(nlp(sent).noun_chunks)\n",
    "    if len(noun_chunks) <= 1:\n",
    "        continue\n",
    "    ents = random.sample(noun_chunks, 2)\n",
    "    neg_dataset_2.append({'label' : 0, 'ent1' : str(ents[0]), 'rel' : '_', 'ent2' : str(ents[1]), 'sent' : sent})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Forming dataset json\n",
    "dataset = (pos_dataset + neg_dataset_1 + neg_dataset_2)\n",
    "random.shuffle(dataset)\n",
    "split_point = int(len(dataset) * 0.8)\n",
    "final_dataset = {'train' : dataset[:split_point], 'valid' : dataset[split_point:]}\n",
    "json.dump(final_dataset, open('data/my_dataset.json', 'w'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Forming dataset in datasets format\n",
    "temp_train = load_dataset('json', data_files='data/my_dataset.json', field='train')\n",
    "temp_valid = load_dataset('json', data_files='data/my_dataset.json', field='valid')\n",
    "temp_train['valid'] = temp_valid['train']\n",
    "temp_train.save_to_disk('data/single-ollie')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Training test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = load_from_disk('data/single-ollie')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens' : ['<RELATION>']})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_args = TrainingArguments(\"data/single-ollie\", evaluation_strategy=\"epoch\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_args"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess_sf1(examples):\n",
    "    # return BatchEncoding(tokenizer(examples['ent1'], examples['ent2'], padding=True, truncation=True, max_length=100, return_tensors=\"pt\"))\n",
    "    query = ['%s <RELATION> %s' % (ent1, ent2) for ent1, ent2 in zip(examples['ent1'], examples['ent2'])]\n",
    "    return tokenizer(query, examples[\"sent\"], padding=\"max_length\", truncation=True)\n",
    "    \n",
    "train_dataset = dataset['train'].map(preprocess_sf1, batched=True)\n",
    "valid_dataset = dataset['valid'].map(preprocess_sf1, batched=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=valid_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('FWD': conda)"
  },
  "interpreter": {
   "hash": "5e687002bc60377ae87b855adfe470e827b4be244d7382e97081511de02b6558"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}