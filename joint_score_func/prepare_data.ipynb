{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Uncomment for google drive use\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# %cd gdrive/MyDrive/Colab\\ Notebooks/Forward/joint_score_func\n",
    "# ! pip install datasets\n",
    "# ! pip install transformers\n",
    "# ! pip install wikipedia"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.TextProcessing import build_word_tree, process_keywords, nlp, clean_text\n",
    "from tools.BasicUtils import my_write, my_csv_read, my_read, my_json_read\n",
    "from tools.OpenIEUtils import processed_file_reader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "file_description = [\n",
    "    \"data/keyword_f.txt ---- CS keywords\\n\",\n",
    "    \"data/wordtree.json ---- word tree for cs keywords\\n\",\n",
    "    \"data/entity.txt ---- Reformed cs keywords with '_' replacing ' '\\n\",\n",
    "    \"data/co_occur.txt ---- Each line shows the keywords that appear in that line of sentence\\n\",\n",
    "    \"data/occur.json ---- Tell which lines do each keyword occur\\n\",\n",
    "    \"data/eid2ent.json ---- Mapping from entity id to entity name in wikidata\\n\",\n",
    "    \"data/rid2rel.json ---- Mapping from relation id to relation name in wikidat\\na\"\n",
    "    \"data/kg_cs_triples.csv ---- eid-rid-eid triples with eid be referring to possible cs keywords\\n\",\n",
    "    \"data/kg_dataset.csv ---- ent-rel-ent triples constructed on knowledge graph with each entity pair co-occurs no less than 10 times in small_sent.txt\\n\",\n",
    "    \"data/ollie_pos_dataset.csv ---- data containing triples and sentences with confidence greater than 0.9 in csv form\\n\",\n",
    "    \"data/ollie_pos_dataset.json ---- data containing triples and sentences with confidence greater than 0.9\\n\",\n",
    "    \"data/ollie_neg_dataset_1.json ---- data containing triples and sentences with confidence less than 0.3\\n\",\n",
    "    \"data/ollie_neg_dataset_2.json ---- data containing triples and sentences where no extraction is made\\n\",\n",
    "    \"data/my_dataset.json ---- data containing pos, neg_1 and neg_2, splited to train and valid part\\n\",\n",
    "    \"data/my_dataset_temp.json ---- smaller set of my_dataset.json\\n\",\n",
    "    \"data/single-ollie ---- transformers.dataset style file\\n\"\n",
    "]\n",
    "    \n",
    "my_write('README.md', file_description)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Generate basic keyword file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect keywords from terms-cs-cfl-epoch200.txt\n",
    "stable_kw = []\n",
    "unstable_kw = []\n",
    "r = my_csv_read('../data/raw_data/terms-cs-cfl-epoch200.txt', delimiter='\\t')\n",
    "candidate_kw_list = [item[0] for item in r if float(item[1]) > 0.1]\n",
    "stable_kw, unstable_kw = process_keywords(candidate_kw_list)\n",
    "# Save keywords\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "my_write('data/keyword.txt', stable_kw)\n",
    "# Generate word tree (25 seconds)\n",
    "build_word_tree('data/keyword.txt', 'data/wordtree.json', 'data/entity.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Go to py folder and run followings in the backend \n",
    "# \"python gen_co_occur.py ../joint_score_func/data/wordtree.json ../data/corpus/small_sent.txt ../joint_score_func/data/co_occur.txt\"\n",
    "# \"python gen_occur.py ../joint_score_func/data/keyword.txt ../joint_score_func/data/co_occur.txt ../joint_score_func/data/occur.json\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Generate dataset using Wikidata knowledge graph"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load known cs keywords\n",
    "kw_set = set(my_read('data/keyword.txt'))\n",
    "# Map id to text\n",
    "eid2ent_dict = {eid:ent.lower() for eid, ent in my_csv_read('../data/raw_data/wikidata/entity_names.txt', delimiter='\\t') if ent.lower() in kw_set}\n",
    "rid2rel_dict = {rid:rel.lower() for rid, rel in my_csv_read('../data/raw_data/wikidata/relation_names.txt', delimiter='\\t')}\n",
    "# Get the subgraph that have both entities be potential cs keywords and relation be valid\n",
    "kg_cs_triples = [(eid1, eid2, rid) for eid1, eid2, rid in my_csv_read('../data/raw_data/wikidata/triples.txt', delimiter=' ') if eid1 in eid2ent_dict and eid2 in eid2ent_dict and rid in rid2rel_dict]\n",
    "# Get valid cs entities and relations from subgraph\n",
    "cs_eid_set = set()\n",
    "cs_rid_set = set()\n",
    "for eid1, eid2, rid in kg_cs_triples:\n",
    "    cs_eid_set.update((eid1, eid2))\n",
    "    cs_rid_set.add(rid)\n",
    "eid2ent_dict = {eid:ent for eid, ent in eid2ent_dict.items() if eid in cs_eid_set}\n",
    "rid2rel_dict = {rid:rel for rid, rel in rid2rel_dict.items() if rid in cs_rid_set}\n",
    "# Save files\n",
    "json.dump(eid2ent_dict, open('data/eid2ent.json', 'w'))\n",
    "json.dump(rid2rel_dict, open('data/rid2rel.json', 'w'))\n",
    "csv.writer(open('data/kg_cs_triples.csv', 'w')).writerows(kg_cs_triples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Filter out pairs that have little co-occurance\n",
    "eid2ent_dict = json.load(open('data/eid2ent.json'))\n",
    "rid2rel_dict = json.load(open('data/rid2rel.json'))\n",
    "kg_cs_triples = list(my_csv_read('data/kg_cs_triples.csv', delimiter=','))\n",
    "occur_dict = my_json_read('data/occur.json')\n",
    "occur_dict = {k:set(v) for k, v in occur_dict.items()}\n",
    "acceptable_triple_data = []\n",
    "for eid1, eid2, rid in kg_cs_triples:\n",
    "    ent1, ent2 = eid2ent_dict[eid1], eid2ent_dict[eid2]\n",
    "    if len(occur_dict[ent1] & occur_dict[ent2]) > 10 and rid in rid2rel_dict:\n",
    "        acceptable_triple_data.append((ent1, ent2, rid2rel_dict[rid]))\n",
    "csv.writer(open('data/kg_datasets.csv', 'w')).writerows(acceptable_triple_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Generate dataset using public OpenIE training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "openie_triples = json.load(open('../data/corpus/openie_triples.json'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filtered_triples = copy.deepcopy(openie_triples)\n",
    "for item in filtered_triples:\n",
    "    item['triples'] = [tri for tri in item['triples'] if tri[0] >= 0.9]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filtered_triples[3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "openie_triples[3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Generate dataset using Ollie extraction on arxiv corpus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "low_score_list = processed_file_reader('../openie/ollie_test/small_processed_low.txt')\n",
    "high_score_list = processed_file_reader('../openie/ollie_test/small_processed_high.txt')\n",
    "all_score_list = processed_file_reader('../openie/ollie_test/small_processed_all.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "low_score_triple = [item for item in low_score_list if len(item) > 1]\n",
    "high_score_triple = [item for item in high_score_list if len(item) > 1]\n",
    "no_extraction = [item for item in all_score_list if len(item) == 1]\n",
    "print(len(low_score_triple))\n",
    "print(len(high_score_triple))\n",
    "print(len(no_extraction))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate pos dataset\n",
    "pos_dataset = []\n",
    "for item in high_score_triple:\n",
    "    sent = clean_text(item[0])\n",
    "    for triple in item[1:]:\n",
    "        ent1, rel, ent2 = triple.split(';')\n",
    "        ent1, rel, ent2 = clean_text(ent1), clean_text(rel), clean_text(ent2)\n",
    "        pos_dataset.append({'labels' : 1, 'ent1' : ent1, 'rel' : rel, 'ent2' : ent2, 'sent' : sent})\n",
    "json.dump(pos_dataset, open('data/ollie_pos_dataset.json', 'w'))\n",
    "pd.DataFrame(pos_dataset).to_csv('data/ollie_pos_dataset.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pos_dataset[:4]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate neg dataset from low score triples\n",
    "neg_dataset_1 = []\n",
    "for item in low_score_triple:\n",
    "    sent = clean_text(item[0])\n",
    "    for triple in item[1:]:\n",
    "        ent1, rel, ent2 = triple.split(';')\n",
    "        ent1, rel, ent2 = clean_text(ent1), clean_text(rel), clean_text(ent2)\n",
    "        neg_dataset_1.append({'labels' : 0, 'ent1' : ent1, 'rel' : rel, 'ent2' : ent2, 'sent' : sent})\n",
    "json.dump(neg_dataset_1, open('data/ollie_neg_dataset_1.json', 'w'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "neg_dataset_1[:4]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate neg dataset from no extraction sentences with noun chunks\n",
    "neg_dataset_2 = []\n",
    "for item in no_extraction:\n",
    "    sent = clean_text(item[0])\n",
    "    noun_chunks = list(nlp(sent).noun_chunks)\n",
    "    if len(noun_chunks) <= 1:\n",
    "        continue\n",
    "    ents = random.sample(noun_chunks, 2)\n",
    "    neg_dataset_2.append({'labels' : 0, 'ent1' : str(ents[0]), 'rel' : '_', 'ent2' : str(ents[1]), 'sent' : sent})\n",
    "json.dump(neg_dataset_2, open('data/ollie_neg_dataset_2.json', 'w'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Forming dataset json\n",
    "pos_dataset = json.load(open('data/ollie_pos_dataset.json'))\n",
    "neg_dataset_1 = json.load(open('data/ollie_neg_dataset_1.json'))\n",
    "neg_dataset_2 = json.load(open('data/ollie_neg_dataset_2.json'))\n",
    "dataset = (pos_dataset + neg_dataset_1 + neg_dataset_2)\n",
    "random.shuffle(dataset)\n",
    "split_point = int(len(dataset) * 0.8)\n",
    "final_dataset = {'train' : dataset[:split_point], 'valid' : dataset[split_point:]}\n",
    "json.dump(final_dataset, open('data/my_dataset.json', 'w'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Forming dataset in datasets format\n",
    "temp_train = load_dataset('json', data_files='data/my_dataset.json', field='train')\n",
    "temp_valid = load_dataset('json', data_files='data/my_dataset.json', field='valid')\n",
    "temp_train['valid'] = temp_valid['train']\n",
    "temp_train.save_to_disk('data/single-ollie')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reframe the dataset if needed\n",
    "temp_dict = json.load(open('data/my_dataset.json'))\n",
    "temp_dict['train'] = temp_dict['train'][:10000]\n",
    "temp_dict['valid'] = temp_dict['valid'][:2000]\n",
    "json.dump(temp_dict, open('data/my_dataset_temp.json', 'w'))\n",
    "temp_train = load_dataset('json', data_files='data/my_dataset_temp.json', field='train')\n",
    "temp_valid = load_dataset('json', data_files='data/my_dataset_temp.json', field='valid')\n",
    "temp_train['valid'] = temp_valid['train']\n",
    "temp_train.save_to_disk('data/single-ollie-small')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate dataset with only pos_dataset\n",
    "df = pd.read_csv('data/ollie_pos_dataset.csv')\n",
    "pos_only_dataset = Dataset.from_pandas(df)\n",
    "pos_only_dataset = pos_only_dataset.train_test_split(test_size=0.1)\n",
    "DatasetDict({'train': pos_only_dataset['train'], 'valid': pos_only_dataset['test']}).save_to_disk('data/single-ollie-pos-only')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}