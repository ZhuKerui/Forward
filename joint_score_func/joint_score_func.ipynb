{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from transformers import EncoderDecoderModel, BertTokenizer, BertModel, BertForNextSentencePrediction, BatchEncoding\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.TextProcessing import build_word_tree, process_keywords\n",
    "from tools.BasicUtils import my_write, my_csv_read, my_read, my_json_read, ntopidx, batch\n",
    "from joint_score_func import SparseRetrieveSentForPairCoOccur, ScoreFunction1, demo_score_function, ScoreFunction2, Reader1, demo_reader, Reader2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "file_description = [\n",
    "    \"keyword_f.txt ---- CS keywords\",\n",
    "    \"wordtree.json ---- word tree for cs keywords\",\n",
    "    \"entity.txt ---- Reformed cs keywords with '_' replacing ' '\"\n",
    "]\n",
    "    \n",
    "my_write('readme.txt', file_description)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Collect keywords from terms-cs-cfl-epoch200.txt\n",
    "stable_kw = []\n",
    "unstable_kw = []\n",
    "r = my_csv_read('../data/raw_data/terms-cs-cfl-epoch200.txt', delimiter='\\t')\n",
    "candidate_kw_list = [item[0] for item in r if float(item[1]) > 0.1]\n",
    "stable_kw, unstable_kw = process_keywords(candidate_kw_list)\n",
    "# Save keywords\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "my_write('data/keyword.txt', stable_kw)\n",
    "# Generate word tree (25 seconds)\n",
    "build_word_tree('data/keyword.txt', 'data/wordtree.json', 'data/entity.txt')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Go to py folder and run followings in the backend \n",
    "# \"python gen_co_occur.py ../joint_score_func/data/wordtree.json ../data/corpus/small_sent.txt ../joint_score_func/data/co_occur.txt\"\n",
    "# \"python gen_occur.py ../joint_score_func/data/keyword.txt ../joint_score_func/data/co_occur.txt ../joint_score_func/data/occur.json\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Knowledge Graph filtering\n",
    "\n",
    "# Load known cs keywords\n",
    "kw_set = set(my_read('data/keyword.txt'))\n",
    "# Get potential cs entity id\n",
    "eid_set = set([eid for eid, ent in my_csv_read('../data/raw_data/wikidata/entity_names.txt', delimiter='\\t') if ent.lower() in kw_set])\n",
    "# Get the subgraph that both entities are potential cs keywords\n",
    "kg_cs_triples = [(eid1, eid2, rid) for eid1, eid2, rid in my_csv_read('../data/raw_data/wikidata/triples.txt', delimiter=' ') if eid1 in eid_set and eid2 in eid_set]\n",
    "# Get cs entities and relations from subgraph\n",
    "cs_eid_set = set()\n",
    "cs_rid_set = set()\n",
    "for eid1, eid2, rid in kg_cs_triples:\n",
    "    cs_eid_set.update((eid1, eid2))\n",
    "    cs_rid_set.add(rid)\n",
    "# Map id to text\n",
    "eid2ent_dict = {eid:ent.lower() for eid, ent in my_csv_read('../data/raw_data/wikidata/entity_names.txt', delimiter='\\t') if eid in cs_eid_set}\n",
    "rid2rel_dict = {rid:rel.lower() for rid, rel in my_csv_read('../data/raw_data/wikidata/relation_names.txt', delimiter='\\t') if rid in cs_rid_set}\n",
    "# Save files\n",
    "json.dump(eid2ent_dict, open('data/eid2ent.json', 'w'))\n",
    "json.dump(rid2rel_dict, open('data/rid2rel.json', 'w'))\n",
    "csv.writer(open('data/kg_cs_triples.csv', 'w')).writerows(kg_cs_triples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "eid2ent_dict = my_json_read('data/eid2ent.json')\n",
    "rid2rel_dict = my_json_read('data/rid2rel.json')\n",
    "kg_cs_triples = my_csv_read('data/kg_cs_triples.csv')\n",
    "rel_list = list(set(rid2rel_dict.values()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show some examples of graph triples\n",
    "for i, item in enumerate(kg_cs_triples):\n",
    "    ent1, ent2, rel = eid2ent_dict[item[0]], eid2ent_dict[item[1]], rid2rel_dict[item[2]]\n",
    "    print('%s--%s--%s' % (ent1, ent2, rel))\n",
    "    if i >= 10:\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sparse_retriever = SparseRetrieveSentForPairCoOccur('../data/corpus/small_sent.txt', 'data/occur.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_sent = sparse_retriever.retrieve('java', 'python')\n",
    "print(len(test_sent))\n",
    "print(test_sent[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Some helper functions\n",
    "\n",
    "def train(kg_cs_triples, eid2ent_dict, rid2rel_dict, retriever, sf):\n",
    "    for eid1, eid2, rid in kg_cs_triples:\n",
    "        ent1, ent2, rel = eid2ent_dict[eid1], eid2ent_dict[eid2], rid2rel_dict[rid]\n",
    "        candidate_sents = retriever.retrieve(ent1, ent2)\n",
    "        query = '%s <RELATION> %s' % (ent1, ent2)\n",
    "        with torch.no_grad():\n",
    "            scores = [sf(sents, query) for sents in batch(candidate_sents, 16)]\n",
    "        top_idx = ntopidx(5, scores)\n",
    "        sub_sents = [candidate_sents[idx] for idx in top_idx]\n",
    "        sub_score = sf(sub_sents, query).unsqueeze(0)\n",
    "        torch.cuda.empty_cache()\n",
    "        break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sf1 = ScoreFunction1('bert-base-uncased', additional_special_tokens=['<RELATION>'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ScoreFunction1 Example\n",
    "scores, candidate_sents = demo_score_function('python', 'java', sf1, sparse_retriever)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sf2 = ScoreFunction2('bert-base-uncased', 'bert-base-uncased', additional_special_tokens=['<RELATION>'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ScoreFunction2 Example\n",
    "scores, candidate_sents = demo_score_function('python', 'java', sf2, sparse_retriever)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reader1 = Reader1('bert-base-uncased', rel_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Reader1 Example\n",
    "rels = demo_reader('python', 'java', reader1, sparse_retriever)\n",
    "print(rels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reader2 = Reader2('bert-base-uncased', rel_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "m = BertModel.from_pretrained('bert-base-uncased')\n",
    "t = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# initialize Bert2Bert from pre-trained checkpoints\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased')\n",
    "bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "bert2bert.config.eos_token_id = tokenizer.sep_token_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('FWD': conda)"
  },
  "interpreter": {
   "hash": "5e687002bc60377ae87b855adfe470e827b4be244d7382e97081511de02b6558"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}