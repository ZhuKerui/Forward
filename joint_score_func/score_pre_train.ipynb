{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import csv\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Softmax, Sigmoid, BCELoss\n",
    "import pandas\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, BatchEncoding, PreTrainedModel, PretrainedConfig, BertModel, BertConfig, AdamW, BartForConditionalGeneration, BartTokenizer, GPT2Tokenizer, GPT2LMHeadModel\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "from tools.BasicUtils import ntopidx, SparseRetrieveSentForPairCoOccur"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Training test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Score function 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens' : ['<RELATION>']})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define dataset and training arguement\n",
    "dataset = load_from_disk('data/single-ollie')\n",
    "training_args = TrainingArguments(\"data/single-ollie-sf1\", evaluation_strategy=\"epoch\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define data pre-process function and reform dataset\n",
    "def preprocess_sf1(examples):\n",
    "    # return BatchEncoding(tokenizer(examples['ent1'], examples['ent2'], padding=True, truncation=True, max_length=100, return_tensors=\"pt\"))\n",
    "    query = ['%s <RELATION> %s' % (ent1, ent2) for ent1, ent2 in zip(examples['ent1'], examples['ent2'])]\n",
    "    return tokenizer(query, examples[\"sent\"], padding=True, truncation=True, max_length=100)\n",
    "    \n",
    "train_dataset = dataset['train'].map(preprocess_sf1, batched=True)\n",
    "valid_dataset = dataset['valid'].map(preprocess_sf1, batched=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define trainer and do training\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=valid_dataset)\n",
    "trainer.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Score function 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens' : ['<RELATION>']})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define dataset and training arguement\n",
    "dataset = load_from_disk('data/single-ollie')\n",
    "training_args = TrainingArguments(\"data/single-ollie-sf2\", evaluation_strategy=\"epoch\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load model\n",
    "class ScoreFunction2(PreTrainedModel):\n",
    "    def __init__(self, config:PretrainedConfig):\n",
    "        super().__init__(config)\n",
    "        self._context_encoder = BertModel(config)\n",
    "        self._query_encoder = BertModel(config)\n",
    "        self._sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, \n",
    "        context_input_ids,\n",
    "        query_input_ids,\n",
    "        context_token_type_ids=None,\n",
    "        context_attention_mask=None,\n",
    "        query_token_type_ids=None,\n",
    "        query_attention_mask=None):\n",
    "        context_inputs = {'input_ids': context_input_ids, 'token_type_ids': context_token_type_ids, 'attention_mask': context_attention_mask}\n",
    "        query_inputs = {'input_ids': query_input_ids, 'token_type_ids': query_token_type_ids, 'attention_mask': query_attention_mask}\n",
    "        context_emb = self._context_encoder(**context_inputs).last_hidden_state[:, 0, :]\n",
    "        query_emb = self._query_encoder(**query_inputs).last_hidden_state[:, 0, :]\n",
    "        score = self._sigmoid(torch.mul(context_emb, query_emb).sum(dim=1))\n",
    "        return score\n",
    "\n",
    "model = ScoreFunction2(BertConfig())\n",
    "model._query_encoder.resize_token_embeddings(len(tokenizer))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess_sf2(examples):\n",
    "    query = ['%s <RELATION> %s' % (ent1, ent2) for ent1, ent2 in zip(examples['ent1'], examples['ent2'])]\n",
    "    context_tokenized = tokenizer(examples[\"sent\"], padding=True, truncation=True, max_length=100)\n",
    "    query_tokenized = tokenizer(query, padding=True, truncation=True, max_length=100)\n",
    "    return {'context_input_ids': context_tokenized['input_ids'], \n",
    "            'context_token_type_ids': context_tokenized['token_type_ids'], \n",
    "            'context_attention_mask': context_tokenized['attention_mask'],\n",
    "            'query_input_ids': query_tokenized['input_ids'], \n",
    "            'query_token_type_ids': query_tokenized['token_type_ids'], \n",
    "            'query_attention_mask': query_tokenized['attention_mask']}\n",
    "\n",
    "train_dataset = dataset['train'].map(preprocess_sf2, batched=True)\n",
    "valid_dataset = dataset['valid'].map(preprocess_sf2, batched=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ScoreFunction2Trainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss_function = BCELoss()\n",
    "        loss = loss_function(outputs, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "trainer = ScoreFunction2Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=valid_dataset)\n",
    "trainer.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Score function 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens' : ['<ENT1>', '<ENT2>', '<RELATION>']})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define dataset and training arguement\n",
    "dataset = load_from_disk('data/single-ollie-pos-only')\n",
    "training_args_1 = TrainingArguments(\"data/single-ollie-sf3\", evaluation_strategy=\"epoch\")\n",
    "training_args_2 = TrainingArguments(\"data/single-ollie-sf3_2\", evaluation_strategy=\"epoch\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load model\n",
    "relation_extractor = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "relation_extractor.resize_token_embeddings(len(tokenizer))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reconstructor = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "reconstructor.resize_token_embeddings(len(tokenizer))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess_sf3_1(examples):\n",
    "    conditions = ['%s <ENT1> %s <ENT2> %s' % (sent, ent1, ent2) for ent1, ent2, sent in zip(examples['ent1'], examples['ent2'], examples['sent'])]\n",
    "    rel_tokenized = tokenizer(examples[\"sent\"], padding='max_length', truncation=True, max_length=100)\n",
    "    condition_tokenized = tokenizer(conditions, padding='max_length', truncation=True, max_length=100)\n",
    "    return {'input_ids': condition_tokenized['input_ids'],\n",
    "            'labels' : rel_tokenized['input_ids']}\n",
    "\n",
    "def preprocess_sf3_2(examples):\n",
    "    conditions = ['<ENT1> %s <ENT2> %s <RELATION> %s' % (ent1, ent2, rel) for ent1, ent2, rel in zip(examples['ent1'], examples['ent2'], examples['rel'])]\n",
    "    sent_tokenized = tokenizer(examples[\"sent\"], padding='max_length', truncation=True, max_length=100)\n",
    "    condition_tokenized = tokenizer(conditions, padding='max_length', truncation=True, max_length=100)\n",
    "    return {'input_ids': condition_tokenized['input_ids'],\n",
    "            'labels' : sent_tokenized['input_ids']}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset_1 = dataset['train'].map(preprocess_sf3_1, batched=True)\n",
    "valid_dataset_1 = dataset['valid'].map(preprocess_sf3_1, batched=True)\n",
    "train_dataset_2 = dataset['train'].map(preprocess_sf3_2, batched=True)\n",
    "valid_dataset_2 = dataset['valid'].map(preprocess_sf3_2, batched=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define trainer and do training\n",
    "trainer_1 = Trainer(model=relation_extractor, args=training_args_1, train_dataset=train_dataset_1, eval_dataset=valid_dataset_1)\n",
    "trainer_1.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer_2 = Trainer(model=reconstructor, args=training_args_2, train_dataset=train_dataset_2, eval_dataset=valid_dataset_2)\n",
    "trainer_2.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Score function 3 with GPT2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens' : ['<ENT1>', '<ENT2>', '<RELATION>'], 'pad_token': '[PAD]'})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define dataset and training arguement\n",
    "dataset = load_from_disk('data/single-ollie-pos-only')\n",
    "training_args_1_gpt2 = TrainingArguments(\"data/single-ollie-sf3-1-GPT2\", evaluation_strategy=\"epoch\")\n",
    "training_args_2_gpt2 = TrainingArguments(\"data/single-ollie-sf3_2-GPT2\", evaluation_strategy=\"epoch\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load model\n",
    "relation_extractor = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "relation_extractor.resize_token_embeddings(len(tokenizer))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reconstructor = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "reconstructor.resize_token_embeddings(len(tokenizer))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess_sf3_1_gpt2(examples):\n",
    "    inputs = ['%s <ENT1> %s <ENT2> %s <RELATION> %s' % (sent, ent1, ent2, rel) for ent1, ent2, sent, rel in zip(examples['ent1'], examples['ent2'], examples['sent'], examples['rel'])]\n",
    "    input_tokenized = tokenizer(inputs, padding='max_length', truncation=True, max_length=100)\n",
    "    return {'input_ids': input_tokenized['input_ids'],\n",
    "            'attention_mask': input_tokenized['attention_mask'],\n",
    "            'labels' : input_tokenized['input_ids']}\n",
    "\n",
    "train_dataset_1_gpt2 = dataset['train'].map(preprocess_sf3_1_gpt2, batched=True)\n",
    "valid_dataset_1_gpt2 = dataset['valid'].map(preprocess_sf3_1_gpt2, batched=True)\n",
    "\n",
    "def preprocess_sf3_2_gpt2(examples):\n",
    "    inputs = ['<ENT1> %s <ENT2> %s <RELATION> %s <SENT> %s' % (ent1, ent2, rel, sent) for ent1, ent2, sent, rel in zip(examples['ent1'], examples['ent2'], examples['sent'], examples['rel'])]\n",
    "    input_tokenized = tokenizer(inputs, padding='max_length', truncation=True, max_length=100)\n",
    "    return {'input_ids': input_tokenized['input_ids'],\n",
    "            'attention_mask': input_tokenized['attention_mask'],\n",
    "            'labels' : input_tokenized['input_ids']}\n",
    "\n",
    "train_dataset_2_gpt2 = dataset['train'].map(preprocess_sf3_2_gpt2, batched=True)\n",
    "valid_dataset_2_gpt2 = dataset['valid'].map(preprocess_sf3_2_gpt2, batched=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define trainer and do training\n",
    "trainer_1 = Trainer(model=relation_extractor, args=training_args_1_gpt2, train_dataset=train_dataset_1_gpt2, eval_dataset=valid_dataset_1_gpt2)\n",
    "trainer_1.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer_2 = Trainer(model=reconstructor, args=training_args_2_gpt2, train_dataset=train_dataset_2_gpt2, eval_dataset=valid_dataset_2_gpt2)\n",
    "trainer_2.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1> Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Score function 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('data/single-ollie/checkpoint-3500')\n",
    "# model = ScoreFunction2.from_pretrained('data/single-ollie2')\n",
    "model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sparse_retriever = SparseRetrieveSentForPairCoOccur('../data/corpus/small_sent.txt', 'data/occur.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ent1 = 'data mining'\n",
    "ent2 = 'machine learning'\n",
    "s = 'in this paper, we show that by using the fuzzy statistics analysis and the data mining technology, the target - oriented fuzzy correlation rules can be obtained from a given database.'\n",
    "sent = sparse_retriever.retrieve(ent1, ent2)\n",
    "test_list = [{'sent' : s, 'ent1' : ent1, 'ent2' : ent2, 'labels' : 1} for s in sent]\n",
    "# test_list = [{'sent' : s, 'ent1' : ent1, 'ent2' : ent2, 'labels' : 1}]\n",
    "print(len(test_list))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "temp_dict = json.load(open('data/my_dataset.json'))\n",
    "test_list = temp_dict['valid'][:200]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valid_df = pandas.DataFrame.from_dict(test_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function that help generate score\n",
    "def get_score(sents:List[str], ent1s:List[str], ent2s:List[str]):\n",
    "    query = ['%s <RELATION> %s' % (ent1, ent2) for ent1, ent2 in zip(ent1s, ent2s)]\n",
    "    with torch.no_grad():\n",
    "        inputs = BatchEncoding(tokenizer(query, sents, padding=True, truncation=True, max_length=80, return_tensors='pt'))\n",
    "        output = model(**inputs)\n",
    "        s = Softmax(1)\n",
    "        return s(output.logits)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get logits score\n",
    "val_output = get_score(valid_df.sent.to_list(), valid_df.ent1.to_list(), valid_df.ent2.to_list())\n",
    "# Get prediction label\n",
    "cls_result = np.argmax(val_output.numpy(), axis=1)\n",
    "# Get prediction score\n",
    "cls_score = val_output.numpy()[:, 1]\n",
    "# Get ground truth\n",
    "val_label = np.array(valid_df.labels.to_list())\n",
    "# Get correct ones\n",
    "correct_prediction = val_label == cls_result\n",
    "# Sum the number of correct ones\n",
    "correct_num = np.sum(correct_prediction)\n",
    "# Get the wrong prediction idx\n",
    "wrong_prediction_idx = np.arange(0, len(val_label))[val_label != cls_result]\n",
    "# Get the wrong ones\n",
    "wrong_samples = [(cls_result[idx], valid_df.labels[idx], valid_df.ent1[idx], valid_df.ent2[idx], valid_df.sent[idx]) for idx in wrong_prediction_idx]\n",
    "# Write the wrong ones to file\n",
    "with open('data/wrong_prediction.tsv', 'w') as f_out:\n",
    "    w = csv.writer(f_out, delimiter='\\t')\n",
    "    w.writerows(wrong_samples)\n",
    "\n",
    "# Get rank\n",
    "rank_ids = ntopidx(len(cls_score), cls_score)\n",
    "rank_list = [(cls_score[idx], valid_df.ent1[idx], valid_df.ent2[idx], valid_df.sent[idx]) for idx in rank_ids]\n",
    "with open('data/rank_list.tsv', 'w') as f_out:\n",
    "    w = csv.writer(f_out, delimiter='\\t')\n",
    "    w.writerows(rank_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2> Score function 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "relation_extractor = BartForConditionalGeneration.from_pretrained('data/single-ollie-sf3_1/checkpoint-6500')\n",
    "relation_extractor.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "sparse_retriever = SparseRetrieveSentForPairCoOccur('../data/corpus/small_sent.txt', 'data/occur.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "ent1 = 'data mining'\n",
    "ent2 = 'machine learning'\n",
    "s = 'in this paper, we show that by using the fuzzy statistics analysis and the data mining technology, the target - oriented fuzzy correlation rules can be obtained from a given database.'\n",
    "sent = sparse_retriever.retrieve(ent1, ent2)\n",
    "test_list = [{'sent' : s, 'ent1' : ent1, 'ent2' : ent2, 'labels' : 1} for s in sent]\n",
    "# test_list = [{'sent' : s, 'ent1' : ent1, 'ent2' : ent2, 'labels' : 1}]\n",
    "print(len(test_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "191\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "test_df = pd.DataFrame(test_list)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset_input = preprocess_sf3_1(test_dataset)\n",
    "output = relation_extractor.generate(torch.LongTensor(test_dataset_input['input_ids']))\n",
    "test_df['extraction'] = tokenizer.batch_decode(output)\n",
    "test_df[['ent1', 'ent2', 'extraction', 'sent']].to_csv('test_result.tsv', sep='\\t', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9  ('FWD': conda)"
  },
  "interpreter": {
   "hash": "5e687002bc60377ae87b855adfe470e827b4be244d7382e97081511de02b6558"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}